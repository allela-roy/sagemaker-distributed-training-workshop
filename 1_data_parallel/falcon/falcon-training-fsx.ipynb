{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare C4 dataset and Pre-train Falcon model using SageMaker Distributed Data Parallel Library (SMDDP) and PyTorch Fully Sharded Data Parallelism (FSDP)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/training|distributed_training|pytorch|data_parallel|fully_sharded_data_parallel|falcon|smddp_fsdp_example.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, we will show how to train or fine-tune [Falcon-7B-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) on the [GLUE/SST2](https://huggingface.co/datasets/glue/viewer/sst2/train) dataset.  We will use 2 p4d.24xlarge instances, which come with 8 NVIDIA A100 40GB GPUs along with the PyTorch Fully Sharded Data Parallelism (FSDP) technique to efficiently train this large model with limited GPU memory.  \n",
    "\n",
    "To accelerate training speed, we will also use the **SageMaker Distributed Data Parallel Library (SMDDP)** which speeds up GPU communication across P4d instances during sharded data parallel training.  \n",
    "\n",
    "## Files\n",
    "* `scripts/data.py` - Script to download, tokenize and group into a specific sequence length.\n",
    "* `scripts/train.py` - The entry point for the training script where we initialize the SMDDP library.\n",
    "* `scripts/utils.py` - Helper script for defining dataloaders\n",
    "* `scripts/requirements.txt` - List of dependencies required for this example to train on SageMaker\n",
    "\n",
    "*Note: The SMDDP library for accelerated sharded data parallel training is compatible with deep learning containers from PyTorch 2.0 onwards.  Ensure you are using PyTorch >=2.0 for this example.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How optimized GPU communication is enabled with SMDDP in FSDP\n",
    "Enabling the SMDDP library in an existing FSDP training script is seamless.  As shown in `train.py`, the only code modifications required are:\n",
    "* Importing the library: `import smdistributed.dataparallel.torch.torch_smddp`\n",
    "* Creating the process group with `\"smddp\"` backend: `torch.distributed.init_process_group(\"smddp\")`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started\n",
    "\n",
    "First, we'll install some dependencies in our current environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment, you need access to an IAM Role with the required permissions for Sagemaker. You can find more about it [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, datetime\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "As the base dataset, we will use the C4 (https://huggingface.co/datasets/allenai/c4) dataset, but before training the model, we need to preprocess the data. We will create chunks of `2048` tokens ([model max length](https://huggingface.co/EleutherAI/gpt-neox-20b)) to avoid unnecessary padding and computing. \n",
    "\n",
    "The first step is to prepare batches of dataset splits from Hugging Face c4 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_worker_batches(num_of_hf_splits, num_of_workers, token, split_name):\n",
    "    \"\"\"\n",
    "    Create a list of worker batches for processing a dataset.\n",
    "\n",
    "    Args:\n",
    "        num_of_hf_splits (int): The number of Hugging Face dataset splits.\n",
    "        num_of_workers (int): The number of worker processes.\n",
    "        token (str): The token to be replaced in the split name.\n",
    "        split_name (str): The base name of the dataset split files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of worker batches, where each batch is a list of file names.\n",
    "    \"\"\"\n",
    "    batch_size = num_of_hf_splits // num_of_workers\n",
    "\n",
    "    worker_batches = []\n",
    "    for worker_index in range(num_of_workers):\n",
    "        worker_node = []\n",
    "        start = worker_index * batch_size\n",
    "        end = start + batch_size\n",
    "        for batch in range(start, end):\n",
    "            file_name = split_name.replace(token, str(batch).zfill(5))\n",
    "            worker_node.append(file_name)\n",
    "        worker_batches.append(worker_node)\n",
    "\n",
    "    return worker_batches\n",
    "\n",
    "\n",
    "# Example usage\n",
    "num_of_hf_splits = 1024\n",
    "num_of_workers = 200\n",
    "token = \"split_number\"\n",
    "split_name = f\"en/c4-train.{token}-of-01024.json.gz\"\n",
    "\n",
    "worker_batches = create_worker_batches(\n",
    "    num_of_hf_splits, num_of_workers, token, split_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(worker_batches)):\n",
    "    print(worker_batches[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Amazon SageMaker Job parallelism to process the data in parallel with multiple jobs\n",
    "\n",
    "We will begin by spining up multiple SageMaker jobs each processing a batch of split files in parallel and writing it FSx for lustre filesystem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fsx = True\n",
    "\n",
    "if use_fsx:\n",
    "    from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "    file_system_directory_path = \"/56uy3bev/c4\"\n",
    "    file_system_access_mode = \"rw\"\n",
    "    file_system_type = \"FSxLustre\"\n",
    "    train_fs = FileSystemInput(\n",
    "        file_system_id='fs-0335d20f0f69afd9b',\n",
    "        file_system_type=file_system_type,\n",
    "        directory_path=file_system_directory_path,\n",
    "        file_system_access_mode=file_system_access_mode,\n",
    "    )\n",
    "    data_channels = {\"train\": train_fs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_output_path = \"s3://sagemaker-demo-c4/\"\n",
    "\n",
    "base_job_name = f'huggingface-dataset-workertest'\n",
    "\n",
    "job_names=[]\n",
    "\n",
    "#for worker_index in range(len(worker_batches)):\n",
    "for worker_index in range(2):\n",
    "    current_time = datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "\n",
    "    dataset_job_name = f'{base_job_name}-{worker_index}-{current_time}'\n",
    "\n",
    "    print(dataset_job_name)\n",
    "\n",
    "    job_names.append(dataset_job_name)\n",
    "    \n",
    "    # hyperparameters, which are passed into the training job\n",
    "    hyperparameters = {\n",
    "        \"num_proc\": 72,\n",
    "        \"split_range\": ','.join(map(str, worker_batches[worker_index])),\n",
    "        \"job_name\": dataset_job_name\n",
    "    }\n",
    "    \n",
    "    # estimator\n",
    "    estimator = PyTorch(\n",
    "        entry_point=\"data.py\",\n",
    "        max_run=1800,\n",
    "        role=role,\n",
    "        framework_version=\"2.0.1\",\n",
    "        py_version=\"py310\",\n",
    "        source_dir=\"./scripts\",\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.c5.18xlarge\",\n",
    "        volume_size=200,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        disable_output_compression=True,\n",
    "        keep_alive_period_in_seconds=600,\n",
    "        hyperparameters=hyperparameters,\n",
    "        output_path=data_output_path,\n",
    "        subnets=['subnet-0067baa7d7be55e38'],\n",
    "        security_group_ids=['sg-05ffe325d7d90c501']\n",
    "    )\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    estimator.fit(inputs=data_channels, wait=True, job_name=dataset_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can started multiple Sagemaker job, with the `.fit()` method passing our FSx file system path to the data script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "training_job_name = f'huggingface-training-worker-{current_time}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,  # model id from huggingface.co/models\n",
    "    \"dataset_path\": \"/opt/ml/input/data/train\",  # path where sagemaker will save training dataset\n",
    "    \"valid_path\": \"/opt/ml/input/data/valid\",\n",
    "    \"gradient_checkpointing\": True,  # enable gradient checkpointing\n",
    "    \"bf16\": True,  # enable mixed precision training\n",
    "    \"optimizer\": \"adamw_torch\",  # optimizer\n",
    "    \"per_device_train_batch_size\": 1,  # batch size per device during training\n",
    "    \"epochs\": 1,  # number of epochs to train\n",
    "    \"fsdp\": '\"full_shard auto_wrap\"',  # fully sharded data parallelism\n",
    "    \"cache_dir\": \"/opt/ml/sagemaker/warmpoolcache\",  # change this to /tmp if not using warmpools\n",
    "    \"max_steps\": 1000,\n",
    "}\n",
    "\n",
    "# estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    max_run=1800,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    framework_version=\"2.0.1\",\n",
    "    py_version=\"py310\",\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    disable_output_compression=True,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    keep_alive_period_in_seconds=600,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=training_input_path,\n",
    "    subnets=['subnet-0067baa7d7be55e38'],\n",
    "    security_group_ids=['sg-05ffe325d7d90c501']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(inputs=data_channels, wait=True,job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Terminate the warm pool cluster if no longer needed\n",
    "\n",
    "You can terminate the warm pool cluster once finished experimenting to reduce billed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.update_training_job(\n",
    "    estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\": 0}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
