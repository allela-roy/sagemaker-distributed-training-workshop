{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare C4 dataset and Pre-train Falcon model using PyTorch Fully Sharded Data Parallelism (FSDP)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/training|distributed_training|pytorch|data_parallel|fully_sharded_data_parallel|falcon|smddp_fsdp_example.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, we will show how to train or fine-tune [Falcon-7B-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) on the [GLUE/SST2](https://huggingface.co/datasets/glue/viewer/sst2/train) dataset.  We will use 2 p4d.24xlarge instances, which come with 8 NVIDIA A100 40GB GPUs along with the PyTorch Fully Sharded Data Parallelism (FSDP) technique to efficiently train this large model with limited GPU memory.  \n",
    "\n",
    "To accelerate training speed, we will also use the **SageMaker Distributed Data Parallel Library (SMDDP)** which speeds up GPU communication across P4d instances during sharded data parallel training.  \n",
    "\n",
    "## Files\n",
    "* `scripts/data.py` - Script to download, tokenize and group into a specific sequence length.\n",
    "* `scripts/train.py` - The entry point for the training script where we initialize the SMDDP library.\n",
    "* `scripts/utils.py` - Helper script for defining dataloaders\n",
    "* `scripts/requirements.txt` - List of dependencies required for this example to train on SageMaker\n",
    "\n",
    "*Note: The SMDDP library for accelerated sharded data parallel training is compatible with deep learning containers from PyTorch 2.0 onwards.  Ensure you are using PyTorch >=2.0 for this example.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How optimized GPU communication is enabled with SMDDP in FSDP\n",
    "Enabling the SMDDP library in an existing FSDP training script is seamless.  As shown in `train.py`, the only code modifications required are:\n",
    "* Importing the library: `import smdistributed.dataparallel.torch.torch_smddp`\n",
    "* Creating the process group with `\"smddp\"` backend: `torch.distributed.init_process_group(\"smddp\")`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started\n",
    "\n",
    "First, we'll install some dependencies in our current environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment, you need access to an IAM Role with the required permissions for Sagemaker. You can find more about it [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::015476483300:role/service-role/AmazonSageMaker-ExecutionRole-20240126T111548\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, datetime, time\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "As the base dataset, we will use the C4 (https://huggingface.co/datasets/allenai/c4) dataset, but before training the model, we need to preprocess the data. We will create chunks of `2048` tokens ([model max length](https://huggingface.co/EleutherAI/gpt-neox-20b)) to avoid unnecessary padding and computing. \n",
    "\n",
    "The first step is to prepare batches of dataset splits from Hugging Face c4 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,68',\n",
       " '68,136',\n",
       " '136,204',\n",
       " '204,272',\n",
       " '272,340',\n",
       " '340,408',\n",
       " '408,476',\n",
       " '476,544',\n",
       " '544,612',\n",
       " '612,680',\n",
       " '680,748',\n",
       " '748,816',\n",
       " '816,884',\n",
       " '884,952',\n",
       " '952,1020']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_worker_range(num_of_hf_splits, num_of_workers):\n",
    "    \"\"\"\n",
    "    Create a list of worker batches for processing a dataset.\n",
    "\n",
    "    Args:\n",
    "        num_of_hf_splits (int): The number of Hugging Face dataset splits.\n",
    "        num_of_workers (int): The number of worker processes.\n",
    "        token (str): The token to be replaced in the split name.\n",
    "        split_name (str): The base name of the dataset split files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of worker batches, where each batch is a list of file names.\n",
    "    \"\"\"\n",
    "    batch_size = num_of_hf_splits // num_of_workers\n",
    "\n",
    "    worker_range=[]\n",
    "    for worker_index in range(num_of_workers):\n",
    "        start = worker_index * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        worker_range.append(f\"{start},{end}\")\n",
    "\n",
    "    return worker_range\n",
    "\n",
    "# Example usage\n",
    "num_of_hf_splits = 1024\n",
    "num_of_workers = 15\n",
    "\n",
    "worker_split_range = create_worker_range(\n",
    "    num_of_hf_splits, num_of_workers\n",
    ")\n",
    "\n",
    "worker_split_range"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Amazon SageMaker Job parallelism to process the data in parallel with multiple jobs\n",
    "\n",
    "We will begin by spining up multiple SageMaker jobs each processing a batch of split files in parallel and writing it FSx for lustre filesystem\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "2dde7079-ae12-46a6-aa81-c020e6d3ae46.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:2dde7079-ae12-46a6-aa81-c020e6d3ae46.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fsx = True\n",
    "\n",
    "if use_fsx:\n",
    "    from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "    file_system_directory_path = \"/56uy3bev/c4\"\n",
    "    file_system_access_mode = \"rw\"\n",
    "    file_system_type = \"FSxLustre\"\n",
    "    train_fs = FileSystemInput(\n",
    "        file_system_id='fs-0335d20f0f69afd9b',\n",
    "        file_system_type=file_system_type,\n",
    "        directory_path=file_system_directory_path,\n",
    "        file_system_access_mode=file_system_access_mode,\n",
    "    )\n",
    "    data_channels = {\"train\": train_fs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface-dataset-workertest-0-21-55-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-0-21-55-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-21 21:55:29 Starting - Starting the training job...\n",
      "2024-04-21 21:55:54 Starting - Preparing the instances for training...\n",
      "2024-04-21 21:56:29 Downloading - Downloading input data........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:39,485 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:39,486 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:39,486 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:39,496 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:39,497 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:40,827 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.33.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.9/119.9 kB 5.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.21 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting einops (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (0.19.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (1.26.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (2023.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (14.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.9.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 86.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 56.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 19.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading einops-0.7.0-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 kB 7.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 57.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 117.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 35.2 MB/s eta 0:00:00\u001b[0m\n",
      "\n",
      "2024-04-21 21:57:39 Training - Training image download completed. Training in progress.\u001b[34mInstalling collected packages: tokenizers, xxhash, pyarrow-hotfix, einops, huggingface-hub, transformers, bitsandbytes, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.15.0\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.19.4\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.19.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.19.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.35.2\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.35.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.35.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed bitsandbytes-0.43.1 datasets-2.19.0 einops-0.7.0 huggingface-hub-0.22.2 pyarrow-hotfix-0.6 tokenizers-0.13.3 transformers-4.33.0 xxhash-3.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,381 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,381 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,382 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,383 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,394 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,394 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,404 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,405 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,415 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"job_name\": \"huggingface-dataset-workertest-0-21-55-28\",\n",
      "        \"model_id\": \"tiiuae/falcon-7b\",\n",
      "        \"num_proc\": 96,\n",
      "        \"split_range\": \"0,68\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"huggingface-dataset-workertest-0-21-55-28\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-demo-c4/huggingface-dataset-workertest-0-21-55-28/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"data\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"data.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"job_name\":\"huggingface-dataset-workertest-0-21-55-28\",\"model_id\":\"tiiuae/falcon-7b\",\"num_proc\":96,\"split_range\":\"0,68\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=data.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=data\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-demo-c4/huggingface-dataset-workertest-0-21-55-28/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"job_name\":\"huggingface-dataset-workertest-0-21-55-28\",\"model_id\":\"tiiuae/falcon-7b\",\"num_proc\":96,\"split_range\":\"0,68\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"huggingface-dataset-workertest-0-21-55-28\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-demo-c4/huggingface-dataset-workertest-0-21-55-28/source/sourcedir.tar.gz\",\"module_name\":\"data\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"data.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--job_name\",\"huggingface-dataset-workertest-0-21-55-28\",\"--model_id\",\"tiiuae/falcon-7b\",\"--num_proc\",\"96\",\"--split_range\",\"0,68\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_JOB_NAME=huggingface-dataset-workertest-0-21-55-28\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=tiiuae/falcon-7b\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_PROC=96\u001b[0m\n",
      "\u001b[34mSM_HP_SPLIT_RANGE=0,68\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 data.py --job_name huggingface-dataset-workertest-0-21-55-28 --model_id tiiuae/falcon-7b --num_proc 96 --split_range 0,68\u001b[0m\n",
      "\u001b[34m2024-04-21 21:57:54,440 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34msplit batch:['en/c4-train.00000-of-01024.json.gz', 'en/c4-train.00001-of-01024.json.gz', 'en/c4-train.00002-of-01024.json.gz', 'en/c4-train.00003-of-01024.json.gz', 'en/c4-train.00004-of-01024.json.gz', 'en/c4-train.00005-of-01024.json.gz', 'en/c4-train.00006-of-01024.json.gz', 'en/c4-train.00007-of-01024.json.gz', 'en/c4-train.00008-of-01024.json.gz', 'en/c4-train.00009-of-01024.json.gz', 'en/c4-train.00010-of-01024.json.gz', 'en/c4-train.00011-of-01024.json.gz', 'en/c4-train.00012-of-01024.json.gz', 'en/c4-train.00013-of-01024.json.gz', 'en/c4-train.00014-of-01024.json.gz', 'en/c4-train.00015-of-01024.json.gz', 'en/c4-train.00016-of-01024.json.gz', 'en/c4-train.00017-of-01024.json.gz', 'en/c4-train.00018-of-01024.json.gz', 'en/c4-train.00019-of-01024.json.gz', 'en/c4-train.00020-of-01024.json.gz', 'en/c4-train.00021-of-01024.json.gz', 'en/c4-train.00022-of-01024.json.gz', 'en/c4-train.00023-of-01024.json.gz', 'en/c4-train.00024-of-01024.json.gz', 'en/c4-train.00025-of-01024.json.gz', 'en/c4-train.00026-of-01024.json.gz', 'en/c4-train.00027-of-01024.json.gz', 'en/c4-train.00028-of-01024.json.gz', 'en/c4-train.00029-of-01024.json.gz', 'en/c4-train.00030-of-01024.json.gz', 'en/c4-train.00031-of-01024.json.gz', 'en/c4-train.00032-of-01024.json.gz', 'en/c4-train.00033-of-01024.json.gz', 'en/c4-train.00034-of-01024.json.gz', 'en/c4-train.00035-of-01024.json.gz', 'en/c4-train.00036-of-01024.json.gz', 'en/c4-train.00037-of-01024.json.gz', 'en/c4-train.00038-of-01024.json.gz', 'en/c4-train.00039-of-01024.json.gz', 'en/c4-train.00040-of-01024.json.gz', 'en/c4-train.00041-of-01024.json.gz', 'en/c4-train.00042-of-01024.json.gz', 'en/c4-train.00043-of-01024.json.gz', 'en/c4-train.00044-of-01024.json.gz', 'en/c4-train.00045-of-01024.json.gz', 'en/c4-train.00046-of-01024.json.gz', 'en/c4-train.00047-of-01024.json.gz', 'en/c4-train.00048-of-01024.json.gz', 'en/c4-train.00049-of-01024.json.gz', 'en/c4-train.00050-of-01024.json.gz', 'en/c4-train.00051-of-01024.json.gz', 'en/c4-train.00052-of-01024.json.gz', 'en/c4-train.00053-of-01024.json.gz', 'en/c4-train.00054-of-01024.json.gz', 'en/c4-train.00055-of-01024.json.gz', 'en/c4-train.00056-of-01024.json.gz', 'en/c4-train.00057-of-01024.json.gz', 'en/c4-train.00058-of-01024.json.gz', 'en/c4-train.00059-of-01024.json.gz', 'en/c4-train.00060-of-01024.json.gz', 'en/c4-train.00061-of-01024.json.gz', 'en/c4-train.00062-of-01024.json.gz', 'en/c4-train.00063-of-01024.json.gz', 'en/c4-train.00064-of-01024.json.gz', 'en/c4-train.00065-of-01024.json.gz', 'en/c4-train.00066-of-01024.json.gz', 'en/c4-train.00067-of-01024.json.gz']\u001b[0m\n",
      "\u001b[34mDownloading readme:   0%|          | 0.00/41.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading readme: 100%|██████████| 41.1k/41.1k [00:00<00:00, 85.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/319M [00:00<00:06, 48.9MB/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/319M [00:00<00:01, 147MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/319M [00:00<00:01, 205MB/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/319M [00:00<00:00, 251MB/s] #033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/319M [00:00<00:00, 252MB/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/319M [00:00<00:00, 262MB/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/319M [00:00<00:00, 265MB/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/319M [00:01<00:00, 267MB/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/320M [00:00<?, ?B/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/320M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  86%|████████▌ | 273M/319M [00:01<00:00, 255MB/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/320M [00:00<00:07, 41.2MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/320M [00:00<00:06, 49.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▌| 304M/319M [00:01<00:00, 218MB/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/318M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/320M [00:00<00:03, 77.0MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/320M [00:00<00:04, 62.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:01<00:00, 215MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/320M [00:00<00:03, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/320M [00:00<00:03, 74.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/319M [00:00<00:06, 46.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/318M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/320M [00:00<00:03, 74.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/320M [00:00<00:03, 73.6MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/319M [00:00<00:05, 54.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/320M [00:00<00:03, 70.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/319M [00:00<00:04, 61.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/320M [00:00<00:04, 60.2MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/320M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/318M [00:00<00:09, 32.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/320M [00:00<00:03, 67.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:01<00:00, 172MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/319M [00:00<00:04, 57.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/320M [00:01<00:04, 57.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/318M [00:00<00:08, 34.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/320M [00:01<00:05, 47.7MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/318M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/319M [00:00<00:04, 55.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/318M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/318M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▌       | 83.9M/320M [00:01<00:04, 56.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/320M [00:00<00:14, 21.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/319M [00:00<00:08, 37.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/318M [00:00<00:08, 35.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/319M [00:01<00:05, 50.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/318M [00:00<00:05, 52.0MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/318M [00:01<00:38, 8.04MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▌       | 83.9M/320M [00:01<00:05, 39.7MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/319M [00:00<00:03, 83.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/320M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/318M [00:00<00:02, 117MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  29%|██▉       | 94.4M/320M [00:01<00:04, 49.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/318M [00:00<00:13, 23.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/319M [00:00<00:13, 23.2MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/318M [00:01<00:18, 16.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/320M [00:00<00:11, 25.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/319M [00:01<00:05, 47.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/319M [00:00<00:02, 99.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/318M [00:01<00:07, 35.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/320M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/320M [00:00<00:05, 55.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/318M [00:00<00:02, 119MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/320M [00:01<00:05, 38.5MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/318M [00:00<00:08, 33.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/318M [00:00<00:15, 19.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/320M [00:00<00:01, 148MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/320M [00:01<00:05, 43.0MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/318M [00:01<00:12, 22.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/319M [00:00<00:10, 27.8MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/320M [00:00<00:07, 39.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/320M [00:00<00:01, 162MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/319M [00:01<00:05, 41.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/319M [00:00<00:03, 79.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/320M [00:01<00:11, 26.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/318M [00:00<00:10, 29.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/318M [00:00<00:02, 81.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/318M [00:01<00:08, 31.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/320M [00:00<00:01, 191MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/318M [00:00<00:08, 34.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/320M [00:02<00:04, 42.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/320M [00:02<00:06, 34.3MB/s] #033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/318M [00:02<00:10, 27.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/320M [00:00<00:06, 44.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/319M [00:01<00:03, 72.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/319M [00:01<00:10, 28.6MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/318M [00:01<00:03, 71.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/318M [00:01<00:08, 33.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/319M [00:02<00:05, 37.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/320M [00:01<00:10, 27.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/320M [00:00<00:01, 139MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/318M [00:01<00:08, 33.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/319M [00:01<00:03, 62.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/320M [00:00<00:06, 41.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/318M [00:02<00:09, 28.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/318M [00:02<00:08, 29.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/319M [00:00<00:08, 35.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/320M [00:02<00:06, 31.0MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/318M [00:01<00:03, 57.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/319M [00:01<00:38, 8.07MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/318M [00:01<00:08, 33.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/319M [00:01<00:09, 28.0MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/319M [00:02<00:06, 35.1MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/318M [00:01<00:08, 32.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/318M [00:00<?, ?B/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/320M [00:01<00:07, 37.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/319M [00:01<00:04, 49.9MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/320M [00:01<00:01, 99.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/320M [00:02<00:10, 25.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/318M [00:02<00:09, 27.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/319M [00:00<00:09, 30.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/318M [00:00<?, ?B/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/318M [00:00<00:05, 56.6MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/318M [00:02<00:09, 26.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/318M [00:01<00:05, 41.1MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/319M [00:01<00:09, 26.7MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/319M [00:01<00:23, 12.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/318M [00:01<00:09, 27.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/319M [00:02<00:07, 28.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   3%|▎         | 10.5M/318M [00:00<00:07, 39.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/319M [00:01<00:09, 28.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  52%|█████▏    | 168M/320M [00:01<00:02, 66.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/320M [00:02<00:11, 23.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/318M [00:02<00:09, 25.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/320M [00:01<00:11, 23.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/318M [00:02<00:11, 22.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/319M [00:02<00:07, 29.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/318M [00:00<00:12, 23.1MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/319M [00:02<00:10, 24.2MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/318M [00:03<00:12, 20.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/320M [00:02<00:02, 55.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/318M [00:02<00:07, 28.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/319M [00:02<00:20, 13.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:   7%|▋         | 21.0M/318M [00:00<00:14, 20.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/318M [00:02<00:12, 20.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/319M [00:01<00:13, 20.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/320M [00:02<00:02, 46.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/320M [00:02<00:12, 20.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/318M [00:01<00:13, 20.5MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/318M [00:03<00:12, 19.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/320M [00:03<00:14, 17.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/318M [00:03<00:12, 18.6MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/318M [00:04<00:12, 18.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  62%|██████▏   | 199M/320M [00:02<00:02, 41.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/319M [00:03<00:13, 18.1MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/318M [00:03<00:13, 18.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:05<00:00, 60.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/319M [00:03<00:20, 13.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/320M [00:03<00:12, 19.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  10%|▉         | 31.5M/318M [00:01<00:18, 15.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/318M [00:01<00:14, 19.3MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/318M [00:03<00:12, 18.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/320M [00:03<00:03, 35.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/319M [00:02<00:16, 15.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/318M [00:04<00:13, 17.0MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/318M [00:03<00:12, 18.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/320M [00:03<00:02, 33.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▌       | 83.9M/320M [00:04<00:15, 15.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/318M [00:04<00:13, 15.3MB/s] #033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▌       | 83.9M/320M [00:03<00:12, 19.3MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/318M [00:02<00:14, 18.7MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/319M [00:04<00:15, 15.5MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/318M [00:04<00:11, 18.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/320M [00:03<00:02, 34.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/319M [00:04<00:21, 12.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  13%|█▎        | 41.9M/318M [00:02<00:19, 14.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/318M [00:05<00:12, 17.7MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/319M [00:03<00:16, 15.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/318M [00:04<00:12, 18.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  29%|██▉       | 94.4M/320M [00:04<00:11, 19.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  75%|███████▌  | 241M/320M [00:04<00:02, 33.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/320M [00:05<00:14, 15.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▋      | 115M/318M [00:05<00:13, 15.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/319M [00:04<00:13, 16.1MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/319M [00:04<00:17, 14.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  16%|█▋        | 52.4M/318M [00:03<00:15, 16.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/318M [00:04<00:12, 17.6MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/318M [00:03<00:15, 16.6MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/319M [00:03<00:14, 17.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/319M [00:05<00:11, 18.8MB/s] #033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  20%|█▉        | 62.9M/318M [00:03<00:12, 19.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/319M [00:05<00:14, 17.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/319M [00:04<00:11, 20.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▊  | 252M/320M [00:04<00:02, 25.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/318M [00:06<00:13, 14.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/320M [00:04<00:12, 16.7MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/318M [00:05<00:11, 17.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/318M [00:05<00:13, 15.4MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/318M [00:03<00:14, 17.1MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/319M [00:05<00:10, 19.1MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  23%|██▎       | 73.4M/318M [00:03<00:12, 19.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/319M [00:05<00:13, 18.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/320M [00:06<00:16, 13.1MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/320M [00:05<00:10, 19.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▋      | 115M/318M [00:05<00:11, 17.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/318M [00:04<00:11, 19.5MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/318M [00:04<00:08, 25.6MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/319M [00:05<00:13, 16.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  26%|██▋       | 83.9M/318M [00:04<00:11, 19.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/319M [00:06<00:12, 18.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  30%|██▉       | 94.4M/318M [00:04<00:08, 25.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/319M [00:06<00:08, 24.4MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/320M [00:07<00:34, 5.71MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  40%|███▉      | 126M/318M [00:07<00:18, 10.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/320M [00:06<00:16, 12.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▌      | 115M/319M [00:06<00:07, 27.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/318M [00:04<00:07, 28.5MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/318M [00:05<00:10, 20.9MB/s] #033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/320M [00:08<00:34, 5.62MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  40%|███▉      | 126M/318M [00:07<00:16, 11.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  33%|███▎      | 105M/319M [00:05<00:13, 15.8MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/320M [00:08<00:26, 7.04MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/319M [00:07<00:15, 12.4MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/318M [00:07<00:15, 11.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/320M [00:06<00:04, 12.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  36%|███▋      | 115M/318M [00:05<00:09, 20.8MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/320M [00:08<00:17, 9.71MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/318M [00:08<00:13, 13.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/319M [00:07<00:11, 16.2MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/320M [00:08<00:12, 12.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/318M [00:08<00:11, 14.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/320M [00:09<00:27, 6.68MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  85%|████████▌ | 273M/320M [00:07<00:03, 14.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/318M [00:08<00:10, 16.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/319M [00:07<00:09, 18.9MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  52%|█████▏    | 168M/320M [00:09<00:09, 15.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/318M [00:08<00:09, 17.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▊ | 283M/320M [00:07<00:01, 18.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  40%|███▉      | 126M/318M [00:07<00:20, 9.19MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/320M [00:09<00:19, 9.07MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/319M [00:08<00:08, 19.6MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/320M [00:09<00:08, 17.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/318M [00:09<00:09, 16.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/320M [00:07<00:01, 19.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/318M [00:09<00:08, 17.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/320M [00:09<00:05, 22.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  40%|███▉      | 126M/318M [00:06<00:10, 17.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/318M [00:09<00:06, 21.7MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/318M [00:08<00:16, 10.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/319M [00:08<00:06, 23.6MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▌| 304M/320M [00:08<00:00, 24.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/319M [00:08<00:15, 12.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/318M [00:06<00:08, 21.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/318M [00:08<00:11, 14.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/319M [00:08<00:05, 27.8MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/319M [00:08<00:10, 16.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/320M [00:09<00:03, 32.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/318M [00:08<00:08, 18.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/318M [00:09<00:07, 19.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:08<00:00, 38.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  98%|█████████▊| 315M/320M [00:08<00:00, 26.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/319M [00:08<00:04, 31.1MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/318M [00:07<00:07, 22.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/320M [00:08<00:24, 7.91MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/318M [00:09<00:05, 24.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/318M [00:09<00:06, 22.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/319M [00:09<00:09, 17.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/318M [00:07<00:07, 22.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:09<00:00, 19.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  63%|██████▎   | 199M/318M [00:10<00:05, 23.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/320M [00:10<00:20, 8.11MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/318M [00:09<00:06, 22.4MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  63%|██████▎   | 199M/319M [00:09<00:04, 25.4MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/319M [00:09<00:07, 21.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/318M [00:10<00:09, 15.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:09<00:00, 34.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/320M [00:08<00:19, 9.55MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/320M [00:09<00:13, 12.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/318M [00:07<00:06, 24.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/319M [00:09<00:03, 28.6MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/320M [00:11<00:04, 20.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  52%|█████▏    | 168M/320M [00:11<00:14, 10.1MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/318M [00:09<00:05, 23.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/318M [00:10<00:05, 20.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/319M [00:09<00:07, 19.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/318M [00:11<00:08, 15.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/319M [00:10<00:04, 23.9MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/320M [00:09<00:12, 13.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/320M [00:11<00:12, 11.6MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/318M [00:08<00:07, 19.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/319M [00:10<00:06, 20.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/320M [00:11<00:05, 17.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  52%|█████▏    | 168M/320M [00:10<00:08, 17.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/318M [00:11<00:04, 19.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/319M [00:10<00:03, 25.9MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  63%|██████▎   | 199M/318M [00:10<00:06, 18.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  63%|██████▎   | 199M/318M [00:11<00:07, 15.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/319M [00:09<00:23, 8.19MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/318M [00:09<00:06, 20.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/320M [00:12<00:09, 13.3MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  75%|███████▌  | 241M/320M [00:12<00:04, 18.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/318M [00:11<00:05, 20.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/319M [00:11<00:03, 25.5MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/319M [00:10<00:06, 20.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/318M [00:12<00:05, 18.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/319M [00:10<00:17, 10.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  73%|███████▎  | 231M/318M [00:11<00:04, 20.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  63%|██████▎   | 199M/318M [00:09<00:04, 24.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  62%|██████▏   | 199M/320M [00:12<00:06, 17.3MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/320M [00:10<00:08, 16.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:13<00:00, 24.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▊  | 252M/320M [00:12<00:03, 20.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/318M [00:12<00:03, 23.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/320M [00:11<00:06, 20.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/318M [00:11<00:04, 20.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/319M [00:11<00:03, 22.3MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/319M [00:10<00:14, 11.5MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/318M [00:09<00:04, 23.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/320M [00:13<00:06, 18.0MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/318M [00:12<00:05, 18.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/320M [00:13<00:02, 22.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/318M [00:12<00:02, 22.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  62%|██████▏   | 199M/319M [00:11<00:07, 16.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/319M [00:12<00:02, 24.2MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/319M [00:11<00:11, 13.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/320M [00:13<00:05, 19.7MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/318M [00:10<00:04, 22.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/318M [00:13<00:04, 18.6MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  85%|████████▌ | 273M/320M [00:13<00:02, 21.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/318M [00:12<00:04, 18.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  62%|██████▏   | 199M/320M [00:11<00:07, 17.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/318M [00:13<00:02, 22.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/319M [00:12<00:06, 16.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  86%|████████▌ | 273M/319M [00:12<00:02, 21.9MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/320M [00:12<00:05, 20.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  88%|████████▊ | 283M/320M [00:13<00:01, 24.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/319M [00:11<00:10, 14.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:14<00:00, 22.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  73%|███████▎  | 231M/318M [00:10<00:03, 22.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:11<00:00, 28.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/320M [00:14<00:04, 19.0MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/319M [00:12<00:04, 21.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/318M [00:13<00:04, 19.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  86%|████████▌ | 273M/318M [00:13<00:02, 22.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▉ | 283M/319M [00:13<00:01, 23.8MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/318M [00:13<00:04, 17.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/320M [00:14<00:01, 25.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/318M [00:11<00:03, 24.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/319M [00:11<00:08, 17.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/320M [00:12<00:04, 21.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  40%|███▉      | 126M/318M [00:13<00:48, 4.01MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▉ | 283M/318M [00:13<00:01, 25.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  75%|███████▌  | 241M/320M [00:14<00:03, 20.0MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/319M [00:13<00:04, 21.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/320M [00:12<00:03, 26.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/318M [00:11<00:02, 26.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/318M [00:14<00:03, 18.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▍| 304M/320M [00:14<00:00, 23.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/319M [00:12<00:07, 18.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/318M [00:13<00:03, 17.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  75%|███████▌  | 241M/320M [00:13<00:02, 26.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/319M [00:13<00:01, 19.5MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/318M [00:14<00:01, 22.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/318M [00:12<00:02, 26.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▊  | 252M/320M [00:15<00:03, 19.6MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/318M [00:13<00:34, 5.20MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  75%|███████▌  | 241M/319M [00:13<00:03, 20.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/318M [00:14<00:02, 20.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:15<00:00, 20.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  62%|██████▏   | 199M/319M [00:12<00:05, 20.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▊  | 252M/320M [00:13<00:02, 29.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:14<00:00, 21.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/318M [00:13<00:24, 7.13MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:14<00:00, 21.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  98%|█████████▊| 315M/320M [00:15<00:00, 22.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  96%|█████████▌| 304M/318M [00:14<00:00, 24.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/318M [00:14<00:03, 18.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▌| 304M/319M [00:14<00:00, 21.6MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:15<00:00, 20.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  86%|████████▌ | 273M/318M [00:15<00:02, 22.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/320M [00:14<00:53, 3.61MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:15<00:00, 22.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/320M [00:13<00:02, 27.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  86%|████████▌ | 273M/318M [00:12<00:02, 22.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/320M [00:15<00:03, 18.2MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:15<00:00, 20.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/319M [00:13<00:05, 19.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:15<00:00, 20.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  86%|████████▌ | 273M/318M [00:14<00:02, 19.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:14<00:00, 21.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/319M [00:14<00:03, 17.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  99%|█████████▊| 315M/319M [00:14<00:00, 21.9MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/318M [00:14<00:18, 8.61MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  99%|█████████▉| 315M/318M [00:15<00:00, 23.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  43%|████▎     | 136M/320M [00:15<00:37, 4.92MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:14<00:00, 21.9MB/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:14<00:00, 21.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▉ | 283M/318M [00:15<00:01, 20.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  85%|████████▌ | 273M/320M [00:14<00:01, 26.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/319M [00:13<00:04, 21.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  85%|████████▌ | 273M/320M [00:16<00:02, 19.7MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/318M [00:14<00:13, 11.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:15<00:00, 20.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:15<00:00, 20.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▉ | 283M/318M [00:13<00:01, 21.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▉ | 283M/318M [00:14<00:01, 21.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:15<00:00, 20.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:16<00:00, 19.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:15<00:00, 20.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/320M [00:15<00:26, 6.46MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/319M [00:14<00:03, 18.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/318M [00:16<00:01, 22.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  88%|████████▊ | 283M/320M [00:14<00:01, 26.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:15<00:00, 20.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:15<00:00, 21.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/318M [00:15<00:10, 13.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:16<00:00, 19.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/319M [00:14<00:03, 22.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▊ | 283M/320M [00:16<00:01, 20.8MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/318M [00:13<00:01, 22.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:15<00:00, 20.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/318M [00:15<00:01, 22.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▌| 304M/318M [00:16<00:00, 24.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/320M [00:15<00:00, 27.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/320M [00:15<00:19, 8.23MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/320M [00:16<00:01, 24.2MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  96%|█████████▌| 304M/318M [00:13<00:00, 26.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/318M [00:15<00:07, 16.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  85%|████████▌ | 273M/319M [00:15<00:02, 19.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/319M [00:14<00:03, 23.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:16<00:00, 18.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 321M/321M [00:15<00:00, 20.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:17<00:00, 18.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 19.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:16<00:00, 19.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▍| 304M/320M [00:15<00:00, 29.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 19.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▌| 304M/318M [00:15<00:00, 21.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  99%|█████████▉| 315M/318M [00:16<00:00, 24.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  99%|█████████▉| 315M/318M [00:14<00:00, 27.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▊ | 283M/319M [00:15<00:01, 20.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  63%|██████▎   | 199M/318M [00:15<00:06, 18.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:14<00:00, 21.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▌| 304M/320M [00:17<00:00, 23.8MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/319M [00:14<00:02, 25.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  52%|█████▏    | 168M/320M [00:16<00:15, 10.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:15<00:00, 20.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:14<00:00, 26.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:14<00:00, 22.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  98%|█████████▊| 315M/320M [00:15<00:00, 31.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  99%|█████████▉| 315M/318M [00:16<00:00, 26.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:17<00:00, 21.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:17<00:00, 18.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/319M [00:15<00:01, 29.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/318M [00:16<00:04, 21.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/320M [00:16<00:10, 13.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:14<00:00, 21.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:16<00:00, 24.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:16<00:00, 19.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:17<00:00, 18.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/319M [00:16<00:01, 22.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  98%|█████████▊| 315M/320M [00:17<00:00, 24.2MB/s]\u001b[0m\n",
      "\u001b[34m#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/318M [00:16<00:03, 26.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  85%|████████▌ | 273M/319M [00:15<00:01, 32.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/320M [00:16<00:07, 16.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:17<00:00, 25.7MB/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:16<00:00, 23.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  73%|███████▎  | 231M/318M [00:16<00:02, 32.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▊ | 283M/319M [00:15<00:00, 37.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:18<00:00, 17.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 19.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:16<00:00, 19.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  62%|██████▏   | 199M/320M [00:17<00:05, 21.0MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:18<00:00, 17.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:18<00:00, 17.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▌| 304M/319M [00:16<00:00, 23.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 19.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/318M [00:16<00:02, 36.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:15<00:00, 20.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/320M [00:17<00:04, 26.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/319M [00:15<00:00, 40.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:14<00:00, 22.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  98%|█████████▊| 315M/319M [00:16<00:00, 29.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 19.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 19.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:16<00:00, 19.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/318M [00:16<00:01, 41.3MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 321M/321M [00:17<00:00, 17.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 29.9MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:18<00:00, 17.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  69%|██████▉   | 220M/320M [00:17<00:03, 30.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  95%|█████████▌| 304M/319M [00:16<00:00, 41.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 18.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/318M [00:17<00:01, 46.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:15<00:00, 20.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:16<00:00, 19.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:15<00:00, 20.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  99%|█████████▊| 315M/319M [00:16<00:00, 48.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:17<00:00, 18.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  86%|████████▌ | 273M/318M [00:17<00:00, 54.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 19.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  75%|███████▌  | 241M/320M [00:17<00:01, 45.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:16<00:00, 19.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/318M [00:17<00:00, 76.8MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/320M [00:17<00:00, 62.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:18<00:00, 17.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  99%|█████████▉| 315M/318M [00:17<00:00, 98.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▊ | 283M/320M [00:17<00:00, 82.4MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:17<00:00, 18.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:18<00:00, 17.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  98%|█████████▊| 315M/320M [00:18<00:00, 115MB/s] #033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:18<00:00, 17.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 320M/320M [00:17<00:00, 18.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/319M [00:19<01:38, 1.96MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/319M [00:19<00:36, 4.46MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  59%|█████▉    | 189M/319M [00:19<00:16, 7.96MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/319M [00:20<00:06, 14.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  85%|████████▌ | 273M/319M [00:20<00:02, 23.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  99%|█████████▊| 315M/319M [00:20<00:00, 35.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:20<00:00, 15.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:18<00:00, 17.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  39%|███▉      | 126M/319M [00:21<01:42, 1.89MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/319M [00:21<00:39, 4.11MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/319M [00:21<00:22, 6.18MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/319M [00:21<00:10, 10.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  79%|███████▉  | 252M/319M [00:21<00:03, 18.5MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  89%|████████▉ | 283M/319M [00:21<00:01, 26.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:22<00:00, 38.7MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:22<00:00, 14.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  40%|███▉      | 126M/318M [00:22<01:46, 1.81MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  46%|████▌     | 147M/318M [00:22<00:53, 3.24MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  40%|███▉      | 126M/318M [00:20<01:30, 2.11MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  53%|█████▎    | 168M/318M [00:22<00:28, 5.22MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  49%|████▉     | 157M/318M [00:20<00:33, 4.81MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  63%|██████▎   | 199M/318M [00:22<00:12, 9.44MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  56%|█████▌    | 178M/318M [00:21<00:19, 7.31MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  72%|███████▏  | 231M/318M [00:22<00:05, 15.2MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  66%|██████▌   | 210M/318M [00:21<00:08, 12.5MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  82%|████████▏ | 262M/318M [00:22<00:02, 23.1MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  76%|███████▌  | 241M/318M [00:21<00:03, 19.6MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  92%|█████████▏| 294M/318M [00:22<00:00, 33.6MB/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data:  86%|████████▌ | 273M/318M [00:21<00:01, 29.2MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 319M/319M [00:22<00:00, 14.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:22<00:00, 13.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading data:  96%|█████████▌| 304M/318M [00:21<00:00, 41.9MB/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 100%|██████████| 318M/318M [00:21<00:00, 14.8MB/s]\u001b[0m\n",
      "\u001b[34mSetting num_proc from 96 to 68 for the train split as it only contains 68 shards.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4435 examples [00:00, 9665.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 36613 examples [00:00, 83312.58 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 73385 examples [00:00, 154583.74 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 114918 examples [00:00, 220389.78 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 150885 examples [00:00, 257960.79 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 187599 examples [00:00, 287431.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 233534 examples [00:01, 334405.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 273969 examples [00:01, 338914.27 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 319765 examples [00:01, 366850.15 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 361656 examples [00:01, 370481.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 407039 examples [00:01, 379950.33 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 447931 examples [00:01, 383199.10 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 488730 examples [00:01, 381166.64 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 538730 examples [00:01, 406675.42 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 583381 examples [00:01, 413176.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 628739 examples [00:02, 391744.82 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 669766 examples [00:02, 389093.13 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1138782 examples [00:02, 1564531.39 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1306348 examples [00:02, 879833.34 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1434100 examples [00:02, 909406.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1556503 examples [00:03, 578007.20 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1652238 examples [00:03, 572631.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1733917 examples [00:03, 590814.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1815090 examples [00:03, 620523.10 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1891879 examples [00:03, 601743.26 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1964941 examples [00:03, 605704.49 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2056148 examples [00:04, 651579.75 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2128989 examples [00:04, 562216.55 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2192762 examples [00:04, 529147.56 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2256566 examples [00:04, 550455.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2329016 examples [00:04, 590043.16 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2393344 examples [00:04, 398466.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2443880 examples [00:04, 405716.98 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2536301 examples [00:05, 514429.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2618011 examples [00:05, 583249.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2686041 examples [00:05, 606434.54 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2754375 examples [00:05, 576762.70 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2818610 examples [00:05, 586290.57 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2882427 examples [00:05, 583754.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2945941 examples [00:05, 582906.69 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3009273 examples [00:05, 540411.94 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3072817 examples [00:05, 536414.07 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3131747 examples [00:06, 345876.44 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3182448 examples [00:06, 369047.80 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3228004 examples [00:06, 367513.92 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3277903 examples [00:06, 393268.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3323319 examples [00:06, 323351.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3365116 examples [00:07, 298277.05 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3411174 examples [00:07, 331169.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3451915 examples [00:07, 290875.10 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3487726 examples [00:07, 283372.20 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3519842 examples [00:07, 232227.47 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3565190 examples [00:07, 275512.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3597100 examples [00:07, 283158.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3628945 examples [00:08, 185002.69 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3660683 examples [00:08, 201271.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3720337 examples [00:08, 277328.53 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3756589 examples [00:08, 277490.44 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3801950 examples [00:08, 314611.01 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3865539 examples [00:08, 357141.44 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3906159 examples [00:09, 244997.54 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3946948 examples [00:09, 265733.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3997019 examples [00:09, 309989.31 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4042387 examples [00:09, 325256.31 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4079552 examples [00:09, 225361.31 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4111426 examples [00:09, 227406.02 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4156850 examples [00:10, 265315.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4202180 examples [00:10, 296489.17 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4270777 examples [00:10, 380680.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4316026 examples [00:10, 360363.49 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4356597 examples [00:10, 296986.78 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4393058 examples [00:10, 278281.35 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4448297 examples [00:10, 335498.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4502681 examples [00:10, 374873.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4548160 examples [00:11, 391803.63 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4592967 examples [00:11, 391475.92 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4638077 examples [00:11, 365672.96 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4679554 examples [00:11, 354293.74 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4719859 examples [00:11, 317375.42 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4770522 examples [00:11, 341472.51 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4815932 examples [00:11, 363775.94 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4898266 examples [00:11, 471574.84 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4957408 examples [00:12, 498925.96 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5012146 examples [00:12, 300614.84 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5075743 examples [00:12, 355255.27 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5121644 examples [00:12, 354071.92 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5167318 examples [00:12, 354272.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5231012 examples [00:12, 416403.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5280754 examples [00:13, 380487.79 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5354018 examples [00:13, 456914.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5408679 examples [00:13, 433133.61 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5481630 examples [00:13, 501492.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5536241 examples [00:13, 485930.99 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5590647 examples [00:13, 443924.26 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5641120 examples [00:13, 403040.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5719240 examples [00:13, 489140.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5773686 examples [00:14, 502478.10 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5842546 examples [00:14, 511738.73 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5897702 examples [00:14, 446069.44 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5947372 examples [00:14, 454848.99 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5997391 examples [00:14, 466001.65 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6057264 examples [00:14, 498710.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6112095 examples [00:14, 379473.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6175513 examples [00:14, 436042.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6230259 examples [00:15, 462334.64 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6285160 examples [00:15, 445951.13 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6340118 examples [00:15, 460861.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6390644 examples [00:15, 295650.10 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6435863 examples [00:15, 323557.88 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6514098 examples [00:15, 418298.88 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6604853 examples [00:15, 525671.79 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6668353 examples [00:16, 384070.53 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6736585 examples [00:16, 433894.22 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6800236 examples [00:16, 467729.92 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6859000 examples [00:16, 471182.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6913843 examples [00:16, 483742.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 6968424 examples [00:16, 356213.49 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7018901 examples [00:17, 385198.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7064340 examples [00:17, 378802.62 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7127858 examples [00:17, 436385.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7178402 examples [00:17, 423422.84 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7228491 examples [00:17, 295567.85 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7282382 examples [00:17, 342251.94 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7341479 examples [00:17, 387434.73 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7396192 examples [00:18, 406791.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7459717 examples [00:18, 446717.57 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7510045 examples [00:18, 423954.28 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7583017 examples [00:18, 468162.24 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7637689 examples [00:18, 484667.43 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7691476 examples [00:18, 411791.17 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7736961 examples [00:18, 396027.73 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7782321 examples [00:19, 267239.24 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7842208 examples [00:19, 325442.72 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7901873 examples [00:19, 380521.68 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 7951972 examples [00:19, 397964.70 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8001882 examples [00:19, 406951.06 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8056649 examples [00:19, 440563.80 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8106670 examples [00:19, 370609.16 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8151301 examples [00:19, 377989.26 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8237402 examples [00:20, 477548.30 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8291382 examples [00:20, 450428.70 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8341897 examples [00:20, 286033.52 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8418730 examples [00:20, 371879.58 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8469070 examples [00:20, 380447.61 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8518499 examples [00:20, 401402.50 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8592035 examples [00:20, 477409.28 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8646505 examples [00:21, 336118.17 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8697088 examples [00:21, 356215.98 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8770678 examples [00:21, 423581.57 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8820607 examples [00:21, 395441.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8879877 examples [00:21, 424627.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 8930250 examples [00:22, 299311.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9012727 examples [00:22, 393930.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9113143 examples [00:22, 485762.35 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9172576 examples [00:22, 494626.18 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9232613 examples [00:22, 426369.77 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9291455 examples [00:22, 458143.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9369347 examples [00:22, 530984.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9428102 examples [00:22, 541186.58 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9487326 examples [00:23, 313101.44 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9533528 examples [00:23, 309914.72 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9625250 examples [00:23, 419859.53 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9720883 examples [00:23, 525670.84 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9788206 examples [00:23, 457621.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9847798 examples [00:24, 310025.47 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9893283 examples [00:24, 317744.77 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9938754 examples [00:24, 336172.69 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10002774 examples [00:24, 352966.79 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10048367 examples [00:24, 369647.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10094308 examples [00:24, 369610.60 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10162371 examples [00:25, 432519.71 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10212872 examples [00:25, 446547.22 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10285702 examples [00:25, 506196.26 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10340822 examples [00:25, 351836.56 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10386678 examples [00:25, 334258.62 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10463790 examples [00:25, 414959.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10546046 examples [00:25, 504512.28 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10604683 examples [00:26, 474540.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10659874 examples [00:26, 440568.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10709915 examples [00:26, 431619.13 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10769413 examples [00:26, 449664.79 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10828624 examples [00:26, 471304.83 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10901898 examples [00:26, 533739.52 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10961584 examples [00:26, 337074.56 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11034164 examples [00:27, 402951.34 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11088351 examples [00:27, 423540.88 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11143052 examples [00:27, 414491.83 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11192984 examples [00:27, 432605.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11251839 examples [00:27, 462730.88 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11314999 examples [00:27, 491283.43 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11374165 examples [00:27, 486945.35 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11428792 examples [00:27, 458665.42 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11478824 examples [00:28, 331122.78 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11519353 examples [00:28, 265028.93 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11605472 examples [00:28, 373256.26 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11659898 examples [00:28, 407889.93 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11718920 examples [00:28, 436899.60 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11773428 examples [00:28, 440283.73 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11823171 examples [00:29, 404068.95 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11891249 examples [00:29, 467013.85 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11946522 examples [00:29, 402093.73 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12001139 examples [00:29, 431830.75 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12083459 examples [00:29, 515093.74 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12142578 examples [00:29, 374020.56 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12192429 examples [00:29, 372200.05 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12237878 examples [00:30, 346243.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12278833 examples [00:30, 358928.81 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12342525 examples [00:30, 422712.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12428769 examples [00:30, 529923.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12487477 examples [00:30, 511859.81 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12542486 examples [00:30, 501032.39 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12596873 examples [00:30, 477409.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12647256 examples [00:30, 483883.80 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12697361 examples [00:31, 358949.07 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12742963 examples [00:31, 318483.82 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12783630 examples [00:31, 278645.47 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12879328 examples [00:31, 392055.27 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 12951710 examples [00:31, 460314.60 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13010803 examples [00:31, 473438.11 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13065535 examples [00:32, 349585.28 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13133685 examples [00:32, 385646.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13183795 examples [00:32, 385562.50 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13229137 examples [00:32, 282975.70 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13315019 examples [00:32, 386754.52 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13382860 examples [00:32, 446404.06 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13441648 examples [00:33, 410813.27 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13496559 examples [00:33, 436645.19 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13565111 examples [00:33, 485436.79 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13619820 examples [00:33, 356553.96 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13683250 examples [00:33, 393531.75 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13774394 examples [00:33, 503404.74 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13843580 examples [00:33, 546182.26 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13907073 examples [00:33, 465491.77 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13961800 examples [00:34, 338202.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14011854 examples [00:34, 368005.75 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14116579 examples [00:34, 504659.48 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14179981 examples [00:34, 349551.95 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14234190 examples [00:34, 382445.39 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14320355 examples [00:35, 468625.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14406602 examples [00:35, 549742.92 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14475230 examples [00:35, 343666.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14529857 examples [00:35, 328243.41 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14634234 examples [00:35, 450552.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14711975 examples [00:35, 508057.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14780245 examples [00:36, 507467.16 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14844696 examples [00:36, 448261.46 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14899225 examples [00:36, 425072.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 14949597 examples [00:36, 425633.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15003655 examples [00:36, 437573.83 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15053550 examples [00:36, 429756.01 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15099251 examples [00:37, 270785.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15226775 examples [00:37, 448155.48 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15291129 examples [00:37, 481699.98 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15354986 examples [00:37, 419811.05 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15409582 examples [00:37, 308084.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15468608 examples [00:37, 354985.48 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15572788 examples [00:38, 487464.18 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15640079 examples [00:38, 269277.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15781573 examples [00:38, 423090.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15900172 examples [00:38, 542699.34 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 15990380 examples [00:39, 474521.04 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16081359 examples [00:39, 548390.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16158496 examples [00:39, 580170.64 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16236830 examples [00:39, 399189.71 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16332132 examples [00:39, 484299.53 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16404611 examples [00:39, 514963.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16473909 examples [00:40, 375066.68 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16528923 examples [00:40, 382882.26 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16602745 examples [00:40, 434139.11 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16676050 examples [00:40, 439425.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16731105 examples [00:40, 386142.67 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16776804 examples [00:40, 352071.83 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16900584 examples [00:41, 524390.39 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 16968297 examples [00:41, 450619.45 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17022639 examples [00:41, 436763.40 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17082301 examples [00:41, 466405.50 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17136966 examples [00:41, 385849.34 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17218702 examples [00:41, 469437.59 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17273196 examples [00:41, 443868.19 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17323076 examples [00:42, 386777.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17373352 examples [00:42, 409628.63 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17455382 examples [00:42, 490756.57 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17509907 examples [00:42, 378638.57 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17555264 examples [00:42, 390488.80 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17636613 examples [00:42, 486700.95 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17695119 examples [00:43, 355935.48 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17740632 examples [00:43, 325537.84 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17781662 examples [00:43, 283156.31 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17862982 examples [00:43, 374923.61 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17926550 examples [00:43, 424307.38 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 17976332 examples [00:43, 339790.01 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18045220 examples [00:44, 408559.47 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18113544 examples [00:44, 468564.50 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18172329 examples [00:44, 414885.11 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18222203 examples [00:44, 345295.94 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18277100 examples [00:44, 385670.00 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18323030 examples [00:44, 375492.65 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18377030 examples [00:44, 407984.94 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18450551 examples [00:44, 486398.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18505462 examples [00:45, 481790.69 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18560607 examples [00:45, 496800.63 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18615056 examples [00:45, 433407.78 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18665018 examples [00:45, 396302.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18710800 examples [00:45, 370088.27 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18753250 examples [00:45, 265655.88 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18822120 examples [00:46, 341004.14 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18899477 examples [00:46, 414227.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 18949936 examples [00:46, 373701.33 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19018276 examples [00:46, 436149.66 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19068782 examples [00:46, 446878.33 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19132382 examples [00:46, 459961.24 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19196281 examples [00:46, 472546.08 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19246132 examples [00:46, 384851.78 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19291864 examples [00:47, 287013.60 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19341615 examples [00:47, 315377.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19406058 examples [00:47, 372704.67 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19451722 examples [00:47, 385308.38 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19510351 examples [00:47, 431284.39 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19564331 examples [00:47, 430175.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19619200 examples [00:47, 454721.46 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19687321 examples [00:48, 509101.94 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19742164 examples [00:48, 513271.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19796993 examples [00:48, 450547.01 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19847487 examples [00:48, 444518.18 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19903727 examples [00:48, 431304.78 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19949000 examples [00:48, 345973.20 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 19994935 examples [00:48, 364005.77 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20049649 examples [00:48, 394673.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20091944 examples [00:49, 379419.05 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20160941 examples [00:49, 426237.24 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20206784 examples [00:49, 389975.98 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20248089 examples [00:49, 251233.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20302998 examples [00:49, 297357.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20348629 examples [00:49, 325885.46 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20398632 examples [00:50, 362479.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20441285 examples [00:50, 282774.05 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20487372 examples [00:50, 314512.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20524478 examples [00:50, 319141.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20579111 examples [00:50, 359665.10 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20624408 examples [00:50, 379382.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20674539 examples [00:50, 406790.11 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20720615 examples [00:51, 339705.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20761464 examples [00:51, 327434.50 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20813984 examples [00:51, 366652.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20854750 examples [00:51, 347827.88 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20895030 examples [00:51, 296064.80 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20936365 examples [00:51, 309423.53 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 20986421 examples [00:51, 348141.18 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21025707 examples [00:51, 337964.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21063737 examples [00:52, 346185.92 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21099625 examples [00:52, 275627.53 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21131325 examples [00:52, 211016.12 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21227932 examples [00:52, 354593.52 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21276444 examples [00:52, 361343.31 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21319571 examples [00:52, 370279.23 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21365304 examples [00:52, 375882.45 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21407024 examples [00:53, 377045.85 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21448005 examples [00:53, 376282.39 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21502614 examples [00:53, 410980.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21570006 examples [00:53, 478680.61 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21620917 examples [00:53, 412834.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21666627 examples [00:53, 343008.27 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21713397 examples [00:53, 369391.01 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21770383 examples [00:53, 417676.54 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21817225 examples [00:54, 363973.77 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21858387 examples [00:54, 233051.22 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21890629 examples [00:54, 242715.48 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21931780 examples [00:54, 271763.08 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21982257 examples [00:54, 317239.55 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22032569 examples [00:54, 358786.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22085580 examples [00:55, 374371.88 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22127271 examples [00:55, 339729.71 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22165654 examples [00:55, 288590.07 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22213420 examples [00:55, 326308.94 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22249828 examples [00:55, 250018.52 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22299984 examples [00:55, 288509.15 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22359469 examples [00:55, 353445.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22410262 examples [00:56, 387581.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22456092 examples [00:56, 388330.66 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22506727 examples [00:56, 415449.72 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22568156 examples [00:56, 468090.33 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22619309 examples [00:56, 458057.18 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22667674 examples [00:56, 349175.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22708246 examples [00:56, 263618.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22740826 examples [00:57, 261929.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22777650 examples [00:57, 282273.85 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22814211 examples [00:57, 280400.59 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22860217 examples [00:57, 313089.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22896385 examples [00:57, 324450.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22937781 examples [00:57, 337986.75 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 22990511 examples [00:57, 384410.17 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23035915 examples [00:57, 402787.15 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23081793 examples [00:58, 365609.52 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23138740 examples [00:58, 392453.16 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23179754 examples [00:58, 326927.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23250031 examples [00:58, 412775.18 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23295839 examples [00:58, 382972.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23338040 examples [00:58, 372476.64 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23392753 examples [00:58, 411288.03 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23438284 examples [00:58, 352334.67 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23479895 examples [00:59, 312793.39 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23516400 examples [00:59, 324228.91 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23556957 examples [00:59, 342692.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23602406 examples [00:59, 367112.52 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23643343 examples [00:59, 377497.83 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23696365 examples [00:59, 403627.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23741880 examples [00:59, 387666.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23783325 examples [00:59, 340894.27 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23820658 examples [01:00, 331243.39 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23858600 examples [01:00, 333319.55 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23894742 examples [01:00, 327986.96 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23933683 examples [01:00, 337173.87 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 23969629 examples [01:00, 311208.84 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24005851 examples [01:00, 298423.06 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24038543 examples [01:00, 275004.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24070086 examples [01:00, 280311.72 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24099231 examples [01:01, 254829.89 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24125459 examples [01:01, 249874.49 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24152530 examples [01:01, 212212.92 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24176156 examples [01:01, 195496.47 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24199096 examples [01:01, 159973.06 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24219769 examples [01:02, 119154.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 24229575 examples [01:02, 389017.05 examples/s]\u001b[0m\n",
      "\u001b[34mdataset printing:DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url'],\n",
      "        num_rows: 24229575\n",
      "    })\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mcolumn names: ['text', 'timestamp', 'url']\u001b[0m\n",
      "\u001b[34mtext cloumn name identified: text\u001b[0m\n",
      "\u001b[34msample train rown:{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': datetime.datetime(2019, 4, 25, 12, 57, 54), 'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'}\u001b[0m\n",
      "\u001b[34mrepo id: tiiuae/falcon-7b\u001b[0m\n",
      "\u001b[34mcloumn names: ['text', 'timestamp', 'url']\u001b[0m\n",
      "\u001b[34msample train row again:{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': datetime.datetime(2019, 4, 25, 12, 57, 54), 'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'}\u001b[0m\n",
      "\u001b[34mparallel_proc:96\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 0/24229575 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2537 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2373 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4139 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3302 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 1000/24229575 [00:02<15:20:06, 438.87 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (6059 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (6275 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4763 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2836 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2380 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 5000/24229575 [00:02<2:39:02, 2538.72 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2318 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (5095 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2466 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 8000/24229575 [00:02<1:32:06, 4382.68 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (5835 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (5846 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4768 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2779 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3606 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 11000/24229575 [00:02<1:02:24, 6468.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 13000/24229575 [00:02<52:58, 7618.37 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3280 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3225 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2922 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2346 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 17000/24229575 [00:03<35:44, 11289.10 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (7091 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4111 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 20000/24229575 [00:03<32:30, 12412.81 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2714 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2679 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 22000/24229575 [00:03<38:20, 10521.38 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2577 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2431 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2698 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3264 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 25000/24229575 [00:03<35:11, 11463.80 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2845 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4217 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3523 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 28000/24229575 [00:04<42:13, 9550.91 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (6988 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2903 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3003 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 33000/24229575 [00:04<33:10, 12157.75 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2130 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2191 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3486 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (9142 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2053 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 36000/24229575 [00:04<37:39, 10709.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 38000/24229575 [00:04<36:01, 11193.91 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3599 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (7680 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2628 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2114 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4679 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2935 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (13240 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3721 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4048 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 42000/24229575 [00:05<34:58, 11523.61 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2397 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4039 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 44000/24229575 [00:05<35:48, 11259.11 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3360 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3950 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (13949 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 53000/24229575 [00:05<18:28, 21818.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 57000/24229575 [00:05<17:16, 23331.36 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4281 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2509 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (5708 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 60000/24229575 [00:05<17:32, 22973.18 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2092 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 63000/24229575 [00:06<18:47, 21437.71 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2323 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 66000/24229575 [00:06<18:45, 21461.04 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (5092 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2227 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2448 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2736 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 69000/24229575 [00:06<23:56, 16817.03 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3818 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (8627 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2088 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2968 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 74000/24229575 [00:06<22:42, 17724.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 81000/24229575 [00:06<15:24, 26119.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 85000/24229575 [00:07<15:39, 25691.24 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (6647 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 89000/24229575 [00:07<14:08, 28435.40 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (22395 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4985 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2186 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (10051 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2160 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 93000/24229575 [00:07<22:30, 17878.07 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2116 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2062 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 96000/24229575 [00:07<20:53, 19258.61 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2118 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2180 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (5573 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3134 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3097 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3469 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 99000/24229575 [00:07<25:16, 15914.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 104000/24229575 [00:08<19:13, 20908.54 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3089 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 107000/24229575 [00:08<18:35, 21615.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 110000/24229575 [00:08<18:14, 22042.95 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2680 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3521 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2080 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 113000/24229575 [00:08<23:06, 17397.15 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (3688 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 116000/24229575 [00:08<23:19, 17230.80 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2634 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   0%|          | 119000/24229575 [00:08<21:55, 18325.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 124000/24229575 [00:09<17:52, 22479.66 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2180 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 129000/24229575 [00:09<15:28, 25946.26 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2363 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 132000/24229575 [00:09<16:55, 23730.13 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (6526 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 135000/24229575 [00:09<16:15, 24696.59 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (4138 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (6682 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2159 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 138000/24229575 [00:09<18:53, 21249.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 141000/24229575 [00:09<17:24, 23062.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 146000/24229575 [00:09<14:11, 28277.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 150000/24229575 [00:10<14:35, 27503.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 156000/24229575 [00:10<11:39, 34409.75 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2331 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2431 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 160000/24229575 [00:10<18:51, 21270.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 163000/24229575 [00:10<18:29, 21686.22 examples/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (2087 > 2048). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 168000/24229575 [00:10<17:21, 23109.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 171000/24229575 [00:11<22:55, 17495.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 176000/24229575 [00:11<19:03, 21029.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 179000/24229575 [00:11<22:06, 18128.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 183000/24229575 [00:11<26:37, 15055.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 186000/24229575 [00:12<25:08, 15940.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 189000/24229575 [00:12<23:00, 17408.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 192000/24229575 [00:12<24:42, 16219.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 196000/24229575 [00:12<20:05, 19933.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 199000/24229575 [00:12<19:02, 21031.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 202000/24229575 [00:12<20:50, 19211.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 205000/24229575 [00:12<20:31, 19507.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 208000/24229575 [00:13<20:35, 19444.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 211000/24229575 [00:13<20:39, 19373.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 216000/24229575 [00:13<15:42, 25476.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 219000/24229575 [00:13<15:30, 25804.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 224000/24229575 [00:13<12:39, 31601.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 230000/24229575 [00:13<11:18, 35347.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 234000/24229575 [00:13<12:24, 32246.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 238000/24229575 [00:14<14:57, 26728.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 242000/24229575 [00:14<14:28, 27610.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 245000/24229575 [00:14<19:39, 20341.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 248000/24229575 [00:14<18:06, 22069.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 251000/24229575 [00:14<17:51, 22380.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 257000/24229575 [00:15<16:54, 23640.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 261000/24229575 [00:15<16:46, 23813.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 266000/24229575 [00:15<14:29, 27556.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 269000/24229575 [00:15<16:11, 24676.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 272000/24229575 [00:15<16:50, 23717.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 275000/24229575 [00:15<25:07, 15889.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 280000/24229575 [00:16<19:08, 20846.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 283000/24229575 [00:16<24:49, 16073.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 286000/24229575 [00:16<23:06, 17273.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 289000/24229575 [00:16<21:18, 18729.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 292000/24229575 [00:16<27:20, 14593.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 296000/24229575 [00:17<21:30, 18548.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|          | 301000/24229575 [00:17<18:39, 21371.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 305000/24229575 [00:17<16:16, 24511.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 308000/24229575 [00:17<16:57, 23518.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 311000/24229575 [00:17<17:28, 22820.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 315000/24229575 [00:17<15:40, 25436.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 321000/24229575 [00:17<14:34, 27338.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 324000/24229575 [00:18<15:11, 26233.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 327000/24229575 [00:18<21:45, 18309.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 330000/24229575 [00:18<21:50, 18239.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 336000/24229575 [00:18<16:52, 23601.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 339000/24229575 [00:18<16:36, 23983.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 342000/24229575 [00:18<16:44, 23778.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 348000/24229575 [00:19<12:41, 31371.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 352000/24229575 [00:19<13:32, 29381.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 356000/24229575 [00:19<15:20, 25937.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 359000/24229575 [00:19<17:32, 22683.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   1%|▏         | 362000/24229575 [00:19<19:31, 20373.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 365000/24229575 [00:19<18:14, 21800.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 368000/24229575 [00:20<29:49, 13335.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 370000/24229575 [00:20<28:20, 14032.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 372000/24229575 [00:20<33:50, 11749.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 376000/24229575 [00:20<27:52, 14261.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 381000/24229575 [00:21<20:10, 19708.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 386000/24229575 [00:21<16:01, 24806.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 390000/24229575 [00:21<22:08, 17946.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 394000/24229575 [00:21<19:13, 20663.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 398000/24229575 [00:21<17:01, 23323.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 401000/24229575 [00:21<17:23, 22835.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 406000/24229575 [00:22<15:18, 25933.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 409000/24229575 [00:22<15:33, 25513.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 412000/24229575 [00:22<15:52, 25013.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 415000/24229575 [00:22<15:53, 24987.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 419000/24229575 [00:22<14:16, 27815.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 422000/24229575 [00:22<16:18, 24321.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 425000/24229575 [00:22<17:29, 22690.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 430000/24229575 [00:23<15:35, 25451.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 433000/24229575 [00:23<15:42, 25256.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 436000/24229575 [00:23<15:11, 26113.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 441000/24229575 [00:23<12:40, 31266.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 445000/24229575 [00:23<16:48, 23574.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 448000/24229575 [00:23<16:30, 24004.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 451000/24229575 [00:23<16:42, 23723.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 454000/24229575 [00:24<16:19, 24280.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 457000/24229575 [00:24<16:33, 23923.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 460000/24229575 [00:24<25:38, 15450.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 463000/24229575 [00:24<26:44, 14808.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 467000/24229575 [00:24<24:26, 16201.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 469000/24229575 [00:25<26:42, 14828.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 471000/24229575 [00:25<35:24, 11183.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 475000/24229575 [00:25<30:33, 12953.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 479000/24229575 [00:25<26:26, 14968.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 484000/24229575 [00:26<22:08, 17867.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 487000/24229575 [00:26<22:04, 17930.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 492000/24229575 [00:26<17:08, 23070.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 496000/24229575 [00:26<15:03, 26271.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 500000/24229575 [00:26<14:15, 27749.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 504000/24229575 [00:26<15:30, 25487.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 511000/24229575 [00:26<11:34, 34160.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 518000/24229575 [00:27<10:24, 37958.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 523000/24229575 [00:27<10:37, 37175.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 527000/24229575 [00:27<15:47, 25025.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 532000/24229575 [00:27<17:43, 22276.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 535000/24229575 [00:27<17:43, 22271.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 538000/24229575 [00:28<16:56, 23297.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 541000/24229575 [00:28<17:32, 22514.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 544000/24229575 [00:28<17:03, 23130.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 547000/24229575 [00:28<23:26, 16838.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 550000/24229575 [00:28<25:20, 15569.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 554000/24229575 [00:28<21:52, 18040.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 557000/24229575 [00:29<23:01, 17130.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 559000/24229575 [00:29<28:07, 14028.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 561000/24229575 [00:29<32:47, 12032.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 564000/24229575 [00:29<27:16, 14457.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 566000/24229575 [00:29<27:59, 14092.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 568000/24229575 [00:30<26:04, 15123.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 572000/24229575 [00:30<19:23, 20325.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 576000/24229575 [00:30<15:59, 24657.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 579000/24229575 [00:30<15:54, 24766.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 582000/24229575 [00:30<16:27, 23943.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 587000/24229575 [00:30<16:50, 23398.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 591000/24229575 [00:30<17:57, 21944.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 594000/24229575 [00:31<18:08, 21715.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 597000/24229575 [00:31<21:14, 18542.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   2%|▏         | 599000/24229575 [00:31<21:12, 18569.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 607000/24229575 [00:31<13:11, 29830.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 611000/24229575 [00:31<13:40, 28789.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 615000/24229575 [00:31<15:39, 25145.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 619000/24229575 [00:32<16:06, 24420.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 623000/24229575 [00:32<14:22, 27373.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 630000/24229575 [00:32<13:25, 29310.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 634000/24229575 [00:32<14:01, 28031.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 638000/24229575 [00:32<16:52, 23310.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 641000/24229575 [00:33<23:44, 16555.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 644000/24229575 [00:33<23:23, 16802.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 648000/24229575 [00:33<20:24, 19258.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 651000/24229575 [00:33<19:40, 19979.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 654000/24229575 [00:33<19:37, 20017.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 657000/24229575 [00:33<20:55, 18775.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 660000/24229575 [00:34<19:57, 19678.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 663000/24229575 [00:34<24:06, 16289.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 667000/24229575 [00:34<19:17, 20352.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 671000/24229575 [00:34<18:46, 20916.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 674000/24229575 [00:34<17:17, 22694.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 678000/24229575 [00:34<14:55, 26299.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 683000/24229575 [00:35<17:25, 22528.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 686000/24229575 [00:35<18:50, 20834.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 690000/24229575 [00:35<18:32, 21164.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 693000/24229575 [00:35<23:12, 16904.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 696000/24229575 [00:35<23:42, 16541.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 700000/24229575 [00:36<21:54, 17902.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 703000/24229575 [00:36<21:30, 18227.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 706000/24229575 [00:36<20:32, 19093.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 711000/24229575 [00:36<17:47, 22023.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 714000/24229575 [00:36<20:21, 19245.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 718000/24229575 [00:36<17:48, 22006.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 721000/24229575 [00:37<17:24, 22499.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 724000/24229575 [00:37<16:17, 24056.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 728000/24229575 [00:37<15:43, 24918.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 731000/24229575 [00:37<15:53, 24646.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 735000/24229575 [00:37<15:54, 24622.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 741000/24229575 [00:37<13:22, 29285.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 745000/24229575 [00:37<13:42, 28555.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 748000/24229575 [00:38<14:18, 27351.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 752000/24229575 [00:38<14:21, 27251.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 755000/24229575 [00:38<15:21, 25474.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 758000/24229575 [00:38<19:04, 20502.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 761000/24229575 [00:38<20:10, 19386.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 764000/24229575 [00:38<21:32, 18159.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 768000/24229575 [00:39<17:55, 21804.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 772000/24229575 [00:39<16:03, 24355.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 775000/24229575 [00:39<21:42, 18005.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 778000/24229575 [00:39<22:09, 17645.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 781000/24229575 [00:39<20:13, 19327.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 784000/24229575 [00:39<19:56, 19589.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 787000/24229575 [00:40<24:21, 16036.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 793000/24229575 [00:40<17:36, 22183.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 796000/24229575 [00:40<20:46, 18796.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 799000/24229575 [00:40<24:57, 15643.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 804000/24229575 [00:40<18:35, 21004.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 807000/24229575 [00:41<22:46, 17144.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 810000/24229575 [00:41<20:55, 18650.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 814000/24229575 [00:41<18:07, 21531.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 818000/24229575 [00:41<16:58, 22986.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 821000/24229575 [00:41<17:45, 21966.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 826000/24229575 [00:41<14:25, 27055.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 829000/24229575 [00:42<16:55, 23049.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 834000/24229575 [00:42<14:18, 27239.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 838000/24229575 [00:42<13:20, 29206.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 842000/24229575 [00:42<14:06, 27615.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 845000/24229575 [00:42<16:13, 24015.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   3%|▎         | 848000/24229575 [00:42<15:31, 25114.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 851000/24229575 [00:42<18:42, 20822.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 854000/24229575 [00:43<20:08, 19348.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 857000/24229575 [00:43<18:23, 21172.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 862000/24229575 [00:43<15:32, 25049.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 865000/24229575 [00:43<22:01, 17677.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 869000/24229575 [00:43<22:53, 17002.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 871000/24229575 [00:44<23:21, 16665.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 874000/24229575 [00:44<21:49, 17835.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 876000/24229575 [00:44<23:38, 16458.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 883000/24229575 [00:44<16:51, 23089.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 886000/24229575 [00:44<18:27, 21071.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 889000/24229575 [00:44<17:15, 22531.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 892000/24229575 [00:45<18:10, 21392.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 895000/24229575 [00:45<24:40, 15756.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 898000/24229575 [00:45<22:00, 17674.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 902000/24229575 [00:45<18:10, 21388.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▎         | 907000/24229575 [00:45<18:37, 20878.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 910000/24229575 [00:45<18:26, 21075.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 914000/24229575 [00:46<15:45, 24646.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 917000/24229575 [00:46<15:06, 25711.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 920000/24229575 [00:46<16:29, 23557.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 923000/24229575 [00:46<16:41, 23265.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 926000/24229575 [00:46<16:27, 23595.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 929000/24229575 [00:46<17:29, 22207.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 933000/24229575 [00:46<16:01, 24236.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 936000/24229575 [00:47<15:55, 24371.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 941000/24229575 [00:47<15:04, 25761.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 944000/24229575 [00:47<15:04, 25733.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 948000/24229575 [00:47<14:30, 26745.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 951000/24229575 [00:47<18:33, 20900.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 954000/24229575 [00:47<21:34, 17983.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 956000/24229575 [00:48<23:22, 16595.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 959000/24229575 [00:48<25:23, 15270.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 963000/24229575 [00:48<20:14, 19160.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 966000/24229575 [00:48<18:32, 20904.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):   4%|▍         | 969000/24229575 [00:48<17:08, 22624.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9600000/24229575 [06:41<11:54, 20488.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9603000/24229575 [06:42<12:25, 19614.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9607000/24229575 [06:42<10:30, 23209.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9610000/24229575 [06:42<11:50, 20588.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9615000/24229575 [06:42<09:13, 26402.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9618000/24229575 [06:42<11:12, 21711.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9625000/24229575 [06:42<07:48, 31165.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9629000/24229575 [06:42<08:04, 30125.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9633000/24229575 [06:43<07:39, 31739.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9639000/24229575 [06:43<09:09, 26543.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9644000/24229575 [06:43<08:52, 27380.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9650000/24229575 [06:43<07:14, 33570.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9654000/24229575 [06:43<07:23, 32848.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9658000/24229575 [06:43<08:43, 27832.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9662000/24229575 [06:44<08:37, 28163.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9666000/24229575 [06:44<09:08, 26533.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9671000/24229575 [06:44<08:10, 29679.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9675000/24229575 [06:44<08:40, 27978.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9678000/24229575 [06:44<10:23, 23336.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9681000/24229575 [06:44<10:33, 22954.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9684000/24229575 [06:44<10:13, 23709.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9688000/24229575 [06:45<09:32, 25394.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|███▉      | 9691000/24229575 [06:45<11:44, 20643.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9695000/24229575 [06:45<11:46, 20566.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9698000/24229575 [06:45<12:19, 19653.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9701000/24229575 [06:45<12:05, 20023.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9704000/24229575 [06:46<12:48, 18907.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9706000/24229575 [06:46<12:41, 19076.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9710000/24229575 [06:46<10:24, 23254.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9717000/24229575 [06:46<07:14, 33437.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9722000/24229575 [06:46<07:06, 34004.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9726000/24229575 [06:46<07:13, 33459.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9731000/24229575 [06:46<06:33, 36855.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9735000/24229575 [06:46<09:23, 25722.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9741000/24229575 [06:47<08:21, 28869.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9746000/24229575 [06:47<07:49, 30835.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9750000/24229575 [06:47<09:20, 25850.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9753000/24229575 [06:47<09:29, 25408.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9756000/24229575 [06:47<10:11, 23674.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9760000/24229575 [06:47<09:27, 25476.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9763000/24229575 [06:48<09:43, 24806.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9767000/24229575 [06:48<08:58, 26877.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9770000/24229575 [06:48<09:05, 26501.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9774000/24229575 [06:48<09:01, 26697.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9777000/24229575 [06:48<09:30, 25318.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9780000/24229575 [06:48<09:23, 25639.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9783000/24229575 [06:48<10:00, 24074.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9786000/24229575 [06:49<14:13, 16929.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9789000/24229575 [06:49<13:38, 17644.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9791000/24229575 [06:49<14:05, 17073.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9794000/24229575 [06:49<12:21, 19456.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9799000/24229575 [06:49<10:49, 22219.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9806000/24229575 [06:49<08:15, 29094.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  40%|████      | 9810000/24229575 [06:50<08:36, 27912.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9813000/24229575 [06:50<08:28, 28329.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9816000/24229575 [06:50<09:47, 24525.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9821000/24229575 [06:50<07:58, 30091.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9826000/24229575 [06:50<07:24, 32388.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9832000/24229575 [06:50<06:55, 34660.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9836000/24229575 [06:51<10:50, 22126.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9840000/24229575 [06:51<09:53, 24239.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9844000/24229575 [06:51<09:07, 26252.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9848000/24229575 [06:51<09:28, 25303.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9853000/24229575 [06:51<08:56, 26820.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9856000/24229575 [06:51<09:05, 26338.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9860000/24229575 [06:51<08:40, 27592.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9865000/24229575 [06:52<07:30, 31872.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9869000/24229575 [06:52<08:41, 27520.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9872000/24229575 [06:52<08:39, 27621.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9877000/24229575 [06:52<08:03, 29688.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9881000/24229575 [06:52<13:12, 18112.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9885000/24229575 [06:53<11:30, 20769.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9888000/24229575 [06:53<11:49, 20207.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9892000/24229575 [06:53<10:39, 22410.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9898000/24229575 [06:53<08:10, 29203.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9903000/24229575 [06:53<07:22, 32359.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9907000/24229575 [06:53<09:05, 26274.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9911000/24229575 [06:53<09:52, 24148.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9915000/24229575 [06:54<10:09, 23480.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9918000/24229575 [06:54<10:13, 23334.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9923000/24229575 [06:54<08:26, 28234.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9929000/24229575 [06:54<07:36, 31305.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9933000/24229575 [06:54<08:50, 26942.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9936000/24229575 [06:54<10:05, 23605.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9939000/24229575 [06:55<09:39, 24652.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9944000/24229575 [06:55<08:04, 29464.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9948000/24229575 [06:55<10:29, 22670.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9952000/24229575 [06:55<09:44, 24418.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9955000/24229575 [06:55<10:52, 21875.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9963000/24229575 [06:55<07:31, 31584.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9967000/24229575 [06:56<09:30, 24988.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9971000/24229575 [06:56<09:41, 24517.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9975000/24229575 [06:56<08:52, 26776.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9978000/24229575 [06:56<09:28, 25059.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9981000/24229575 [06:56<09:18, 25517.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9984000/24229575 [06:56<09:32, 24870.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9987000/24229575 [06:56<10:51, 21858.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████      | 9993000/24229575 [06:57<07:58, 29774.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 9997000/24229575 [06:57<08:04, 29393.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10001000/24229575 [06:57<09:31, 24883.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10005000/24229575 [06:57<08:45, 27091.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10008000/24229575 [06:57<09:36, 24661.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10013000/24229575 [06:57<08:36, 27546.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10016000/24229575 [06:57<09:02, 26202.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10019000/24229575 [06:58<09:40, 24487.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10022000/24229575 [06:58<12:30, 18927.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10026000/24229575 [06:58<11:16, 20988.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10030000/24229575 [06:58<09:50, 24054.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10033000/24229575 [06:58<11:15, 21010.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10039000/24229575 [06:58<08:10, 28924.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10043000/24229575 [06:59<10:25, 22663.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10046000/24229575 [06:59<10:11, 23202.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10049000/24229575 [06:59<10:03, 23481.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  41%|████▏     | 10053000/24229575 [06:59<09:29, 24877.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10058000/24229575 [06:59<08:38, 27329.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10061000/24229575 [06:59<09:27, 24954.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10064000/24229575 [07:00<09:19, 25323.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10067000/24229575 [07:00<09:09, 25779.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10073000/24229575 [07:00<07:06, 33230.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10077000/24229575 [07:00<09:47, 24087.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10083000/24229575 [07:00<07:46, 30300.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10087000/24229575 [07:00<07:19, 32190.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10091000/24229575 [07:00<07:43, 30516.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10095000/24229575 [07:01<07:23, 31869.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10099000/24229575 [07:01<06:58, 33748.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10104000/24229575 [07:01<09:00, 26127.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10108000/24229575 [07:01<08:42, 27021.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10112000/24229575 [07:01<10:06, 23260.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10116000/24229575 [07:01<10:36, 22161.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10122000/24229575 [07:02<08:40, 27081.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10125000/24229575 [07:02<11:56, 19688.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10128000/24229575 [07:02<11:08, 21090.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10131000/24229575 [07:02<10:32, 22279.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10137000/24229575 [07:02<07:49, 30030.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10141000/24229575 [07:02<08:47, 26731.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10145000/24229575 [07:03<08:32, 27473.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10149000/24229575 [07:03<11:42, 20055.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10154000/24229575 [07:03<09:48, 23915.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10157000/24229575 [07:03<09:21, 25057.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10160000/24229575 [07:03<09:28, 24745.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10165000/24229575 [07:03<07:44, 30272.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10172000/24229575 [07:03<06:00, 38990.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10177000/24229575 [07:04<07:45, 30158.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10181000/24229575 [07:04<08:22, 27971.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10185000/24229575 [07:04<08:38, 27103.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10191000/24229575 [07:04<07:02, 33228.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10195000/24229575 [07:04<07:05, 32990.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10199000/24229575 [07:04<08:15, 28333.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10203000/24229575 [07:05<10:02, 23267.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10206000/24229575 [07:05<11:27, 20386.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10210000/24229575 [07:05<10:31, 22210.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10213000/24229575 [07:05<11:53, 19636.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10216000/24229575 [07:05<11:35, 20136.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10223000/24229575 [07:06<07:49, 29819.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10228000/24229575 [07:06<08:15, 28261.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10232000/24229575 [07:06<09:33, 24404.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10236000/24229575 [07:06<10:02, 23234.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10240000/24229575 [07:06<09:22, 24864.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10244000/24229575 [07:06<09:22, 24880.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10248000/24229575 [07:07<09:24, 24754.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10252000/24229575 [07:07<08:47, 26514.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10259000/24229575 [07:07<06:34, 35387.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10263000/24229575 [07:07<07:23, 31479.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10267000/24229575 [07:07<08:32, 27254.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10271000/24229575 [07:07<08:55, 26079.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10276000/24229575 [07:07<07:44, 30018.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10280000/24229575 [07:08<09:10, 25341.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10285000/24229575 [07:08<08:06, 28638.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10289000/24229575 [07:08<10:15, 22657.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10294000/24229575 [07:08<08:58, 25871.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  42%|████▏     | 10297000/24229575 [07:08<10:17, 22570.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10302000/24229575 [07:09<08:22, 27727.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10306000/24229575 [07:09<09:37, 24097.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10309000/24229575 [07:09<12:04, 19214.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10314000/24229575 [07:09<09:35, 24188.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10318000/24229575 [07:09<08:31, 27205.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10322000/24229575 [07:09<09:06, 25447.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10325000/24229575 [07:10<10:23, 22317.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10328000/24229575 [07:10<09:57, 23258.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10332000/24229575 [07:10<09:09, 25275.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10335000/24229575 [07:10<09:45, 23724.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10339000/24229575 [07:10<08:52, 26080.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10342000/24229575 [07:10<09:30, 24336.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10348000/24229575 [07:10<08:07, 28486.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10356000/24229575 [07:11<05:55, 39063.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10361000/24229575 [07:11<07:37, 30293.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10365000/24229575 [07:11<08:38, 26735.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10370000/24229575 [07:11<07:59, 28884.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10374000/24229575 [07:11<08:42, 26493.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10378000/24229575 [07:11<08:06, 28477.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10382000/24229575 [07:12<08:11, 28146.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10385000/24229575 [07:12<10:24, 22157.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10388000/24229575 [07:12<10:33, 21834.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10391000/24229575 [07:12<10:40, 21601.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10394000/24229575 [07:12<12:02, 19160.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10397000/24229575 [07:12<11:52, 19401.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10400000/24229575 [07:13<10:42, 21534.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10406000/24229575 [07:13<08:02, 28643.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10410000/24229575 [07:13<08:18, 27706.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10413000/24229575 [07:13<09:34, 24049.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10417000/24229575 [07:13<09:38, 23872.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10421000/24229575 [07:13<08:32, 26969.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10426000/24229575 [07:13<08:21, 27498.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10432000/24229575 [07:14<07:21, 31277.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10436000/24229575 [07:14<06:55, 33168.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10440000/24229575 [07:14<07:43, 29764.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10446000/24229575 [07:14<06:55, 33170.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10450000/24229575 [07:14<06:50, 33546.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10455000/24229575 [07:14<06:12, 36987.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10459000/24229575 [07:15<08:33, 26793.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10463000/24229575 [07:15<09:11, 24955.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10466000/24229575 [07:15<11:55, 19230.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10469000/24229575 [07:15<11:21, 20195.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10473000/24229575 [07:15<09:41, 23646.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10478000/24229575 [07:15<08:19, 27536.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10482000/24229575 [07:16<09:23, 24393.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10485000/24229575 [07:16<10:04, 22730.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10488000/24229575 [07:16<12:42, 18028.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10491000/24229575 [07:16<14:16, 16035.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10494000/24229575 [07:16<12:44, 17965.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10499000/24229575 [07:16<09:49, 23283.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10506000/24229575 [07:17<07:01, 32590.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10510000/24229575 [07:17<06:58, 32816.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10514000/24229575 [07:17<07:25, 30804.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10518000/24229575 [07:17<07:28, 30541.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10522000/24229575 [07:17<07:20, 31116.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10526000/24229575 [07:17<07:36, 30008.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10531000/24229575 [07:17<06:37, 34456.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  43%|████▎     | 10535000/24229575 [07:17<07:05, 32160.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10541000/24229575 [07:18<05:52, 38882.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10546000/24229575 [07:18<08:59, 25348.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10550000/24229575 [07:18<09:47, 23290.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10555000/24229575 [07:18<09:20, 24380.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10558000/24229575 [07:18<09:31, 23940.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10561000/24229575 [07:19<09:24, 24211.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10564000/24229575 [07:19<09:08, 24892.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10567000/24229575 [07:19<08:51, 25702.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10570000/24229575 [07:19<09:11, 24785.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10575000/24229575 [07:19<07:46, 29288.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10579000/24229575 [07:19<07:33, 30072.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10583000/24229575 [07:20<11:39, 19513.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10586000/24229575 [07:20<15:45, 14422.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▎     | 10593000/24229575 [07:20<10:18, 22056.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10601000/24229575 [07:20<07:35, 29939.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10605000/24229575 [07:20<08:17, 27388.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10609000/24229575 [07:21<08:45, 25939.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10613000/24229575 [07:21<08:37, 26297.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10619000/24229575 [07:21<07:36, 29826.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10624000/24229575 [07:21<06:56, 32686.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10628000/24229575 [07:21<07:16, 31144.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10634000/24229575 [07:21<06:22, 35511.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10638000/24229575 [07:22<08:59, 25205.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10642000/24229575 [07:22<11:07, 20359.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10645000/24229575 [07:22<10:29, 21591.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10649000/24229575 [07:22<09:26, 23956.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10652000/24229575 [07:22<10:03, 22493.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10655000/24229575 [07:22<09:44, 23230.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10659000/24229575 [07:22<09:22, 24106.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10662000/24229575 [07:23<09:43, 23251.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10666000/24229575 [07:23<08:26, 26752.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10669000/24229575 [07:23<08:19, 27142.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10672000/24229575 [07:23<08:34, 26337.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10675000/24229575 [07:23<08:19, 27114.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10680000/24229575 [07:23<08:54, 25351.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10685000/24229575 [07:23<07:19, 30839.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10689000/24229575 [07:23<06:51, 32918.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10696000/24229575 [07:24<06:17, 35878.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10700000/24229575 [07:24<07:11, 31332.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10704000/24229575 [07:24<08:30, 26515.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10707000/24229575 [07:24<11:27, 19667.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10710000/24229575 [07:24<10:51, 20755.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10716000/24229575 [07:25<08:41, 25910.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10719000/24229575 [07:25<11:16, 19973.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10724000/24229575 [07:25<08:57, 25135.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10728000/24229575 [07:25<08:53, 25284.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10733000/24229575 [07:25<07:57, 28239.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10737000/24229575 [07:25<08:27, 26581.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10740000/24229575 [07:26<08:22, 26866.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10743000/24229575 [07:26<10:47, 20831.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10746000/24229575 [07:26<11:04, 20297.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10749000/24229575 [07:26<10:13, 21965.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10755000/24229575 [07:26<07:53, 28481.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10760000/24229575 [07:26<07:08, 31410.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10764000/24229575 [07:27<08:10, 27467.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10769000/24229575 [07:27<08:15, 27158.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10774000/24229575 [07:27<07:02, 31823.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10778000/24229575 [07:27<06:45, 33205.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  44%|████▍     | 10782000/24229575 [07:27<06:26, 34760.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10787000/24229575 [07:27<06:28, 34582.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10791000/24229575 [07:27<08:00, 27940.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10795000/24229575 [07:28<09:57, 22488.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10799000/24229575 [07:28<09:18, 24064.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10803000/24229575 [07:28<10:31, 21261.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10806000/24229575 [07:28<10:20, 21630.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10809000/24229575 [07:28<11:56, 18719.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10812000/24229575 [07:28<11:04, 20197.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10815000/24229575 [07:29<10:41, 20919.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10820000/24229575 [07:29<08:35, 26024.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10825000/24229575 [07:29<07:49, 28547.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10829000/24229575 [07:29<08:44, 25558.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10832000/24229575 [07:29<09:46, 22857.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10835000/24229575 [07:29<10:37, 21010.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10841000/24229575 [07:30<08:02, 27726.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10845000/24229575 [07:30<07:40, 29052.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10849000/24229575 [07:30<08:05, 27542.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10852000/24229575 [07:30<08:02, 27733.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10856000/24229575 [07:30<07:57, 28018.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10860000/24229575 [07:30<07:15, 30677.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10864000/24229575 [07:30<07:39, 29063.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10869000/24229575 [07:30<06:39, 33471.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10873000/24229575 [07:31<07:05, 31415.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10882000/24229575 [07:31<05:13, 42545.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10887000/24229575 [07:31<07:18, 30439.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10891000/24229575 [07:31<09:21, 23770.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10894000/24229575 [07:31<09:44, 22802.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10897000/24229575 [07:32<11:38, 19085.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▍     | 10901000/24229575 [07:32<11:58, 18551.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10905000/24229575 [07:32<10:43, 20713.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10910000/24229575 [07:32<09:11, 24163.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10913000/24229575 [07:32<10:17, 21581.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10917000/24229575 [07:33<09:09, 24219.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10920000/24229575 [07:33<10:21, 21421.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10928000/24229575 [07:33<07:36, 29151.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10932000/24229575 [07:33<09:10, 24155.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10937000/24229575 [07:33<08:00, 27651.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10941000/24229575 [07:33<07:54, 28013.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10945000/24229575 [07:34<08:28, 26125.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10950000/24229575 [07:34<07:11, 30794.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10956000/24229575 [07:34<06:18, 35060.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10961000/24229575 [07:34<06:12, 35634.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10966000/24229575 [07:34<05:58, 37018.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10970000/24229575 [07:34<08:16, 26710.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10974000/24229575 [07:34<08:16, 26681.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10977000/24229575 [07:35<09:57, 22185.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10983000/24229575 [07:35<07:56, 27818.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10987000/24229575 [07:35<09:12, 23982.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10990000/24229575 [07:35<09:35, 23017.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10993000/24229575 [07:35<09:06, 24232.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 10996000/24229575 [07:35<08:41, 25380.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 11001000/24229575 [07:35<07:09, 30803.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 11005000/24229575 [07:36<10:35, 20801.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 11010000/24229575 [07:36<08:32, 25798.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 11014000/24229575 [07:36<09:47, 22490.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 11017000/24229575 [07:36<11:49, 18621.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  45%|████▌     | 11021000/24229575 [07:37<11:13, 19621.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11028000/24229575 [07:37<08:12, 26830.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11032000/24229575 [07:37<07:59, 27548.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11036000/24229575 [07:37<07:59, 27493.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11040000/24229575 [07:37<08:28, 25962.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11048000/24229575 [07:37<06:45, 32490.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11052000/24229575 [07:37<06:42, 32701.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11056000/24229575 [07:38<07:30, 29223.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11060000/24229575 [07:38<07:35, 28898.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11063000/24229575 [07:38<07:58, 27526.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11066000/24229575 [07:38<09:28, 23172.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11072000/24229575 [07:38<07:08, 30699.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11076000/24229575 [07:38<08:24, 26077.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11079000/24229575 [07:39<08:38, 25371.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11082000/24229575 [07:39<10:51, 20186.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11085000/24229575 [07:39<10:34, 20702.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11088000/24229575 [07:39<09:59, 21916.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11093000/24229575 [07:39<07:52, 27789.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11097000/24229575 [07:39<07:24, 29553.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11101000/24229575 [07:39<06:50, 32011.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11105000/24229575 [07:40<06:50, 31960.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11109000/24229575 [07:40<11:20, 19290.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11112000/24229575 [07:40<10:54, 20044.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11116000/24229575 [07:40<10:00, 21849.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11119000/24229575 [07:40<10:45, 20305.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11122000/24229575 [07:40<09:50, 22199.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11127000/24229575 [07:41<07:59, 27328.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11131000/24229575 [07:41<08:20, 26158.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11138000/24229575 [07:41<06:21, 34306.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11142000/24229575 [07:41<07:13, 30158.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11146000/24229575 [07:41<07:34, 28757.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11150000/24229575 [07:41<07:04, 30826.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11155000/24229575 [07:41<06:26, 33788.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11159000/24229575 [07:42<06:43, 32381.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11163000/24229575 [07:42<07:55, 27457.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11166000/24229575 [07:42<09:20, 23297.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11171000/24229575 [07:42<07:40, 28333.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11175000/24229575 [07:42<07:24, 29341.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11179000/24229575 [07:42<08:22, 25957.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11183000/24229575 [07:43<08:06, 26794.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11186000/24229575 [07:43<08:58, 24235.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11192000/24229575 [07:43<07:15, 29969.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11196000/24229575 [07:43<13:00, 16705.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11201000/24229575 [07:43<10:51, 20001.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▌     | 11205000/24229575 [07:44<10:16, 21113.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11208000/24229575 [07:44<09:53, 21948.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11213000/24229575 [07:44<08:18, 26094.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11217000/24229575 [07:44<10:50, 20005.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11220000/24229575 [07:44<10:42, 20232.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11223000/24229575 [07:44<10:03, 21544.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11227000/24229575 [07:45<08:51, 24455.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11231000/24229575 [07:45<07:56, 27260.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11239000/24229575 [07:45<05:54, 36621.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11245000/24229575 [07:45<05:10, 41849.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11250000/24229575 [07:45<05:46, 37479.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11255000/24229575 [07:45<07:16, 29750.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11259000/24229575 [07:46<07:41, 28089.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  46%|████▋     | 11263000/24229575 [07:46<07:45, 27852.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11268000/24229575 [07:46<07:37, 28328.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11274000/24229575 [07:46<06:22, 33854.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11278000/24229575 [07:46<06:33, 32899.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11282000/24229575 [07:46<07:02, 30670.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11286000/24229575 [07:46<07:45, 27813.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11289000/24229575 [07:47<09:22, 22987.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11292000/24229575 [07:47<14:32, 14824.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11297000/24229575 [07:47<12:22, 17408.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11301000/24229575 [07:47<10:35, 20350.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11305000/24229575 [07:47<09:06, 23654.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11309000/24229575 [07:48<08:09, 26388.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11313000/24229575 [07:48<07:40, 28053.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11317000/24229575 [07:48<09:34, 22467.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11320000/24229575 [07:48<09:24, 22874.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11324000/24229575 [07:48<08:16, 25972.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11329000/24229575 [07:48<06:56, 30943.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11333000/24229575 [07:48<07:17, 29488.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11337000/24229575 [07:49<07:41, 27923.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11340000/24229575 [07:49<07:42, 27876.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11344000/24229575 [07:49<07:58, 26909.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11348000/24229575 [07:49<07:29, 28651.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11353000/24229575 [07:49<07:15, 29575.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11358000/24229575 [07:49<06:55, 30941.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11362000/24229575 [07:49<06:46, 31635.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11366000/24229575 [07:50<07:01, 30527.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11370000/24229575 [07:50<06:46, 31623.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11376000/24229575 [07:50<05:43, 37467.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11380000/24229575 [07:50<06:20, 33794.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11384000/24229575 [07:50<11:53, 18004.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11387000/24229575 [07:51<11:17, 18967.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11391000/24229575 [07:51<11:44, 18232.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11394000/24229575 [07:51<12:00, 17803.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11399000/24229575 [07:51<09:10, 23326.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11403000/24229575 [07:51<08:35, 24891.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11406000/24229575 [07:51<08:15, 25860.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11409000/24229575 [07:52<10:15, 20818.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11412000/24229575 [07:52<09:38, 22150.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11415000/24229575 [07:52<09:28, 22551.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11420000/24229575 [07:52<08:13, 25932.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11425000/24229575 [07:52<06:52, 31022.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11429000/24229575 [07:52<07:38, 27895.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11433000/24229575 [07:52<07:53, 27017.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11436000/24229575 [07:53<08:08, 26187.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11439000/24229575 [07:53<08:05, 26360.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11443000/24229575 [07:53<07:38, 27863.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11446000/24229575 [07:53<08:41, 24513.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11449000/24229575 [07:53<08:34, 24832.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11454000/24229575 [07:53<07:13, 29498.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11459000/24229575 [07:53<06:19, 33611.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11463000/24229575 [07:53<06:37, 32142.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11468000/24229575 [07:54<05:58, 35559.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11473000/24229575 [07:54<05:50, 36367.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11479000/24229575 [07:54<09:09, 23198.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11483000/24229575 [07:54<10:45, 19742.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11486000/24229575 [07:55<11:02, 19223.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11491000/24229575 [07:55<10:56, 19406.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11494000/24229575 [07:55<10:30, 20185.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11497000/24229575 [07:55<10:12, 20798.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11500000/24229575 [07:55<10:34, 20067.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11503000/24229575 [07:55<09:38, 21988.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  47%|████▋     | 11508000/24229575 [07:55<08:38, 24538.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11511000/24229575 [07:56<08:45, 24199.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11521000/24229575 [07:56<05:19, 39786.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11526000/24229575 [07:56<07:01, 30129.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11530000/24229575 [07:56<08:46, 24125.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11534000/24229575 [07:56<08:32, 24786.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11539000/24229575 [07:57<07:21, 28747.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11543000/24229575 [07:57<08:00, 26385.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11549000/24229575 [07:57<06:41, 31604.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11553000/24229575 [07:57<06:58, 30286.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11557000/24229575 [07:57<06:39, 31755.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11562000/24229575 [07:57<06:12, 34027.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11566000/24229575 [07:57<06:09, 34298.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11570000/24229575 [07:58<08:06, 26028.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11575000/24229575 [07:58<08:16, 25485.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11578000/24229575 [07:58<08:12, 25702.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11581000/24229575 [07:58<10:15, 20554.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11584000/24229575 [07:58<09:28, 22257.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11587000/24229575 [07:58<10:31, 20018.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11592000/24229575 [07:59<08:56, 23555.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11595000/24229575 [07:59<10:09, 20726.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11598000/24229575 [07:59<09:54, 21235.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11603000/24229575 [07:59<08:37, 24418.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11609000/24229575 [07:59<07:15, 28995.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11612000/24229575 [07:59<07:27, 28180.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11615000/24229575 [08:00<09:06, 23071.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11618000/24229575 [08:00<09:59, 21052.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11622000/24229575 [08:00<09:42, 21636.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11625000/24229575 [08:00<10:35, 19825.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11630000/24229575 [08:00<08:34, 24500.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11634000/24229575 [08:00<08:12, 25586.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11638000/24229575 [08:00<07:58, 26324.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11642000/24229575 [08:01<08:09, 25690.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11648000/24229575 [08:01<06:20, 33053.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11654000/24229575 [08:01<05:29, 38142.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11659000/24229575 [08:01<06:17, 33338.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11664000/24229575 [08:01<05:39, 37016.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11669000/24229575 [08:01<06:03, 34573.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11673000/24229575 [08:01<06:10, 33873.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11677000/24229575 [08:02<08:32, 24474.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11680000/24229575 [08:02<10:15, 20388.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11683000/24229575 [08:02<11:24, 18333.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11688000/24229575 [08:02<10:02, 20804.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11693000/24229575 [08:02<08:04, 25862.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11697000/24229575 [08:03<07:36, 27431.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11701000/24229575 [08:03<07:49, 26680.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11705000/24229575 [08:03<07:36, 27422.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11708000/24229575 [08:03<08:18, 25138.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11711000/24229575 [08:03<10:17, 20259.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11714000/24229575 [08:03<11:01, 18922.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11717000/24229575 [08:04<10:53, 19133.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11722000/24229575 [08:04<10:10, 20493.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11726000/24229575 [08:04<09:19, 22341.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11730000/24229575 [08:04<08:28, 24575.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11733000/24229575 [08:04<08:38, 24123.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11738000/24229575 [08:04<07:52, 26415.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11742000/24229575 [08:05<07:13, 28830.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11747000/24229575 [08:05<06:22, 32625.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  48%|████▊     | 11751000/24229575 [08:05<06:11, 33633.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11757000/24229575 [08:05<05:39, 36774.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11761000/24229575 [08:05<06:10, 33611.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11765000/24229575 [08:05<06:06, 34015.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11769000/24229575 [08:05<06:57, 29815.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11773000/24229575 [08:06<08:28, 24484.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11776000/24229575 [08:06<09:42, 21391.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11779000/24229575 [08:06<09:26, 21961.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11783000/24229575 [08:06<08:22, 24785.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11786000/24229575 [08:06<09:26, 21953.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11790000/24229575 [08:06<09:17, 22312.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11796000/24229575 [08:06<07:00, 29570.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11800000/24229575 [08:07<08:28, 24434.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11806000/24229575 [08:07<06:49, 30304.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▊     | 11810000/24229575 [08:07<08:06, 25511.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11813000/24229575 [08:07<08:40, 23847.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11816000/24229575 [08:07<08:29, 24346.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11819000/24229575 [08:08<10:01, 20636.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11822000/24229575 [08:08<09:35, 21564.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11825000/24229575 [08:08<09:22, 22033.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11830000/24229575 [08:08<08:49, 23420.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11834000/24229575 [08:08<08:39, 23840.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11838000/24229575 [08:08<08:03, 25618.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11845000/24229575 [08:08<06:01, 34218.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11849000/24229575 [08:09<07:12, 28636.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11853000/24229575 [08:09<07:01, 29364.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11857000/24229575 [08:09<06:38, 31037.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11861000/24229575 [08:09<07:20, 28060.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11864000/24229575 [08:09<08:07, 25352.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11868000/24229575 [08:09<08:04, 25491.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11873000/24229575 [08:09<06:41, 30749.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11877000/24229575 [08:10<08:31, 24157.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11881000/24229575 [08:10<08:29, 24241.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11885000/24229575 [08:10<07:44, 26578.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11888000/24229575 [08:10<09:10, 22425.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11893000/24229575 [08:10<08:04, 25445.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11896000/24229575 [08:10<08:51, 23193.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11902000/24229575 [08:11<07:37, 26974.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11906000/24229575 [08:11<07:26, 27618.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11909000/24229575 [08:11<07:19, 28006.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11913000/24229575 [08:11<07:09, 28705.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11916000/24229575 [08:11<10:35, 19382.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11919000/24229575 [08:11<09:40, 21211.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11922000/24229575 [08:12<09:37, 21302.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11926000/24229575 [08:12<08:40, 23648.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11930000/24229575 [08:12<07:38, 26849.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11934000/24229575 [08:12<07:11, 28489.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11939000/24229575 [08:12<06:23, 32011.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11943000/24229575 [08:12<07:02, 29049.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11947000/24229575 [08:12<07:36, 26930.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11950000/24229575 [08:12<07:31, 27220.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11953000/24229575 [08:13<08:33, 23885.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11956000/24229575 [08:13<08:13, 24882.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11962000/24229575 [08:13<06:54, 29605.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11965000/24229575 [08:13<07:59, 25588.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11969000/24229575 [08:13<07:10, 28506.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11975000/24229575 [08:13<06:27, 31594.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11979000/24229575 [08:14<08:04, 25292.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11983000/24229575 [08:14<07:48, 26163.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11986000/24229575 [08:14<08:41, 23473.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  49%|████▉     | 11990000/24229575 [08:14<08:41, 23464.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 11994000/24229575 [08:14<08:02, 25379.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 11997000/24229575 [08:14<07:47, 26176.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12002000/24229575 [08:14<06:45, 30176.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12006000/24229575 [08:15<09:48, 20774.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12011000/24229575 [08:15<09:05, 22394.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12016000/24229575 [08:15<07:27, 27316.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12020000/24229575 [08:15<07:12, 28224.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12024000/24229575 [08:15<07:41, 26453.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12027000/24229575 [08:15<07:40, 26510.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12030000/24229575 [08:16<07:29, 27128.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12033000/24229575 [08:16<07:52, 25799.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12037000/24229575 [08:16<07:43, 26323.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12041000/24229575 [08:16<06:56, 29231.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12045000/24229575 [08:16<08:10, 24820.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12049000/24229575 [08:16<07:28, 27169.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12052000/24229575 [08:16<07:35, 26714.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12055000/24229575 [08:17<09:52, 20545.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12058000/24229575 [08:17<09:23, 21584.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12063000/24229575 [08:17<07:17, 27797.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12067000/24229575 [08:17<07:42, 26278.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12075000/24229575 [08:17<06:24, 31608.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12081000/24229575 [08:17<06:08, 32987.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12085000/24229575 [08:17<05:57, 33938.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12089000/24229575 [08:18<07:38, 26465.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12092000/24229575 [08:18<10:18, 19625.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12095000/24229575 [08:18<10:02, 20149.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12099000/24229575 [08:18<08:33, 23642.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12103000/24229575 [08:18<07:35, 26626.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12107000/24229575 [08:18<07:16, 27760.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12111000/24229575 [08:19<07:37, 26496.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|████▉     | 12114000/24229575 [08:19<08:23, 24062.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12117000/24229575 [08:19<09:02, 22340.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12121000/24229575 [08:19<08:05, 24927.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12124000/24229575 [08:19<08:04, 24968.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12128000/24229575 [08:19<08:39, 23297.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12131000/24229575 [08:20<08:15, 24414.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12135000/24229575 [08:20<07:19, 27500.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12138000/24229575 [08:20<07:46, 25901.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12141000/24229575 [08:20<10:16, 19621.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12144000/24229575 [08:20<10:16, 19594.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12148000/24229575 [08:20<08:51, 22733.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12153000/24229575 [08:20<07:39, 26263.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12158000/24229575 [08:21<06:22, 31538.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12164000/24229575 [08:21<05:51, 34318.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12168000/24229575 [08:21<06:22, 31564.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12173000/24229575 [08:21<05:50, 34390.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12177000/24229575 [08:21<05:55, 33861.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12181000/24229575 [08:21<06:13, 32300.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12185000/24229575 [08:21<07:54, 25382.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12188000/24229575 [08:22<11:08, 18008.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12191000/24229575 [08:22<11:41, 17149.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12195000/24229575 [08:22<10:26, 19210.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12199000/24229575 [08:22<08:53, 22536.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12206000/24229575 [08:22<07:18, 27433.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12213000/24229575 [08:23<06:05, 32853.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12217000/24229575 [08:23<07:13, 27699.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12221000/24229575 [08:23<07:32, 26527.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12225000/24229575 [08:23<07:16, 27523.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12228000/24229575 [08:23<08:02, 24898.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12231000/24229575 [08:24<10:28, 19078.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  50%|█████     | 12234000/24229575 [08:24<10:15, 19475.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12237000/24229575 [08:24<09:35, 20856.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12240000/24229575 [08:24<09:36, 20792.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12243000/24229575 [08:24<08:57, 22303.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12251000/24229575 [08:24<05:46, 34567.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12258000/24229575 [08:24<05:24, 36854.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12263000/24229575 [08:24<05:18, 37519.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12267000/24229575 [08:25<06:47, 29342.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12271000/24229575 [08:25<06:45, 29458.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12275000/24229575 [08:25<07:56, 25104.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12278000/24229575 [08:25<08:31, 23351.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12281000/24229575 [08:25<08:21, 23837.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12285000/24229575 [08:25<07:31, 26464.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12288000/24229575 [08:26<08:11, 24311.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12292000/24229575 [08:26<07:49, 25438.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12295000/24229575 [08:26<09:35, 20722.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12298000/24229575 [08:26<08:49, 22549.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12305000/24229575 [08:26<07:36, 26093.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12309000/24229575 [08:26<07:23, 26860.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12312000/24229575 [08:27<07:47, 25466.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12315000/24229575 [08:27<08:59, 22090.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12321000/24229575 [08:27<07:13, 27446.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12324000/24229575 [08:27<10:47, 18399.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12329000/24229575 [08:27<09:01, 21963.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12332000/24229575 [08:28<09:08, 21686.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12336000/24229575 [08:28<08:11, 24216.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12342000/24229575 [08:28<06:31, 30357.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12346000/24229575 [08:28<06:16, 31558.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12355000/24229575 [08:28<05:13, 37924.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12359000/24229575 [08:28<05:37, 35182.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12363000/24229575 [08:28<06:12, 31894.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12367000/24229575 [08:29<08:12, 24107.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12374000/24229575 [08:29<07:57, 24804.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12377000/24229575 [08:29<08:17, 23836.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12380000/24229575 [08:29<08:22, 23601.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12384000/24229575 [08:29<07:50, 25199.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12390000/24229575 [08:29<06:08, 32120.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12395000/24229575 [08:30<05:32, 35584.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12399000/24229575 [08:30<09:46, 20183.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12402000/24229575 [08:30<10:51, 18145.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12407000/24229575 [08:30<08:41, 22651.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12411000/24229575 [08:31<09:03, 21758.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12414000/24229575 [08:31<08:34, 22948.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████     | 12417000/24229575 [08:31<08:23, 23481.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12420000/24229575 [08:31<08:42, 22586.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12423000/24229575 [08:31<08:30, 23132.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12427000/24229575 [08:31<07:44, 25412.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12430000/24229575 [08:31<07:34, 25980.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12435000/24229575 [08:31<06:25, 30594.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12440000/24229575 [08:31<05:45, 34098.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12445000/24229575 [08:32<05:50, 33668.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12449000/24229575 [08:32<07:38, 25710.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12452000/24229575 [08:32<07:29, 26221.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12457000/24229575 [08:32<06:41, 29321.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12463000/24229575 [08:32<06:26, 30435.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12467000/24229575 [08:33<07:45, 25262.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12470000/24229575 [08:33<07:44, 25295.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  51%|█████▏    | 12475000/24229575 [08:33<06:51, 28532.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12479000/24229575 [08:33<07:47, 25136.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12482000/24229575 [08:33<08:32, 22929.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12487000/24229575 [08:33<07:27, 26222.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12491000/24229575 [08:33<06:57, 28106.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12494000/24229575 [08:34<07:39, 25542.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12497000/24229575 [08:34<13:16, 14723.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12502000/24229575 [08:34<09:55, 19698.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12506000/24229575 [08:34<09:01, 21636.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12509000/24229575 [08:34<09:00, 21692.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12513000/24229575 [08:35<08:01, 24317.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12517000/24229575 [08:35<07:08, 27304.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12521000/24229575 [08:35<06:32, 29835.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12530000/24229575 [08:35<04:39, 41889.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12535000/24229575 [08:35<06:02, 32273.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12540000/24229575 [08:35<05:56, 32760.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12545000/24229575 [08:35<06:14, 31192.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12549000/24229575 [08:36<06:50, 28429.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12553000/24229575 [08:36<08:13, 23645.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12558000/24229575 [08:36<06:52, 28312.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12562000/24229575 [08:36<07:25, 26201.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12565000/24229575 [08:36<08:41, 22376.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12568000/24229575 [08:37<08:22, 23197.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12572000/24229575 [08:37<07:44, 25088.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12576000/24229575 [08:37<07:17, 26650.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12580000/24229575 [08:37<07:36, 25499.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12583000/24229575 [08:37<08:09, 23781.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12586000/24229575 [08:37<08:53, 21829.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12589000/24229575 [08:37<09:22, 20683.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12592000/24229575 [08:38<10:34, 18340.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12594000/24229575 [08:38<10:25, 18614.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12597000/24229575 [08:38<09:20, 20738.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12600000/24229575 [08:38<09:53, 19607.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12607000/24229575 [08:38<06:30, 29746.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12611000/24229575 [08:38<06:20, 30533.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12615000/24229575 [08:38<07:36, 25436.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12622000/24229575 [08:39<05:36, 34447.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12626000/24229575 [08:39<06:34, 29428.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12633000/24229575 [08:39<05:20, 36148.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12638000/24229575 [08:39<05:49, 33132.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12642000/24229575 [08:39<07:21, 26272.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12646000/24229575 [08:40<08:36, 22410.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12651000/24229575 [08:40<07:19, 26329.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12655000/24229575 [08:40<06:46, 28461.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12659000/24229575 [08:40<06:53, 27963.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12663000/24229575 [08:40<08:20, 23089.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12667000/24229575 [08:40<07:20, 26241.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12672000/24229575 [08:40<07:09, 26901.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12675000/24229575 [08:41<07:19, 26289.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12678000/24229575 [08:41<07:56, 24223.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12681000/24229575 [08:41<07:46, 24768.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12684000/24229575 [08:41<08:39, 22224.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12689000/24229575 [08:41<06:52, 27998.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12693000/24229575 [08:41<08:23, 22928.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12698000/24229575 [08:42<07:02, 27300.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12702000/24229575 [08:42<09:08, 21006.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12707000/24229575 [08:42<09:01, 21293.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12711000/24229575 [08:42<08:48, 21784.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12716000/24229575 [08:42<07:32, 25435.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  52%|█████▏    | 12719000/24229575 [08:43<08:12, 23380.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12723000/24229575 [08:43<07:26, 25757.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12726000/24229575 [08:43<07:25, 25828.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12729000/24229575 [08:43<07:27, 25711.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12733000/24229575 [08:43<06:58, 27477.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12738000/24229575 [08:43<07:00, 27328.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12746000/24229575 [08:43<05:25, 35295.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12750000/24229575 [08:43<05:44, 33332.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12754000/24229575 [08:44<06:01, 31729.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12760000/24229575 [08:44<05:46, 33147.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12764000/24229575 [08:44<07:00, 27256.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12767000/24229575 [08:44<07:08, 26727.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12770000/24229575 [08:44<09:40, 19727.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12773000/24229575 [08:45<09:47, 19516.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12777000/24229575 [08:45<09:15, 20629.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12783000/24229575 [08:45<06:53, 27658.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12787000/24229575 [08:45<06:27, 29503.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12791000/24229575 [08:45<06:15, 30422.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12795000/24229575 [08:45<06:31, 29231.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12799000/24229575 [08:45<06:55, 27523.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12802000/24229575 [08:46<07:01, 27118.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12805000/24229575 [08:46<07:57, 23922.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12808000/24229575 [08:46<08:28, 22451.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12811000/24229575 [08:46<07:57, 23910.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12814000/24229575 [08:46<07:31, 25261.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12817000/24229575 [08:46<08:11, 23214.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12820000/24229575 [08:46<09:09, 20745.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12824000/24229575 [08:47<08:17, 22930.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12827000/24229575 [08:47<08:02, 23621.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12833000/24229575 [08:47<05:58, 31768.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12837000/24229575 [08:47<07:23, 25661.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12840000/24229575 [08:47<08:05, 23477.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12844000/24229575 [08:47<07:05, 26746.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12847000/24229575 [08:47<06:54, 27484.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12852000/24229575 [08:47<05:46, 32796.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12857000/24229575 [08:48<05:16, 35881.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12861000/24229575 [08:48<06:15, 30310.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12865000/24229575 [08:48<08:49, 21450.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12870000/24229575 [08:48<09:37, 19654.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12875000/24229575 [08:48<07:43, 24513.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12879000/24229575 [08:49<06:59, 27037.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12884000/24229575 [08:49<06:28, 29182.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12890000/24229575 [08:49<05:36, 33673.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12894000/24229575 [08:49<05:28, 34461.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12898000/24229575 [08:49<08:39, 21819.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12901000/24229575 [08:49<08:30, 22190.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12904000/24229575 [08:50<08:39, 21818.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12907000/24229575 [08:50<08:47, 21473.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12911000/24229575 [08:50<07:35, 24868.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12915000/24229575 [08:50<07:07, 26485.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12919000/24229575 [08:50<08:36, 21914.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12922000/24229575 [08:50<08:45, 21534.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12927000/24229575 [08:50<06:59, 26915.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12931000/24229575 [08:51<07:03, 26707.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12934000/24229575 [08:51<08:05, 23242.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12942000/24229575 [08:51<05:41, 33040.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12946000/24229575 [08:51<06:16, 29955.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12950000/24229575 [08:51<07:53, 23811.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12953000/24229575 [08:51<07:43, 24345.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12956000/24229575 [08:52<08:15, 22744.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12959000/24229575 [08:52<07:57, 23588.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  53%|█████▎    | 12962000/24229575 [08:52<07:38, 24577.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 12966000/24229575 [08:52<07:26, 25243.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 12970000/24229575 [08:52<06:32, 28670.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 12976000/24229575 [08:52<05:13, 35918.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 12980000/24229575 [08:52<05:19, 35218.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 12984000/24229575 [08:53<05:57, 31462.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 12988000/24229575 [08:53<07:41, 24347.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 12992000/24229575 [08:53<08:36, 21744.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 12995000/24229575 [08:53<08:42, 21492.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 13000000/24229575 [08:53<08:06, 23084.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 13004000/24229575 [08:54<08:12, 22815.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 13007000/24229575 [08:54<07:59, 23386.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 13011000/24229575 [08:54<07:25, 25199.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 13015000/24229575 [08:54<07:08, 26153.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 13018000/24229575 [08:54<07:57, 23470.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▎    | 13022000/24229575 [08:54<08:15, 22625.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13030000/24229575 [08:54<05:56, 31426.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13034000/24229575 [08:55<06:33, 28453.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13037000/24229575 [08:55<07:51, 23739.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13040000/24229575 [08:55<07:55, 23517.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13043000/24229575 [08:55<08:29, 21963.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13048000/24229575 [08:55<07:06, 26244.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13051000/24229575 [08:55<07:08, 26100.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13056000/24229575 [08:55<06:32, 28476.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13059000/24229575 [08:56<06:46, 27481.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13064000/24229575 [08:56<06:20, 29318.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13067000/24229575 [08:56<06:23, 29141.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13073000/24229575 [08:56<05:08, 36168.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13077000/24229575 [08:56<06:29, 28611.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13082000/24229575 [08:57<10:35, 17543.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13087000/24229575 [08:57<08:53, 20885.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13091000/24229575 [08:57<07:52, 23557.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13095000/24229575 [08:57<07:36, 24407.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13098000/24229575 [08:57<07:51, 23602.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13102000/24229575 [08:57<07:00, 26459.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13108000/24229575 [08:57<06:01, 30798.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13112000/24229575 [08:58<06:35, 28099.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13116000/24229575 [08:58<07:34, 24425.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13121000/24229575 [08:58<06:50, 27034.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13124000/24229575 [08:58<06:43, 27490.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13127000/24229575 [08:58<07:27, 24783.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13133000/24229575 [08:58<06:18, 29325.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13137000/24229575 [08:59<07:34, 24426.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13140000/24229575 [08:59<07:58, 23152.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13143000/24229575 [08:59<07:44, 23873.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13146000/24229575 [08:59<07:29, 24642.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13151000/24229575 [08:59<06:21, 29005.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13160000/24229575 [08:59<04:16, 43113.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13165000/24229575 [09:00<07:03, 26131.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13169000/24229575 [09:00<08:00, 23012.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13173000/24229575 [09:00<07:40, 24015.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13176000/24229575 [09:00<07:34, 24313.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13179000/24229575 [09:00<08:04, 22824.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13183000/24229575 [09:00<07:08, 25807.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13186000/24229575 [09:01<08:02, 22865.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13191000/24229575 [09:01<07:34, 24312.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13196000/24229575 [09:01<06:17, 29217.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13200000/24229575 [09:01<06:01, 30539.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  54%|█████▍    | 13204000/24229575 [09:01<07:12, 25471.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13210000/24229575 [09:01<06:05, 30121.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13214000/24229575 [09:02<07:15, 25321.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13217000/24229575 [09:02<09:20, 19644.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13221000/24229575 [09:02<08:23, 21880.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13225000/24229575 [09:02<07:23, 24785.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13233000/24229575 [09:02<05:24, 33852.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13237000/24229575 [09:02<06:19, 28941.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13242000/24229575 [09:03<05:47, 31586.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13247000/24229575 [09:03<05:24, 33811.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13251000/24229575 [09:03<05:52, 31178.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13255000/24229575 [09:03<06:30, 28110.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13258000/24229575 [09:03<08:47, 20787.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13262000/24229575 [09:03<07:32, 24263.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13265000/24229575 [09:04<07:59, 22880.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13268000/24229575 [09:04<07:55, 23065.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13271000/24229575 [09:04<09:12, 19844.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13275000/24229575 [09:04<09:07, 19992.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13282000/24229575 [09:04<06:34, 27761.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13286000/24229575 [09:04<06:01, 30231.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13291000/24229575 [09:05<06:25, 28373.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13295000/24229575 [09:05<06:15, 29087.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13299000/24229575 [09:05<06:17, 28993.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13303000/24229575 [09:05<06:46, 26867.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13306000/24229575 [09:05<07:40, 23712.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13309000/24229575 [09:05<07:21, 24746.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13312000/24229575 [09:05<07:45, 23464.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13315000/24229575 [09:06<08:25, 21604.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13318000/24229575 [09:06<08:16, 21965.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13321000/24229575 [09:06<08:05, 22476.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▍    | 13326000/24229575 [09:06<06:22, 28475.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13331000/24229575 [09:06<05:43, 31737.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13335000/24229575 [09:06<06:38, 27362.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13342000/24229575 [09:06<05:48, 31213.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13346000/24229575 [09:07<06:34, 27580.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13349000/24229575 [09:07<06:31, 27819.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13353000/24229575 [09:07<07:20, 24718.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13357000/24229575 [09:07<06:43, 26961.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13360000/24229575 [09:07<06:48, 26595.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13363000/24229575 [09:07<09:19, 19418.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13367000/24229575 [09:08<08:10, 22137.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13370000/24229575 [09:08<08:05, 22387.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13374000/24229575 [09:08<07:08, 25319.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13379000/24229575 [09:08<06:30, 27817.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13382000/24229575 [09:08<06:23, 28255.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13385000/24229575 [09:08<07:12, 25098.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13390000/24229575 [09:08<06:26, 28038.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13394000/24229575 [09:08<05:53, 30690.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13398000/24229575 [09:09<05:49, 30964.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13402000/24229575 [09:09<07:49, 23068.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13405000/24229575 [09:09<08:23, 21477.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13410000/24229575 [09:09<06:50, 26333.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13413000/24229575 [09:09<06:59, 25764.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13417000/24229575 [09:09<06:40, 27014.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13420000/24229575 [09:10<06:55, 25992.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13426000/24229575 [09:10<05:58, 30095.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13430000/24229575 [09:10<06:55, 25995.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13433000/24229575 [09:10<06:55, 25985.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13436000/24229575 [09:10<08:30, 21125.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13441000/24229575 [09:10<06:45, 26613.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  55%|█████▌    | 13445000/24229575 [09:10<06:28, 27755.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13449000/24229575 [09:11<06:55, 25920.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13453000/24229575 [09:11<06:50, 26234.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13458000/24229575 [09:11<06:34, 27289.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13461000/24229575 [09:11<08:54, 20153.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13464000/24229575 [09:11<09:03, 19814.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13467000/24229575 [09:12<08:21, 21443.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13471000/24229575 [09:12<07:11, 24950.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13476000/24229575 [09:12<06:46, 26470.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13481000/24229575 [09:12<06:34, 27223.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13486000/24229575 [09:12<05:44, 31195.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13491000/24229575 [09:12<05:32, 32309.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13495000/24229575 [09:12<05:37, 31839.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13499000/24229575 [09:13<07:50, 22809.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13502000/24229575 [09:13<07:26, 24038.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13505000/24229575 [09:13<07:11, 24865.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13510000/24229575 [09:13<06:23, 27986.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13514000/24229575 [09:13<09:41, 18427.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13518000/24229575 [09:14<08:14, 21642.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13525000/24229575 [09:14<05:49, 30597.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13532000/24229575 [09:14<04:49, 36890.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13537000/24229575 [09:14<05:46, 30881.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13541000/24229575 [09:14<06:49, 26117.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13545000/24229575 [09:14<07:34, 23512.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13548000/24229575 [09:15<07:38, 23290.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13551000/24229575 [09:15<07:40, 23199.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13557000/24229575 [09:15<06:12, 28653.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13561000/24229575 [09:15<06:39, 26684.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13564000/24229575 [09:15<06:35, 26954.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13567000/24229575 [09:15<06:45, 26306.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13571000/24229575 [09:16<08:50, 20079.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13576000/24229575 [09:16<07:09, 24779.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13579000/24229575 [09:16<08:16, 21436.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13584000/24229575 [09:16<07:02, 25181.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13592000/24229575 [09:16<05:23, 32912.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13596000/24229575 [09:16<08:02, 22017.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13601000/24229575 [09:17<07:26, 23822.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13605000/24229575 [09:17<06:42, 26381.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13610000/24229575 [09:17<05:59, 29555.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13615000/24229575 [09:17<05:26, 32471.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13619000/24229575 [09:17<05:44, 30832.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13623000/24229575 [09:17<06:37, 26705.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13626000/24229575 [09:17<06:49, 25896.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▌    | 13629000/24229575 [09:18<06:47, 26016.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13633000/24229575 [09:18<06:09, 28662.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13637000/24229575 [09:18<06:41, 26374.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13641000/24229575 [09:18<06:07, 28844.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13645000/24229575 [09:18<07:00, 25189.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13648000/24229575 [09:18<08:18, 21220.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13653000/24229575 [09:19<06:41, 26360.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13656000/24229575 [09:19<07:08, 24672.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13659000/24229575 [09:19<08:43, 20201.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13663000/24229575 [09:19<07:26, 23663.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13666000/24229575 [09:19<08:36, 20444.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13669000/24229575 [09:19<08:59, 19588.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13674000/24229575 [09:20<07:16, 24172.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13677000/24229575 [09:20<08:08, 21611.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13680000/24229575 [09:20<08:25, 20870.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  56%|█████▋    | 13687000/24229575 [09:20<05:39, 31031.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13692000/24229575 [09:20<05:48, 30279.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13697000/24229575 [09:20<05:18, 33042.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13701000/24229575 [09:20<05:37, 31196.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13705000/24229575 [09:21<06:55, 25301.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13708000/24229575 [09:21<07:48, 22464.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13715000/24229575 [09:21<06:01, 29084.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13719000/24229575 [09:21<05:44, 30513.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13723000/24229575 [09:21<06:21, 27542.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13728000/24229575 [09:21<05:41, 30774.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13732000/24229575 [09:22<06:38, 26371.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13735000/24229575 [09:22<06:48, 25667.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13739000/24229575 [09:22<06:28, 26992.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13742000/24229575 [09:22<06:26, 27134.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13746000/24229575 [09:22<07:20, 23777.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13749000/24229575 [09:22<07:49, 22303.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13752000/24229575 [09:23<10:03, 17353.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13758000/24229575 [09:23<07:16, 23979.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13761000/24229575 [09:23<07:51, 22207.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13767000/24229575 [09:23<07:06, 24525.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13770000/24229575 [09:23<06:55, 25200.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13774000/24229575 [09:23<07:00, 24851.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13777000/24229575 [09:24<08:27, 20584.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13781000/24229575 [09:24<08:19, 20921.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13786000/24229575 [09:24<07:36, 22868.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13793000/24229575 [09:24<05:43, 30411.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13797000/24229575 [09:24<05:39, 30750.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13801000/24229575 [09:24<06:01, 28843.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13805000/24229575 [09:25<06:10, 28102.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13812000/24229575 [09:25<05:10, 33599.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13816000/24229575 [09:25<05:29, 31591.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13820000/24229575 [09:25<05:50, 29713.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13824000/24229575 [09:25<05:34, 31098.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13828000/24229575 [09:25<05:55, 29227.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13831000/24229575 [09:25<06:16, 27646.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13836000/24229575 [09:26<06:23, 27088.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13839000/24229575 [09:26<06:46, 25589.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13842000/24229575 [09:26<06:45, 25631.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13846000/24229575 [09:26<06:55, 24969.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13850000/24229575 [09:26<07:08, 24211.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13853000/24229575 [09:26<07:28, 23116.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13857000/24229575 [09:27<07:40, 22521.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13860000/24229575 [09:27<09:09, 18869.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13863000/24229575 [09:27<08:35, 20125.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13866000/24229575 [09:27<08:49, 19554.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13870000/24229575 [09:27<08:38, 19977.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13873000/24229575 [09:27<08:09, 21178.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13876000/24229575 [09:27<08:00, 21536.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13879000/24229575 [09:28<08:56, 19299.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13882000/24229575 [09:28<08:27, 20405.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13888000/24229575 [09:28<05:57, 28923.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13894000/24229575 [09:28<05:15, 32753.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13899000/24229575 [09:28<05:05, 33799.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13907000/24229575 [09:28<03:58, 43331.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13912000/24229575 [09:29<05:12, 33005.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13916000/24229575 [09:29<05:16, 32612.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13920000/24229575 [09:29<05:59, 28672.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13924000/24229575 [09:29<06:43, 25534.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  57%|█████▋    | 13929000/24229575 [09:29<06:08, 27958.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13933000/24229575 [09:29<05:58, 28724.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13937000/24229575 [09:30<06:27, 26590.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13941000/24229575 [09:30<06:17, 27263.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13944000/24229575 [09:30<07:00, 24459.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13947000/24229575 [09:30<09:14, 18535.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13950000/24229575 [09:30<08:29, 20184.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13953000/24229575 [09:30<08:49, 19425.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13956000/24229575 [09:31<08:36, 19875.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13959000/24229575 [09:31<08:03, 21242.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13962000/24229575 [09:31<08:23, 20405.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13965000/24229575 [09:31<07:40, 22267.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13970000/24229575 [09:31<07:19, 23369.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13973000/24229575 [09:31<07:40, 22287.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13978000/24229575 [09:31<06:32, 26143.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13985000/24229575 [09:32<04:57, 34420.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13989000/24229575 [09:32<05:00, 34100.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13994000/24229575 [09:32<04:50, 35257.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 13998000/24229575 [09:32<05:07, 33251.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14002000/24229575 [09:32<04:53, 34869.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14006000/24229575 [09:32<05:56, 28694.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14010000/24229575 [09:32<05:58, 28483.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14015000/24229575 [09:32<05:14, 32513.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14019000/24229575 [09:33<05:12, 32638.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14023000/24229575 [09:33<06:05, 27951.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14028000/24229575 [09:33<05:18, 31997.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14032000/24229575 [09:33<06:26, 26364.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14035000/24229575 [09:33<06:56, 24469.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14038000/24229575 [09:34<11:02, 15385.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14041000/24229575 [09:34<10:58, 15462.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14047000/24229575 [09:34<07:36, 22302.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14050000/24229575 [09:34<07:24, 22891.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14053000/24229575 [09:34<07:58, 21268.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14056000/24229575 [09:34<07:31, 22521.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14059000/24229575 [09:35<07:19, 23121.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14062000/24229575 [09:35<07:18, 23189.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14066000/24229575 [09:35<06:26, 26293.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14070000/24229575 [09:35<05:55, 28566.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14073000/24229575 [09:35<06:36, 25589.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14080000/24229575 [09:35<04:43, 35832.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14084000/24229575 [09:35<05:13, 32375.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14088000/24229575 [09:35<05:06, 33051.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14093000/24229575 [09:36<04:37, 36537.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14097000/24229575 [09:36<04:58, 33985.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14101000/24229575 [09:36<07:10, 23521.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14107000/24229575 [09:36<05:33, 30381.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14111000/24229575 [09:36<06:09, 27384.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14117000/24229575 [09:36<05:14, 32184.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14121000/24229575 [09:37<06:06, 27602.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14125000/24229575 [09:37<07:30, 22453.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14128000/24229575 [09:37<08:15, 20403.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14131000/24229575 [09:37<08:06, 20742.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14134000/24229575 [09:37<09:56, 16928.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14138000/24229575 [09:38<09:24, 17884.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14140000/24229575 [09:38<09:19, 18028.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14145000/24229575 [09:38<07:36, 22067.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14148000/24229575 [09:38<08:03, 20835.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14151000/24229575 [09:38<07:24, 22681.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14156000/24229575 [09:38<06:00, 27976.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14160000/24229575 [09:38<05:48, 28914.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14164000/24229575 [09:39<06:18, 26586.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14167000/24229575 [09:39<06:29, 25813.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  58%|█████▊    | 14174000/24229575 [09:39<04:47, 34930.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14178000/24229575 [09:39<04:46, 35066.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14182000/24229575 [09:39<05:41, 29424.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14187000/24229575 [09:39<05:30, 30378.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14191000/24229575 [09:39<06:08, 27234.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14198000/24229575 [09:40<04:56, 33862.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14202000/24229575 [09:40<05:25, 30832.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14206000/24229575 [09:40<07:04, 23606.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14210000/24229575 [09:40<06:37, 25194.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14214000/24229575 [09:40<06:33, 25436.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14217000/24229575 [09:41<08:20, 19991.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14220000/24229575 [09:41<07:39, 21778.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14223000/24229575 [09:41<07:33, 22084.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14226000/24229575 [09:41<07:18, 22826.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14229000/24229575 [09:41<08:02, 20736.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▊    | 14232000/24229575 [09:41<07:28, 22314.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14235000/24229575 [09:41<08:01, 20773.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14240000/24229575 [09:42<07:37, 21840.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14246000/24229575 [09:42<06:34, 25329.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14252000/24229575 [09:42<06:26, 25820.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14255000/24229575 [09:42<06:32, 25430.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14260000/24229575 [09:42<06:29, 25573.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14265000/24229575 [09:42<06:00, 27623.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14268000/24229575 [09:43<06:43, 24664.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14272000/24229575 [09:43<06:03, 27403.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14276000/24229575 [09:43<05:53, 28129.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14284000/24229575 [09:43<04:42, 35261.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14288000/24229575 [09:43<05:56, 27887.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14292000/24229575 [09:43<06:02, 27419.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14295000/24229575 [09:44<06:15, 26468.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14299000/24229575 [09:44<05:52, 28184.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14302000/24229575 [09:44<06:49, 24266.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14305000/24229575 [09:44<06:39, 24824.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14310000/24229575 [09:44<05:33, 29766.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14314000/24229575 [09:44<06:29, 25445.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14317000/24229575 [09:44<06:37, 24929.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14322000/24229575 [09:45<05:31, 29859.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14326000/24229575 [09:45<06:40, 24722.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14329000/24229575 [09:45<07:24, 22256.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14332000/24229575 [09:45<10:17, 16039.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14337000/24229575 [09:45<08:20, 19761.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14343000/24229575 [09:46<06:50, 24105.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14347000/24229575 [09:46<06:05, 27031.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14351000/24229575 [09:46<07:05, 23201.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14355000/24229575 [09:46<06:24, 25707.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14359000/24229575 [09:46<05:53, 27938.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14363000/24229575 [09:46<05:56, 27667.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14368000/24229575 [09:46<05:06, 32134.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14372000/24229575 [09:47<05:14, 31391.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14376000/24229575 [09:47<05:10, 31714.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14380000/24229575 [09:47<07:36, 21552.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14383000/24229575 [09:47<07:19, 22392.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14387000/24229575 [09:47<06:21, 25781.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14391000/24229575 [09:47<06:07, 26758.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14396000/24229575 [09:48<05:47, 28264.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14400000/24229575 [09:48<07:24, 22097.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14409000/24229575 [09:48<05:11, 31540.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  59%|█████▉    | 14413000/24229575 [09:48<06:17, 26028.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14418000/24229575 [09:48<05:31, 29573.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14422000/24229575 [09:49<06:27, 25296.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14425000/24229575 [09:49<06:52, 23790.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14428000/24229575 [09:49<08:19, 19606.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14431000/24229575 [09:49<08:16, 19740.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14434000/24229575 [09:49<07:40, 21277.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14437000/24229575 [09:49<07:52, 20711.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14441000/24229575 [09:49<06:40, 24425.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14448000/24229575 [09:50<05:06, 31914.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14452000/24229575 [09:50<05:41, 28653.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14457000/24229575 [09:50<06:23, 25489.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14460000/24229575 [09:50<06:31, 24953.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14463000/24229575 [09:50<06:17, 25873.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14466000/24229575 [09:50<07:50, 20745.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14469000/24229575 [09:51<08:18, 19585.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14472000/24229575 [09:51<07:33, 21516.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14475000/24229575 [09:51<07:22, 22053.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14479000/24229575 [09:51<06:58, 23302.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14484000/24229575 [09:51<05:35, 29039.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14489000/24229575 [09:51<05:24, 30002.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14495000/24229575 [09:51<04:29, 36127.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14499000/24229575 [09:52<04:54, 33048.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14503000/24229575 [09:52<06:58, 23231.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14507000/24229575 [09:52<06:11, 26149.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14511000/24229575 [09:52<06:19, 25603.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14515000/24229575 [09:52<06:01, 26894.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14520000/24229575 [09:52<05:45, 28139.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14524000/24229575 [09:53<07:17, 22206.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14527000/24229575 [09:53<07:02, 22965.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14530000/24229575 [09:53<07:41, 20998.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14533000/24229575 [09:53<07:43, 20915.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|█████▉    | 14537000/24229575 [09:53<06:39, 24233.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14542000/24229575 [09:53<06:06, 26445.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14547000/24229575 [09:54<05:34, 28982.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14551000/24229575 [09:54<06:27, 24984.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14556000/24229575 [09:54<06:10, 26114.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14559000/24229575 [09:54<06:09, 26150.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14562000/24229575 [09:54<09:39, 16676.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14565000/24229575 [09:55<09:48, 16410.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14573000/24229575 [09:55<06:13, 25864.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14577000/24229575 [09:55<06:20, 25394.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14582000/24229575 [09:55<05:44, 27980.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14589000/24229575 [09:55<05:14, 30625.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14593000/24229575 [09:55<05:16, 30435.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14598000/24229575 [09:56<05:09, 31090.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14602000/24229575 [09:56<06:13, 25765.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14605000/24229575 [09:56<06:23, 25080.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14609000/24229575 [09:56<06:27, 24812.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14612000/24229575 [09:56<07:18, 21916.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14617000/24229575 [09:56<05:52, 27284.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14621000/24229575 [09:56<05:21, 29869.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14625000/24229575 [09:57<05:45, 27772.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14629000/24229575 [09:57<06:37, 24146.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14632000/24229575 [09:57<06:59, 22867.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14637000/24229575 [09:57<06:44, 23714.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14642000/24229575 [09:57<06:14, 25581.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14645000/24229575 [09:58<06:22, 25045.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14649000/24229575 [09:58<05:56, 26839.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14652000/24229575 [09:58<08:59, 17736.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  60%|██████    | 14656000/24229575 [09:58<07:48, 20441.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14659000/24229575 [09:58<07:52, 20260.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14665000/24229575 [09:58<05:59, 26599.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14669000/24229575 [09:59<06:05, 26134.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14673000/24229575 [09:59<05:40, 28092.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14677000/24229575 [09:59<05:31, 28807.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14681000/24229575 [09:59<06:08, 25935.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14687000/24229575 [09:59<05:35, 28472.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14690000/24229575 [09:59<05:58, 26583.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14693000/24229575 [09:59<06:37, 23998.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14698000/24229575 [10:00<05:36, 28290.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14701000/24229575 [10:00<05:44, 27674.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14705000/24229575 [10:00<05:22, 29545.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14709000/24229575 [10:00<06:49, 23257.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14718000/24229575 [10:00<04:25, 35868.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14723000/24229575 [10:01<06:34, 24123.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14729000/24229575 [10:01<06:10, 25661.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14733000/24229575 [10:01<07:12, 21945.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14736000/24229575 [10:01<07:22, 21455.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14740000/24229575 [10:01<06:37, 23851.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14743000/24229575 [10:02<07:46, 20332.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14746000/24229575 [10:02<08:09, 19357.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14749000/24229575 [10:02<07:28, 21157.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14752000/24229575 [10:02<07:48, 20232.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14757000/24229575 [10:02<06:39, 23740.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14762000/24229575 [10:02<06:43, 23490.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14770000/24229575 [10:02<04:44, 33261.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14774000/24229575 [10:03<04:56, 31930.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14778000/24229575 [10:03<05:35, 28152.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14782000/24229575 [10:03<05:16, 29850.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14789000/24229575 [10:03<04:52, 32264.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14793000/24229575 [10:03<05:52, 26739.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14796000/24229575 [10:03<05:55, 26546.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14799000/24229575 [10:04<05:55, 26529.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14802000/24229575 [10:04<05:45, 27305.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14806000/24229575 [10:04<05:23, 29085.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14811000/24229575 [10:04<04:37, 33943.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14815000/24229575 [10:04<04:49, 32562.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14819000/24229575 [10:04<05:57, 26349.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14822000/24229575 [10:04<06:24, 24444.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14825000/24229575 [10:05<07:30, 20883.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14828000/24229575 [10:05<08:28, 18502.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14831000/24229575 [10:05<07:41, 20350.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14834000/24229575 [10:05<07:23, 21168.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████    | 14837000/24229575 [10:05<07:57, 19675.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14842000/24229575 [10:05<06:14, 25044.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14845000/24229575 [10:06<07:38, 20466.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14848000/24229575 [10:06<08:40, 18021.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14852000/24229575 [10:06<07:06, 21980.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14857000/24229575 [10:06<06:02, 25880.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14860000/24229575 [10:06<05:57, 26242.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14864000/24229575 [10:06<05:18, 29401.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14871000/24229575 [10:06<04:08, 37696.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14875000/24229575 [10:06<04:22, 35629.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14879000/24229575 [10:07<04:53, 31903.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14883000/24229575 [10:07<05:08, 30312.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14888000/24229575 [10:07<04:38, 33496.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14893000/24229575 [10:07<04:35, 33837.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  61%|██████▏   | 14897000/24229575 [10:07<05:59, 25937.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14903000/24229575 [10:07<05:06, 30475.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14907000/24229575 [10:08<05:45, 26958.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14911000/24229575 [10:08<06:10, 25127.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14914000/24229575 [10:08<07:23, 20987.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14918000/24229575 [10:08<07:26, 20869.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14921000/24229575 [10:08<06:59, 22178.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14924000/24229575 [10:08<06:55, 22396.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14927000/24229575 [10:09<06:28, 23950.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14930000/24229575 [10:09<07:42, 20110.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14934000/24229575 [10:09<07:32, 20561.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14937000/24229575 [10:09<07:52, 19682.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14940000/24229575 [10:09<08:49, 17546.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14944000/24229575 [10:10<07:27, 20766.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14950000/24229575 [10:10<05:48, 26655.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14953000/24229575 [10:10<06:06, 25304.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14958000/24229575 [10:10<05:13, 29612.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14962000/24229575 [10:10<04:58, 31061.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14968000/24229575 [10:10<04:29, 34394.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14973000/24229575 [10:10<04:19, 35727.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14979000/24229575 [10:10<03:44, 41252.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14984000/24229575 [10:11<04:13, 36522.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14988000/24229575 [10:11<04:47, 32099.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14992000/24229575 [10:11<04:59, 30851.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14996000/24229575 [10:11<05:54, 26011.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 14999000/24229575 [10:11<08:21, 18401.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15002000/24229575 [10:12<07:45, 19828.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15005000/24229575 [10:12<07:29, 20532.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15008000/24229575 [10:12<08:18, 18502.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15011000/24229575 [10:12<07:33, 20332.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15015000/24229575 [10:12<06:45, 22745.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15018000/24229575 [10:12<06:32, 23453.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15022000/24229575 [10:12<05:49, 26375.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15025000/24229575 [10:13<06:22, 24082.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15028000/24229575 [10:13<06:01, 25433.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15031000/24229575 [10:13<06:11, 24770.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15034000/24229575 [10:13<07:02, 21764.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15037000/24229575 [10:13<06:53, 22208.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15041000/24229575 [10:13<06:23, 23962.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15046000/24229575 [10:13<05:48, 26334.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15049000/24229575 [10:13<05:47, 26408.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15054000/24229575 [10:14<05:39, 27066.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15057000/24229575 [10:14<06:17, 24279.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15066000/24229575 [10:14<04:12, 36312.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15073000/24229575 [10:14<03:30, 43414.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15078000/24229575 [10:14<03:42, 41043.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15083000/24229575 [10:14<04:17, 35495.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15087000/24229575 [10:15<04:44, 32147.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15091000/24229575 [10:15<07:00, 21731.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15094000/24229575 [10:15<08:26, 18036.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15097000/24229575 [10:15<07:46, 19578.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15100000/24229575 [10:15<07:45, 19605.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15103000/24229575 [10:16<07:44, 19656.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15106000/24229575 [10:16<07:22, 20625.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15109000/24229575 [10:16<08:12, 18502.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15115000/24229575 [10:16<06:19, 24026.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15118000/24229575 [10:16<06:04, 25019.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15122000/24229575 [10:16<05:35, 27110.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15127000/24229575 [10:16<05:08, 29485.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15131000/24229575 [10:17<04:52, 31083.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15135000/24229575 [10:17<06:17, 24060.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15138000/24229575 [10:17<06:11, 24500.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  62%|██████▏   | 15143000/24229575 [10:17<05:27, 27715.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15147000/24229575 [10:17<05:14, 28891.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15151000/24229575 [10:17<04:50, 31241.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15155000/24229575 [10:18<06:38, 22751.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15159000/24229575 [10:18<06:01, 25082.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15162000/24229575 [10:18<05:57, 25330.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15165000/24229575 [10:18<07:01, 21486.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15170000/24229575 [10:18<06:00, 25146.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15179000/24229575 [10:18<03:59, 37777.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15184000/24229575 [10:18<04:45, 31718.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15188000/24229575 [10:19<05:11, 29028.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15192000/24229575 [10:19<06:05, 24711.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15195000/24229575 [10:19<07:09, 21040.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15199000/24229575 [10:19<06:34, 22911.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15202000/24229575 [10:19<07:12, 20891.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15205000/24229575 [10:20<08:03, 18683.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15209000/24229575 [10:20<06:42, 22394.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15213000/24229575 [10:20<06:10, 24350.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15217000/24229575 [10:20<05:48, 25891.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15222000/24229575 [10:20<05:06, 29434.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15226000/24229575 [10:20<05:30, 27235.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15230000/24229575 [10:20<05:10, 28938.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15234000/24229575 [10:21<05:51, 25572.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15238000/24229575 [10:21<05:18, 28187.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15243000/24229575 [10:21<05:00, 29869.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15247000/24229575 [10:21<05:30, 27215.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15252000/24229575 [10:21<05:22, 27811.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15255000/24229575 [10:21<06:38, 22505.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15258000/24229575 [10:22<07:47, 19188.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15261000/24229575 [10:22<07:45, 19246.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15265000/24229575 [10:22<06:38, 22500.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15270000/24229575 [10:22<05:27, 27333.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15274000/24229575 [10:22<05:21, 27875.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15277000/24229575 [10:22<05:25, 27475.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15281000/24229575 [10:22<05:41, 26195.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15287000/24229575 [10:23<05:14, 28421.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15291000/24229575 [10:23<04:57, 29996.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15295000/24229575 [10:23<05:05, 29205.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15298000/24229575 [10:23<06:11, 24027.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15302000/24229575 [10:23<05:51, 25388.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15305000/24229575 [10:23<06:11, 23998.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15309000/24229575 [10:24<05:33, 26711.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15313000/24229575 [10:24<05:36, 26534.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15318000/24229575 [10:24<04:41, 31641.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15322000/24229575 [10:24<04:25, 33588.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15326000/24229575 [10:24<05:36, 26477.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15330000/24229575 [10:24<06:30, 22785.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15333000/24229575 [10:25<06:48, 21779.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15338000/24229575 [10:25<05:25, 27313.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15342000/24229575 [10:25<05:27, 27168.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15346000/24229575 [10:25<06:02, 24501.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15349000/24229575 [10:25<06:51, 21555.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15352000/24229575 [10:25<06:55, 21364.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15355000/24229575 [10:25<06:53, 21448.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15358000/24229575 [10:26<06:42, 22023.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15361000/24229575 [10:26<08:28, 17427.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15366000/24229575 [10:26<06:24, 23062.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15370000/24229575 [10:26<05:45, 25624.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15375000/24229575 [10:26<05:04, 29042.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15379000/24229575 [10:26<04:57, 29769.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  63%|██████▎   | 15383000/24229575 [10:26<04:57, 29743.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15388000/24229575 [10:27<04:19, 34097.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15393000/24229575 [10:27<04:55, 29878.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15397000/24229575 [10:27<06:29, 22703.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15403000/24229575 [10:27<05:12, 28279.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15407000/24229575 [10:27<05:39, 26023.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15410000/24229575 [10:28<07:05, 20723.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15414000/24229575 [10:28<06:32, 22454.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15417000/24229575 [10:28<06:29, 22621.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15420000/24229575 [10:28<06:18, 23272.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15424000/24229575 [10:28<06:12, 23657.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15429000/24229575 [10:28<05:25, 27003.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15434000/24229575 [10:28<05:07, 28570.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15439000/24229575 [10:29<05:16, 27773.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▎   | 15442000/24229575 [10:29<05:17, 27690.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15447000/24229575 [10:29<04:47, 30593.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15451000/24229575 [10:29<07:24, 19730.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15456000/24229575 [10:30<08:03, 18157.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15462000/24229575 [10:30<06:17, 23214.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15468000/24229575 [10:30<05:28, 26673.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15472000/24229575 [10:30<05:11, 28155.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15478000/24229575 [10:30<04:20, 33625.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15483000/24229575 [10:30<04:35, 31747.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15487000/24229575 [10:30<05:10, 28154.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15492000/24229575 [10:31<04:42, 30926.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15496000/24229575 [10:31<04:50, 30110.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15500000/24229575 [10:31<04:43, 30790.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15504000/24229575 [10:31<06:18, 23075.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15507000/24229575 [10:31<06:59, 20777.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15511000/24229575 [10:32<07:09, 20303.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15514000/24229575 [10:32<07:00, 20738.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15517000/24229575 [10:32<07:47, 18618.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15520000/24229575 [10:32<07:21, 19733.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15526000/24229575 [10:32<05:14, 27666.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15530000/24229575 [10:32<05:14, 27621.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15537000/24229575 [10:32<04:24, 32901.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15541000/24229575 [10:33<04:31, 31959.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15545000/24229575 [10:33<06:21, 22779.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15548000/24229575 [10:33<06:11, 23366.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15555000/24229575 [10:33<04:35, 31456.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15559000/24229575 [10:33<05:41, 25366.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15563000/24229575 [10:33<05:14, 27586.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15568000/24229575 [10:34<04:53, 29526.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15572000/24229575 [10:34<04:33, 31691.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15576000/24229575 [10:34<04:45, 30320.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15583000/24229575 [10:34<03:52, 37197.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15587000/24229575 [10:34<04:30, 31995.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15591000/24229575 [10:34<05:38, 25541.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15594000/24229575 [10:35<06:38, 21691.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15597000/24229575 [10:35<07:06, 20250.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15602000/24229575 [10:35<06:25, 22404.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15605000/24229575 [10:35<07:29, 19183.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15608000/24229575 [10:35<07:09, 20075.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15611000/24229575 [10:36<07:13, 19881.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15614000/24229575 [10:36<07:20, 19567.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15618000/24229575 [10:36<06:17, 22838.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  64%|██████▍   | 15624000/24229575 [10:36<04:58, 28834.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15629000/24229575 [10:36<04:16, 33475.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15634000/24229575 [10:36<03:57, 36231.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15638000/24229575 [10:36<04:01, 35576.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15642000/24229575 [10:37<05:26, 26282.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15646000/24229575 [10:37<05:02, 28410.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15651000/24229575 [10:37<04:19, 33049.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15655000/24229575 [10:37<04:54, 29128.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15659000/24229575 [10:37<04:45, 30008.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15663000/24229575 [10:37<04:36, 30959.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15670000/24229575 [10:37<03:38, 39114.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15675000/24229575 [10:38<05:30, 25890.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15679000/24229575 [10:38<06:04, 23429.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15683000/24229575 [10:38<05:24, 26316.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15687000/24229575 [10:38<05:29, 25916.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15690000/24229575 [10:38<06:55, 20566.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15696000/24229575 [10:39<05:50, 24374.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15699000/24229575 [10:39<07:45, 18314.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15702000/24229575 [10:39<08:39, 16412.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15707000/24229575 [10:39<06:50, 20774.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15710000/24229575 [10:39<06:46, 20965.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15713000/24229575 [10:39<06:34, 21602.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15716000/24229575 [10:40<06:19, 22462.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15719000/24229575 [10:40<06:31, 21717.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15723000/24229575 [10:40<05:36, 25268.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15729000/24229575 [10:40<04:18, 32826.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15733000/24229575 [10:40<04:11, 33802.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15737000/24229575 [10:40<04:25, 31963.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▍   | 15745000/24229575 [10:40<03:23, 41765.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15750000/24229575 [10:40<03:35, 39403.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15755000/24229575 [10:41<04:42, 30012.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15759000/24229575 [10:41<04:26, 31741.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15765000/24229575 [10:41<04:14, 33260.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15769000/24229575 [10:41<05:00, 28189.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15773000/24229575 [10:41<06:20, 22239.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15776000/24229575 [10:42<07:00, 20107.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15782000/24229575 [10:42<05:55, 23795.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15785000/24229575 [10:42<06:56, 20288.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15788000/24229575 [10:42<06:38, 21180.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15791000/24229575 [10:42<06:33, 21454.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15794000/24229575 [10:43<06:52, 20453.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15797000/24229575 [10:43<07:29, 18741.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15803000/24229575 [10:43<05:41, 24654.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15806000/24229575 [10:43<05:43, 24524.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15809000/24229575 [10:43<07:10, 19575.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15812000/24229575 [10:43<06:41, 20948.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15817000/24229575 [10:43<05:17, 26506.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15822000/24229575 [10:44<04:30, 31079.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15827000/24229575 [10:44<04:06, 34103.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15832000/24229575 [10:44<03:45, 37244.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15838000/24229575 [10:44<03:23, 41214.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15843000/24229575 [10:44<04:36, 30354.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15847000/24229575 [10:44<05:23, 25915.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15852000/24229575 [10:45<04:46, 29281.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15859000/24229575 [10:45<04:01, 34675.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15863000/24229575 [10:45<04:30, 30955.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15867000/24229575 [10:45<05:59, 23267.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  65%|██████▌   | 15870000/24229575 [10:45<06:24, 21742.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15873000/24229575 [10:45<06:06, 22825.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15877000/24229575 [10:46<06:03, 22965.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15880000/24229575 [10:46<06:14, 22313.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15883000/24229575 [10:46<06:45, 20603.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15886000/24229575 [10:46<06:26, 21561.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15889000/24229575 [10:46<07:26, 18672.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15894000/24229575 [10:46<05:47, 23994.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15897000/24229575 [10:47<07:33, 18384.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15901000/24229575 [10:47<06:13, 22279.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15905000/24229575 [10:47<05:31, 25105.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15912000/24229575 [10:47<04:28, 30934.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15918000/24229575 [10:47<03:51, 35903.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15922000/24229575 [10:47<04:24, 31350.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15926000/24229575 [10:47<04:25, 31248.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15930000/24229575 [10:48<04:18, 32070.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15934000/24229575 [10:48<04:31, 30556.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15939000/24229575 [10:48<04:56, 27944.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15942000/24229575 [10:48<04:58, 27757.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15949000/24229575 [10:48<04:49, 28617.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15952000/24229575 [10:48<05:04, 27167.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15957000/24229575 [10:49<04:39, 29627.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15961000/24229575 [10:49<04:27, 30878.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15965000/24229575 [10:49<05:17, 26041.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15968000/24229575 [10:49<05:55, 23233.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15972000/24229575 [10:49<05:16, 26077.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15975000/24229575 [10:50<08:34, 16041.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15978000/24229575 [10:50<07:53, 17411.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15981000/24229575 [10:50<07:35, 18096.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15987000/24229575 [10:50<05:45, 23873.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15990000/24229575 [10:50<06:31, 21049.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 15997000/24229575 [10:50<04:47, 28640.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16001000/24229575 [10:50<04:26, 30907.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16005000/24229575 [10:51<04:14, 32290.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16012000/24229575 [10:51<03:51, 35443.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16016000/24229575 [10:51<04:31, 30256.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16020000/24229575 [10:51<05:14, 26144.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16023000/24229575 [10:51<05:18, 25787.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16026000/24229575 [10:51<05:25, 25233.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16029000/24229575 [10:51<05:17, 25859.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16032000/24229575 [10:52<06:11, 22073.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16038000/24229575 [10:52<04:30, 30307.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16042000/24229575 [10:52<05:58, 22850.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16046000/24229575 [10:52<05:14, 26012.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▌   | 16050000/24229575 [10:52<05:35, 24365.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16053000/24229575 [10:52<05:49, 23365.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16057000/24229575 [10:53<05:11, 26276.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16062000/24229575 [10:53<04:19, 31419.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16066000/24229575 [10:53<04:07, 32927.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16070000/24229575 [10:53<04:29, 30304.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16074000/24229575 [10:53<06:09, 22046.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16077000/24229575 [10:53<07:05, 19166.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16081000/24229575 [10:54<06:16, 21668.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16084000/24229575 [10:54<06:04, 22335.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16087000/24229575 [10:54<05:45, 23557.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16093000/24229575 [10:54<05:28, 24778.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16099000/24229575 [10:54<04:21, 31144.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16103000/24229575 [10:54<04:07, 32862.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16107000/24229575 [10:54<04:07, 32780.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  66%|██████▋   | 16111000/24229575 [10:55<05:07, 26402.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16114000/24229575 [10:55<05:38, 23990.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16117000/24229575 [10:55<05:23, 25064.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16121000/24229575 [10:55<04:59, 27061.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16124000/24229575 [10:55<06:02, 22372.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16127000/24229575 [10:55<06:18, 21411.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16130000/24229575 [10:56<07:23, 18257.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16134000/24229575 [10:56<06:39, 20250.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16138000/24229575 [10:56<05:40, 23762.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16144000/24229575 [10:56<04:16, 31501.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16149000/24229575 [10:56<04:00, 33643.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16153000/24229575 [10:56<04:31, 29747.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16157000/24229575 [10:57<05:37, 23948.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16161000/24229575 [10:57<05:05, 26392.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16164000/24229575 [10:57<05:21, 25111.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16168000/24229575 [10:57<05:03, 26536.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16173000/24229575 [10:57<04:19, 31100.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16177000/24229575 [10:57<05:24, 24781.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16182000/24229575 [10:57<04:35, 29212.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16186000/24229575 [10:57<04:37, 28994.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16190000/24229575 [10:58<04:55, 27243.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16194000/24229575 [10:58<04:36, 29012.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16198000/24229575 [10:58<05:51, 22846.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16202000/24229575 [10:58<05:37, 23785.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16205000/24229575 [10:58<05:31, 24211.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16208000/24229575 [10:58<05:29, 24316.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16212000/24229575 [10:59<05:19, 25122.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16216000/24229575 [10:59<04:48, 27813.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16219000/24229575 [10:59<05:11, 25677.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16222000/24229575 [10:59<05:54, 22560.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16225000/24229575 [10:59<06:14, 21388.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16228000/24229575 [10:59<06:31, 20414.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16232000/24229575 [10:59<05:40, 23492.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16235000/24229575 [11:00<06:01, 22138.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16242000/24229575 [11:00<04:33, 29189.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16245000/24229575 [11:00<05:22, 24731.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16249000/24229575 [11:00<04:49, 27594.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16252000/24229575 [11:00<05:50, 22791.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16256000/24229575 [11:00<06:07, 21698.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16259000/24229575 [11:01<06:17, 21096.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16264000/24229575 [11:01<04:59, 26638.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16267000/24229575 [11:01<04:59, 26559.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16271000/24229575 [11:01<04:48, 27585.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16276000/24229575 [11:01<04:12, 31524.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16280000/24229575 [11:01<04:17, 30917.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16284000/24229575 [11:01<04:00, 33090.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16288000/24229575 [11:01<04:06, 32174.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16294000/24229575 [11:02<04:00, 33012.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16298000/24229575 [11:02<05:01, 26295.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16301000/24229575 [11:02<05:37, 23461.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16304000/24229575 [11:02<06:13, 21197.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16307000/24229575 [11:02<05:55, 22290.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16311000/24229575 [11:02<05:23, 24470.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16314000/24229575 [11:03<05:27, 24165.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16319000/24229575 [11:03<04:40, 28216.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16322000/24229575 [11:03<04:39, 28248.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16325000/24229575 [11:03<06:10, 21336.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16329000/24229575 [11:03<05:18, 24828.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16332000/24229575 [11:03<05:17, 24860.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16335000/24229575 [11:03<05:34, 23568.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16338000/24229575 [11:04<06:26, 20420.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16341000/24229575 [11:04<06:26, 20406.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16346000/24229575 [11:04<05:18, 24714.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16350000/24229575 [11:04<04:47, 27412.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  67%|██████▋   | 16353000/24229575 [11:04<05:18, 24740.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16356000/24229575 [11:04<05:06, 25663.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16359000/24229575 [11:04<05:54, 22191.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16364000/24229575 [11:05<04:55, 26597.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16369000/24229575 [11:05<05:26, 24107.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16372000/24229575 [11:05<05:13, 25081.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16377000/24229575 [11:05<04:25, 29609.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16381000/24229575 [11:05<04:16, 30566.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16385000/24229575 [11:05<05:17, 24704.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16392000/24229575 [11:06<04:02, 32258.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16396000/24229575 [11:06<03:52, 33678.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16400000/24229575 [11:06<04:39, 28003.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16404000/24229575 [11:06<05:29, 23737.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16407000/24229575 [11:06<05:57, 21886.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16410000/24229575 [11:06<06:45, 19267.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16415000/24229575 [11:07<05:14, 24839.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16418000/24229575 [11:07<05:24, 24101.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16423000/24229575 [11:07<04:34, 28415.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16428000/24229575 [11:07<04:06, 31633.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16432000/24229575 [11:07<04:28, 29063.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16436000/24229575 [11:07<05:09, 25156.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16439000/24229575 [11:08<06:14, 20783.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16445000/24229575 [11:08<04:58, 26062.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16448000/24229575 [11:08<05:22, 24095.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16451000/24229575 [11:08<05:43, 22659.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16454000/24229575 [11:08<05:30, 23508.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16457000/24229575 [11:08<06:33, 19743.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16461000/24229575 [11:09<06:10, 20942.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16468000/24229575 [11:09<04:16, 30292.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16472000/24229575 [11:09<04:10, 30948.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16476000/24229575 [11:09<05:12, 24839.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16483000/24229575 [11:09<03:50, 33672.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16488000/24229575 [11:09<04:22, 29484.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16492000/24229575 [11:10<04:55, 26188.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16496000/24229575 [11:10<05:20, 24094.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16500000/24229575 [11:10<05:03, 25450.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16503000/24229575 [11:10<05:38, 22797.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16506000/24229575 [11:10<06:20, 20301.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16509000/24229575 [11:10<06:04, 21161.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16515000/24229575 [11:10<04:25, 29093.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16519000/24229575 [11:11<04:26, 28925.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16524000/24229575 [11:11<03:55, 32728.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16529000/24229575 [11:11<04:13, 30372.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16533000/24229575 [11:11<04:19, 29607.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16537000/24229575 [11:11<06:04, 21116.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16543000/24229575 [11:12<05:14, 24445.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16546000/24229575 [11:12<05:58, 21456.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16549000/24229575 [11:12<06:23, 20041.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16552000/24229575 [11:12<06:15, 20437.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16555000/24229575 [11:12<05:46, 22133.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16560000/24229575 [11:12<04:38, 27528.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16567000/24229575 [11:12<03:36, 35366.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16571000/24229575 [11:13<04:37, 27595.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16575000/24229575 [11:13<04:43, 27006.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16581000/24229575 [11:13<04:49, 26418.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16584000/24229575 [11:13<05:00, 25422.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16587000/24229575 [11:13<05:39, 22509.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16591000/24229575 [11:14<05:06, 24886.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  68%|██████▊   | 16594000/24229575 [11:14<05:27, 23302.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16598000/24229575 [11:14<05:46, 22001.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16604000/24229575 [11:14<04:21, 29176.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16608000/24229575 [11:14<04:40, 27123.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16614000/24229575 [11:14<04:05, 31068.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16618000/24229575 [11:14<04:33, 27847.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16621000/24229575 [11:15<04:37, 27459.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16625000/24229575 [11:15<04:19, 29299.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16629000/24229575 [11:15<04:07, 30736.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16633000/24229575 [11:15<04:37, 27400.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16636000/24229575 [11:15<05:42, 22153.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16639000/24229575 [11:15<05:53, 21476.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16643000/24229575 [11:16<05:25, 23335.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16649000/24229575 [11:16<04:24, 28629.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16652000/24229575 [11:16<05:39, 22318.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▊   | 16655000/24229575 [11:16<05:57, 21200.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16658000/24229575 [11:16<05:34, 22608.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16661000/24229575 [11:16<05:57, 21175.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16666000/24229575 [11:16<04:43, 26639.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16675000/24229575 [11:17<03:22, 37330.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16680000/24229575 [11:17<03:20, 37710.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16684000/24229575 [11:17<05:05, 24716.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16688000/24229575 [11:17<05:38, 22280.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16691000/24229575 [11:17<05:27, 22990.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16694000/24229575 [11:18<05:36, 22404.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16697000/24229575 [11:18<05:48, 21588.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16701000/24229575 [11:18<05:00, 25043.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16704000/24229575 [11:18<05:57, 21073.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16712000/24229575 [11:18<03:51, 32410.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16716000/24229575 [11:18<03:59, 31379.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16720000/24229575 [11:18<04:14, 29460.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16724000/24229575 [11:19<04:13, 29558.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16728000/24229575 [11:19<04:17, 29094.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16732000/24229575 [11:19<04:28, 27972.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16735000/24229575 [11:19<05:07, 24409.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16738000/24229575 [11:19<05:17, 23584.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16742000/24229575 [11:19<05:40, 22016.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16746000/24229575 [11:20<05:08, 24260.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16749000/24229575 [11:20<05:03, 24619.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16752000/24229575 [11:20<05:50, 21306.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16759000/24229575 [11:20<04:44, 26238.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16762000/24229575 [11:20<05:00, 24816.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16765000/24229575 [11:20<05:13, 23789.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16772000/24229575 [11:20<03:55, 31621.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16776000/24229575 [11:21<04:27, 27861.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16780000/24229575 [11:21<04:17, 28952.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16784000/24229575 [11:21<05:41, 21833.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16787000/24229575 [11:21<05:43, 21662.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16792000/24229575 [11:21<04:41, 26410.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16797000/24229575 [11:21<04:29, 27561.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16800000/24229575 [11:22<04:49, 25630.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16803000/24229575 [11:22<05:02, 24525.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16808000/24229575 [11:22<04:34, 27002.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16813000/24229575 [11:22<04:06, 30098.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16817000/24229575 [11:22<04:40, 26449.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16820000/24229575 [11:22<04:33, 27047.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16826000/24229575 [11:22<03:35, 34277.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16830000/24229575 [11:23<05:42, 21615.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16833000/24229575 [11:23<06:16, 19650.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  69%|██████▉   | 16838000/24229575 [11:23<05:00, 24598.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16842000/24229575 [11:23<04:39, 26415.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16846000/24229575 [11:23<05:32, 22205.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16849000/24229575 [11:24<05:23, 22783.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16854000/24229575 [11:24<04:34, 26829.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16858000/24229575 [11:24<05:05, 24121.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16862000/24229575 [11:24<05:07, 23921.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16867000/24229575 [11:24<04:39, 26372.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16873000/24229575 [11:24<03:56, 31042.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16877000/24229575 [11:25<04:36, 26613.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16880000/24229575 [11:25<04:38, 26436.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16883000/24229575 [11:25<04:59, 24543.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16886000/24229575 [11:25<05:08, 23768.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16889000/24229575 [11:25<06:01, 20324.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16893000/24229575 [11:25<05:14, 23298.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16900000/24229575 [11:25<04:00, 30414.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16904000/24229575 [11:26<04:30, 27041.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16908000/24229575 [11:26<04:23, 27789.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16912000/24229575 [11:26<04:24, 27636.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16915000/24229575 [11:26<04:24, 27634.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16918000/24229575 [11:26<04:30, 27048.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16922000/24229575 [11:26<04:29, 27144.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16925000/24229575 [11:26<05:07, 23778.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16928000/24229575 [11:27<05:14, 23250.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16931000/24229575 [11:27<05:17, 22952.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16934000/24229575 [11:27<05:56, 20464.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16937000/24229575 [11:27<05:43, 21241.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16943000/24229575 [11:27<04:10, 29125.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16947000/24229575 [11:27<04:58, 24428.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16950000/24229575 [11:28<05:36, 21657.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16955000/24229575 [11:28<04:29, 27009.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|██████▉   | 16959000/24229575 [11:28<04:19, 28043.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16963000/24229575 [11:28<04:48, 25157.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16966000/24229575 [11:28<05:14, 23065.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16972000/24229575 [11:28<04:39, 26004.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16975000/24229575 [11:29<04:35, 26355.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16978000/24229575 [11:29<05:29, 22021.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16983000/24229575 [11:29<04:27, 27114.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16987000/24229575 [11:29<04:40, 25806.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16991000/24229575 [11:29<04:50, 24927.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 16994000/24229575 [11:29<05:21, 22503.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17000000/24229575 [11:29<04:02, 29848.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17004000/24229575 [11:30<04:12, 28597.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17008000/24229575 [11:30<03:57, 30441.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17013000/24229575 [11:30<04:18, 27909.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17017000/24229575 [11:30<04:13, 28481.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17021000/24229575 [11:30<03:59, 30059.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17025000/24229575 [11:30<04:32, 26455.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17029000/24229575 [11:31<04:44, 25310.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17033000/24229575 [11:31<04:34, 26242.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17036000/24229575 [11:31<05:05, 23528.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17039000/24229575 [11:31<05:25, 22085.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17042000/24229575 [11:31<05:19, 22501.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17045000/24229575 [11:31<06:32, 18315.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17049000/24229575 [11:32<05:40, 21116.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17052000/24229575 [11:32<05:30, 21693.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17056000/24229575 [11:32<05:05, 23448.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17061000/24229575 [11:32<04:18, 27684.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17065000/24229575 [11:32<04:18, 27694.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17069000/24229575 [11:32<03:55, 30377.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17073000/24229575 [11:32<04:55, 24223.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17076000/24229575 [11:33<05:50, 20421.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  70%|███████   | 17080000/24229575 [11:33<05:14, 22722.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17085000/24229575 [11:33<04:49, 24697.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17090000/24229575 [11:33<04:08, 28725.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17094000/24229575 [11:33<03:54, 30410.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17098000/24229575 [11:33<04:14, 28063.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17104000/24229575 [11:33<03:22, 35115.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17108000/24229575 [11:34<04:13, 28089.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17112000/24229575 [11:34<04:25, 26849.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17116000/24229575 [11:34<04:10, 28346.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17120000/24229575 [11:34<04:05, 28948.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17125000/24229575 [11:34<03:45, 31530.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17129000/24229575 [11:34<03:34, 33108.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17133000/24229575 [11:35<05:58, 19777.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17137000/24229575 [11:35<05:14, 22559.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17142000/24229575 [11:35<05:12, 22674.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17146000/24229575 [11:35<04:38, 25394.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17150000/24229575 [11:35<05:04, 23230.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17153000/24229575 [11:36<05:34, 21163.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17156000/24229575 [11:36<05:10, 22817.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17161000/24229575 [11:36<05:43, 20569.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17164000/24229575 [11:36<05:37, 20946.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17167000/24229575 [11:36<06:08, 19154.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17170000/24229575 [11:36<05:46, 20402.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17174000/24229575 [11:36<04:57, 23680.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17180000/24229575 [11:37<03:52, 30342.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17185000/24229575 [11:37<03:34, 32860.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17190000/24229575 [11:37<03:14, 36205.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17194000/24229575 [11:37<03:33, 32892.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17198000/24229575 [11:37<04:03, 28846.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17202000/24229575 [11:37<04:15, 27461.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17208000/24229575 [11:38<04:09, 28149.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17211000/24229575 [11:38<04:10, 27964.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17216000/24229575 [11:38<03:50, 30491.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17220000/24229575 [11:38<04:33, 25606.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17225000/24229575 [11:38<04:09, 28119.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17231000/24229575 [11:38<03:50, 30359.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17235000/24229575 [11:39<04:27, 26156.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17238000/24229575 [11:39<06:15, 18612.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17242000/24229575 [11:39<05:26, 21390.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17247000/24229575 [11:39<05:33, 20950.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17252000/24229575 [11:39<04:33, 25549.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17256000/24229575 [11:40<06:06, 19006.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████   | 17262000/24229575 [11:40<04:56, 23520.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17267000/24229575 [11:40<04:14, 27410.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17272000/24229575 [11:40<04:09, 27903.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17276000/24229575 [11:40<04:03, 28597.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17280000/24229575 [11:40<04:38, 24935.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17283000/24229575 [11:41<04:39, 24875.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17287000/24229575 [11:41<04:28, 25820.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17290000/24229575 [11:41<05:45, 20058.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17296000/24229575 [11:41<04:38, 24852.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17304000/24229575 [11:41<03:24, 33843.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17309000/24229575 [11:41<03:08, 36655.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17315000/24229575 [11:41<02:46, 41610.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  71%|███████▏  | 17320000/24229575 [11:42<04:04, 28222.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17325000/24229575 [11:42<03:34, 32167.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17330000/24229575 [11:42<06:03, 18972.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17334000/24229575 [11:43<06:03, 18962.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17337000/24229575 [11:43<05:56, 19332.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17340000/24229575 [11:43<06:03, 18942.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17345000/24229575 [11:43<04:45, 24109.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17349000/24229575 [11:43<04:18, 26607.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17353000/24229575 [11:43<03:57, 28941.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17357000/24229575 [11:43<03:58, 28780.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17361000/24229575 [11:44<03:52, 29604.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17365000/24229575 [11:44<04:28, 25555.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17368000/24229575 [11:44<04:46, 23969.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17371000/24229575 [11:44<04:46, 23910.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17374000/24229575 [11:44<05:16, 21638.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17377000/24229575 [11:44<05:19, 21444.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17380000/24229575 [11:44<05:07, 22250.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17384000/24229575 [11:45<04:35, 24840.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17391000/24229575 [11:45<03:44, 30465.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17395000/24229575 [11:45<03:55, 29055.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17400000/24229575 [11:45<03:24, 33367.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17404000/24229575 [11:45<03:18, 34357.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17409000/24229575 [11:45<02:59, 37937.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17415000/24229575 [11:45<03:09, 36050.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17419000/24229575 [11:46<04:07, 27555.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17423000/24229575 [11:46<03:54, 29068.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17427000/24229575 [11:46<05:18, 21343.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17430000/24229575 [11:46<06:30, 17392.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17433000/24229575 [11:47<05:59, 18920.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17436000/24229575 [11:47<05:43, 19804.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17440000/24229575 [11:47<04:51, 23293.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17444000/24229575 [11:47<04:25, 25581.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17449000/24229575 [11:47<04:27, 25366.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17453000/24229575 [11:47<04:31, 24942.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17456000/24229575 [11:47<04:38, 24334.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17461000/24229575 [11:47<03:52, 29100.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17465000/24229575 [11:48<04:32, 24784.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17469000/24229575 [11:48<04:23, 25700.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17473000/24229575 [11:48<04:24, 25584.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17476000/24229575 [11:48<04:35, 24479.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17479000/24229575 [11:48<04:46, 23525.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17486000/24229575 [11:48<03:34, 31446.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17490000/24229575 [11:49<03:30, 32059.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17494000/24229575 [11:49<04:13, 26545.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17498000/24229575 [11:49<03:56, 28519.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17505000/24229575 [11:49<03:02, 36941.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17510000/24229575 [11:49<03:56, 28396.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17514000/24229575 [11:49<04:25, 25263.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17517000/24229575 [11:50<04:53, 22863.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17520000/24229575 [11:50<04:52, 22911.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17523000/24229575 [11:50<05:09, 21687.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17527000/24229575 [11:50<04:28, 24923.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17530000/24229575 [11:50<05:06, 21836.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17533000/24229575 [11:50<05:24, 20666.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17536000/24229575 [11:51<05:02, 22154.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17541000/24229575 [11:51<03:59, 27899.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17545000/24229575 [11:51<04:14, 26228.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17548000/24229575 [11:51<04:37, 24048.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17551000/24229575 [11:51<05:54, 18818.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17560000/24229575 [11:51<03:50, 28917.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  72%|███████▏  | 17564000/24229575 [11:52<04:00, 27694.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17569000/24229575 [11:52<03:30, 31698.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17573000/24229575 [11:52<04:09, 26670.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17576000/24229575 [11:52<04:17, 25860.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17582000/24229575 [11:52<03:38, 30451.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17587000/24229575 [11:52<03:14, 34179.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17591000/24229575 [11:53<04:23, 25195.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17594000/24229575 [11:53<04:40, 23658.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17597000/24229575 [11:53<04:28, 24694.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17600000/24229575 [11:53<05:27, 20217.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17603000/24229575 [11:53<05:29, 20106.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17608000/24229575 [11:53<04:29, 24533.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17612000/24229575 [11:53<04:09, 26497.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17615000/24229575 [11:54<04:35, 23969.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17620000/24229575 [11:54<03:45, 29312.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17627000/24229575 [11:54<03:39, 30015.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17631000/24229575 [11:54<04:00, 27415.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17635000/24229575 [11:54<03:43, 29502.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17639000/24229575 [11:54<04:17, 25627.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17642000/24229575 [11:55<05:53, 18633.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17645000/24229575 [11:55<05:26, 20140.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17654000/24229575 [11:55<03:45, 29184.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17658000/24229575 [11:55<03:39, 29915.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17662000/24229575 [11:55<03:59, 27444.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17667000/24229575 [11:55<03:54, 28005.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17673000/24229575 [11:56<03:14, 33729.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17677000/24229575 [11:56<03:54, 27951.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17681000/24229575 [11:56<04:10, 26178.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17684000/24229575 [11:56<05:00, 21770.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17687000/24229575 [11:56<05:18, 20522.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17690000/24229575 [11:57<05:28, 19906.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17693000/24229575 [11:57<05:21, 20339.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17699000/24229575 [11:57<03:49, 28479.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17703000/24229575 [11:57<04:37, 23520.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17706000/24229575 [11:57<04:40, 23233.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17712000/24229575 [11:57<03:38, 29879.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17716000/24229575 [11:57<04:20, 24962.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17719000/24229575 [11:58<04:28, 24250.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17723000/24229575 [11:58<04:10, 26002.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17726000/24229575 [11:58<04:06, 26372.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17730000/24229575 [11:58<04:09, 26089.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17734000/24229575 [11:58<03:42, 29139.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17739000/24229575 [11:58<03:35, 30108.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17743000/24229575 [11:58<04:02, 26699.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17748000/24229575 [11:59<03:31, 30703.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17752000/24229575 [11:59<04:00, 26921.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17756000/24229575 [11:59<03:41, 29185.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17760000/24229575 [11:59<04:22, 24690.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17765000/24229575 [11:59<03:46, 28555.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17769000/24229575 [11:59<03:44, 28753.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17773000/24229575 [12:00<04:17, 25042.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17776000/24229575 [12:00<04:36, 23356.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17779000/24229575 [12:00<04:46, 22525.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17782000/24229575 [12:00<05:05, 21110.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17785000/24229575 [12:00<05:37, 19111.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17789000/24229575 [12:00<05:03, 21208.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17792000/24229575 [12:01<05:17, 20244.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17797000/24229575 [12:01<04:04, 26256.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17800000/24229575 [12:01<04:07, 26007.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  73%|███████▎  | 17806000/24229575 [12:01<03:31, 30327.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17810000/24229575 [12:01<03:50, 27867.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17813000/24229575 [12:01<04:01, 26565.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17817000/24229575 [12:01<03:50, 27772.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17820000/24229575 [12:02<04:08, 25844.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17824000/24229575 [12:02<04:06, 25972.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17827000/24229575 [12:02<04:03, 26345.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17830000/24229575 [12:02<04:19, 24626.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17834000/24229575 [12:02<04:01, 26451.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17839000/24229575 [12:02<03:19, 31999.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17843000/24229575 [12:02<03:10, 33580.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17847000/24229575 [12:02<03:17, 32339.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17853000/24229575 [12:03<02:51, 37236.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17857000/24229575 [12:03<03:39, 29032.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17861000/24229575 [12:03<03:58, 26684.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17865000/24229575 [12:03<03:57, 26813.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▎  | 17868000/24229575 [12:03<05:09, 20559.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17871000/24229575 [12:03<05:03, 20976.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17874000/24229575 [12:04<06:02, 17532.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17876000/24229575 [12:04<05:57, 17794.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17880000/24229575 [12:04<05:25, 19522.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17886000/24229575 [12:04<03:55, 26905.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17890000/24229575 [12:04<04:01, 26234.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17895000/24229575 [12:04<03:34, 29504.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17899000/24229575 [12:05<03:40, 28740.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17903000/24229575 [12:05<04:07, 25556.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17906000/24229575 [12:05<05:19, 19798.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17909000/24229575 [12:05<05:17, 19894.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17912000/24229575 [12:05<05:35, 18829.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17920000/24229575 [12:05<03:43, 28274.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17924000/24229575 [12:06<03:40, 28617.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17931000/24229575 [12:06<02:49, 37239.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17936000/24229575 [12:06<03:12, 32735.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17941000/24229575 [12:06<02:58, 35160.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17945000/24229575 [12:06<03:11, 32862.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17951000/24229575 [12:06<02:42, 38682.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17956000/24229575 [12:06<02:45, 38001.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17961000/24229575 [12:07<04:00, 26023.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17965000/24229575 [12:07<05:15, 19853.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17968000/24229575 [12:07<06:33, 15911.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17972000/24229575 [12:08<05:33, 18764.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17977000/24229575 [12:08<04:45, 21916.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17980000/24229575 [12:08<04:37, 22491.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17986000/24229575 [12:08<03:37, 28718.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17990000/24229575 [12:08<04:16, 24291.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17994000/24229575 [12:08<03:58, 26101.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 17997000/24229575 [12:08<04:08, 25053.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18002000/24229575 [12:09<03:45, 27669.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18005000/24229575 [12:09<04:38, 22361.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18008000/24229575 [12:09<04:23, 23600.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18011000/24229575 [12:09<04:15, 24354.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18017000/24229575 [12:09<03:14, 32020.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18021000/24229575 [12:09<03:23, 30453.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18025000/24229575 [12:09<03:22, 30701.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18029000/24229575 [12:09<03:09, 32710.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18033000/24229575 [12:10<03:19, 31097.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18037000/24229575 [12:10<03:32, 29097.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18042000/24229575 [12:10<03:26, 29911.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18046000/24229575 [12:10<03:49, 26932.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  74%|███████▍  | 18049000/24229575 [12:10<03:53, 26420.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18054000/24229575 [12:10<03:29, 29528.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18058000/24229575 [12:10<03:17, 31187.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18062000/24229575 [12:11<05:47, 17730.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18065000/24229575 [12:11<05:34, 18406.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18068000/24229575 [12:11<05:10, 19841.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18071000/24229575 [12:11<05:51, 17535.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18077000/24229575 [12:12<04:19, 23739.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18080000/24229575 [12:12<04:07, 24843.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18083000/24229575 [12:12<04:10, 24564.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18087000/24229575 [12:12<04:00, 25564.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18090000/24229575 [12:12<03:52, 26374.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18093000/24229575 [12:12<04:54, 20824.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18096000/24229575 [12:12<04:33, 22417.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18102000/24229575 [12:13<03:28, 29388.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18107000/24229575 [12:13<02:59, 34068.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18111000/24229575 [12:13<03:13, 31553.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18118000/24229575 [12:13<02:33, 39898.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18123000/24229575 [12:13<03:15, 31286.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18128000/24229575 [12:13<03:13, 31591.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18132000/24229575 [12:13<03:51, 26362.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18135000/24229575 [12:14<03:51, 26351.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18138000/24229575 [12:14<04:08, 24488.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18141000/24229575 [12:14<04:09, 24354.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18145000/24229575 [12:14<04:33, 22259.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18148000/24229575 [12:14<04:21, 23281.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18151000/24229575 [12:14<05:25, 18665.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18155000/24229575 [12:15<04:59, 20295.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18160000/24229575 [12:15<03:55, 25734.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18165000/24229575 [12:15<03:22, 30010.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▍  | 18169000/24229575 [12:15<04:52, 20705.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18174000/24229575 [12:15<04:00, 25181.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18178000/24229575 [12:15<04:04, 24726.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18181000/24229575 [12:16<04:40, 21533.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18184000/24229575 [12:16<04:26, 22687.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18187000/24229575 [12:16<05:13, 19304.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18195000/24229575 [12:16<03:26, 29266.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18202000/24229575 [12:16<02:48, 35846.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18208000/24229575 [12:16<02:42, 37107.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18213000/24229575 [12:17<03:01, 33144.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18218000/24229575 [12:17<02:48, 35678.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18222000/24229575 [12:17<03:20, 29950.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18226000/24229575 [12:17<03:26, 29091.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18230000/24229575 [12:17<03:46, 26448.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18233000/24229575 [12:17<03:44, 26746.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18236000/24229575 [12:17<03:48, 26274.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18239000/24229575 [12:18<04:10, 23912.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18242000/24229575 [12:18<04:32, 21933.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18245000/24229575 [12:18<05:45, 17339.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18251000/24229575 [12:18<04:57, 20125.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18255000/24229575 [12:18<04:29, 22132.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18258000/24229575 [12:19<05:28, 18170.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18263000/24229575 [12:19<04:13, 23528.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18266000/24229575 [12:19<04:36, 21542.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18270000/24229575 [12:19<03:57, 25081.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18273000/24229575 [12:19<03:48, 26019.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18276000/24229575 [12:19<04:02, 24584.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18280000/24229575 [12:19<03:33, 27829.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18284000/24229575 [12:20<03:20, 29651.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18288000/24229575 [12:20<04:50, 20482.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  75%|███████▌  | 18293000/24229575 [12:20<04:18, 22968.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18299000/24229575 [12:20<03:29, 28311.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18304000/24229575 [12:20<03:24, 28933.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18309000/24229575 [12:20<02:58, 33182.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18313000/24229575 [12:21<02:54, 33834.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18317000/24229575 [12:21<02:59, 32881.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18321000/24229575 [12:21<03:12, 30688.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18325000/24229575 [12:21<03:14, 30300.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18329000/24229575 [12:21<04:53, 20123.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18334000/24229575 [12:21<03:53, 25265.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18338000/24229575 [12:22<04:01, 24446.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18341000/24229575 [12:22<05:03, 19404.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18344000/24229575 [12:22<04:55, 19929.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18347000/24229575 [12:22<05:31, 17735.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18350000/24229575 [12:22<05:07, 19143.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18355000/24229575 [12:23<04:25, 22095.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18362000/24229575 [12:23<03:08, 31204.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18366000/24229575 [12:23<03:15, 30052.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18373000/24229575 [12:23<02:54, 33592.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18377000/24229575 [12:23<03:51, 25285.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18382000/24229575 [12:23<03:28, 28032.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18388000/24229575 [12:23<02:57, 32946.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18393000/24229575 [12:24<03:22, 28823.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18397000/24229575 [12:24<03:22, 28826.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18401000/24229575 [12:24<03:11, 30428.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18405000/24229575 [12:24<03:48, 25515.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18409000/24229575 [12:24<03:43, 25993.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18413000/24229575 [12:24<03:33, 27220.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18416000/24229575 [12:25<03:31, 27424.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18419000/24229575 [12:25<03:39, 26520.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18422000/24229575 [12:25<03:56, 24584.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18425000/24229575 [12:25<05:03, 19128.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18428000/24229575 [12:25<05:16, 18311.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18430000/24229575 [12:26<06:38, 14550.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18433000/24229575 [12:26<05:52, 16451.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18436000/24229575 [12:26<05:47, 16671.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18442000/24229575 [12:26<04:06, 23520.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18447000/24229575 [12:26<03:19, 28984.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18452000/24229575 [12:26<03:01, 31790.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18456000/24229575 [12:26<03:19, 28942.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18460000/24229575 [12:26<03:04, 31280.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18464000/24229575 [12:27<03:49, 25103.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18469000/24229575 [12:27<03:27, 27729.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▌  | 18474000/24229575 [12:27<03:00, 31805.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18480000/24229575 [12:27<02:57, 32470.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18484000/24229575 [12:27<02:50, 33703.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18490000/24229575 [12:27<02:33, 37455.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18494000/24229575 [12:28<03:04, 31165.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18498000/24229575 [12:28<04:01, 23775.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18501000/24229575 [12:28<03:52, 24667.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18504000/24229575 [12:28<03:54, 24389.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18507000/24229575 [12:28<03:58, 24017.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18510000/24229575 [12:28<04:22, 21758.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18514000/24229575 [12:29<03:56, 24139.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18517000/24229575 [12:29<04:22, 21737.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18520000/24229575 [12:29<04:31, 21038.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18523000/24229575 [12:29<04:28, 21274.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18526000/24229575 [12:29<05:03, 18795.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18528000/24229575 [12:29<06:41, 14201.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  76%|███████▋  | 18534000/24229575 [12:30<04:15, 22324.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18537000/24229575 [12:30<04:10, 22685.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18543000/24229575 [12:30<03:09, 29945.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18549000/24229575 [12:30<02:36, 36210.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18554000/24229575 [12:30<03:20, 28314.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18558000/24229575 [12:30<03:21, 28130.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18562000/24229575 [12:30<03:11, 29589.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18567000/24229575 [12:31<02:49, 33418.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18571000/24229575 [12:31<02:45, 34239.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18575000/24229575 [12:31<02:55, 32196.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18579000/24229575 [12:31<02:56, 32033.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18583000/24229575 [12:31<04:25, 21264.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18588000/24229575 [12:31<03:51, 24339.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18593000/24229575 [12:32<03:31, 26640.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18597000/24229575 [12:32<03:34, 26257.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18601000/24229575 [12:32<03:14, 28951.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18605000/24229575 [12:32<03:10, 29552.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18609000/24229575 [12:32<03:59, 23424.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18612000/24229575 [12:32<04:43, 19823.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18615000/24229575 [12:33<04:57, 18894.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18618000/24229575 [12:33<04:49, 19350.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18623000/24229575 [12:33<03:58, 23522.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18626000/24229575 [12:33<04:20, 21496.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18629000/24229575 [12:33<04:57, 18832.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18634000/24229575 [12:33<03:46, 24678.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18640000/24229575 [12:33<03:01, 30740.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18644000/24229575 [12:34<02:59, 31088.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18648000/24229575 [12:34<03:29, 26640.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18651000/24229575 [12:34<03:25, 27107.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18655000/24229575 [12:34<03:12, 28895.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18660000/24229575 [12:34<02:50, 32686.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18665000/24229575 [12:34<03:04, 30119.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18669000/24229575 [12:35<03:09, 29293.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18673000/24229575 [12:35<03:25, 27006.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18676000/24229575 [12:35<03:38, 25463.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18680000/24229575 [12:35<03:19, 27879.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18685000/24229575 [12:35<03:07, 29509.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18689000/24229575 [12:35<03:14, 28432.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18692000/24229575 [12:36<04:40, 19707.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18695000/24229575 [12:36<04:16, 21557.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18698000/24229575 [12:36<04:21, 21164.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18701000/24229575 [12:36<05:04, 18182.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18704000/24229575 [12:36<04:59, 18476.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18707000/24229575 [12:36<05:57, 15463.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18714000/24229575 [12:37<03:51, 23866.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18718000/24229575 [12:37<03:30, 26182.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18723000/24229575 [12:37<03:02, 30217.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18729000/24229575 [12:37<02:48, 32607.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18733000/24229575 [12:37<03:17, 27855.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18737000/24229575 [12:37<03:14, 28280.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18741000/24229575 [12:37<03:02, 30107.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18748000/24229575 [12:38<02:21, 38724.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18753000/24229575 [12:38<02:39, 34281.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18757000/24229575 [12:38<03:33, 25677.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18761000/24229575 [12:38<03:47, 24077.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18764000/24229575 [12:38<04:03, 22402.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18767000/24229575 [12:38<04:03, 22412.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18770000/24229575 [12:39<04:28, 20342.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  77%|███████▋  | 18774000/24229575 [12:39<03:53, 23387.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18778000/24229575 [12:39<03:24, 26613.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18782000/24229575 [12:39<03:06, 29145.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18786000/24229575 [12:39<03:35, 25308.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18789000/24229575 [12:39<04:23, 20616.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18792000/24229575 [12:40<04:06, 22043.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18796000/24229575 [12:40<03:30, 25861.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18800000/24229575 [12:40<04:05, 22114.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18803000/24229575 [12:40<04:52, 18531.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18809000/24229575 [12:40<03:31, 25607.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18817000/24229575 [12:40<02:43, 33156.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18821000/24229575 [12:41<03:14, 27836.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18825000/24229575 [12:41<03:01, 29853.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18832000/24229575 [12:41<02:24, 37455.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18838000/24229575 [12:41<02:20, 38506.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18843000/24229575 [12:41<02:32, 35349.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18847000/24229575 [12:41<03:24, 26295.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18851000/24229575 [12:42<03:58, 22552.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18854000/24229575 [12:42<04:13, 21181.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18857000/24229575 [12:42<04:29, 19917.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18860000/24229575 [12:42<04:16, 20937.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18863000/24229575 [12:42<05:10, 17298.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18865000/24229575 [12:42<05:09, 17360.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18867000/24229575 [12:43<05:01, 17763.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18873000/24229575 [12:43<03:28, 25668.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18879000/24229575 [12:43<03:28, 25680.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18884000/24229575 [12:43<02:56, 30304.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18888000/24229575 [12:43<03:09, 28134.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18893000/24229575 [12:43<02:50, 31315.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18897000/24229575 [12:44<03:10, 27920.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18901000/24229575 [12:44<02:56, 30227.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18905000/24229575 [12:44<02:53, 30699.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18910000/24229575 [12:44<02:40, 33049.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18914000/24229575 [12:44<02:34, 34450.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18918000/24229575 [12:44<02:35, 34254.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18922000/24229575 [12:44<03:25, 25774.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18926000/24229575 [12:45<03:18, 26772.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18929000/24229575 [12:45<03:27, 25518.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18932000/24229575 [12:45<04:12, 21000.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18935000/24229575 [12:45<04:06, 21465.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18941000/24229575 [12:45<03:06, 28367.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18945000/24229575 [12:45<03:10, 27672.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18948000/24229575 [12:45<04:02, 21805.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18951000/24229575 [12:46<05:34, 15760.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18953000/24229575 [12:46<05:22, 16356.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18956000/24229575 [12:46<04:50, 18175.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18961000/24229575 [12:46<04:21, 20147.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18964000/24229575 [12:46<04:51, 18063.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18970000/24229575 [12:47<03:26, 25445.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18976000/24229575 [12:47<03:13, 27170.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18981000/24229575 [12:47<03:10, 27619.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18986000/24229575 [12:47<02:53, 30163.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18991000/24229575 [12:47<02:48, 31057.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 18997000/24229575 [12:47<02:51, 30519.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 19005000/24229575 [12:48<02:18, 37852.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 19010000/24229575 [12:48<02:18, 37595.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 19014000/24229575 [12:48<03:04, 28273.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  78%|███████▊  | 19018000/24229575 [12:48<03:14, 26774.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19023000/24229575 [12:48<02:51, 30323.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19027000/24229575 [12:48<02:58, 29088.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19031000/24229575 [12:49<02:46, 31132.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19035000/24229575 [12:49<03:55, 22060.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19038000/24229575 [12:49<04:34, 18919.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19041000/24229575 [12:49<04:55, 17537.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19044000/24229575 [12:49<04:40, 18477.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19048000/24229575 [12:50<04:25, 19525.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19051000/24229575 [12:50<05:11, 16638.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19053000/24229575 [12:50<05:21, 16089.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19058000/24229575 [12:50<03:56, 21853.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19061000/24229575 [12:50<03:56, 21836.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19065000/24229575 [12:50<03:26, 25008.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19070000/24229575 [12:50<02:51, 30067.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19074000/24229575 [12:51<03:23, 25396.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▊  | 19077000/24229575 [12:51<03:34, 23992.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19081000/24229575 [12:51<03:10, 27049.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19087000/24229575 [12:51<02:30, 34274.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19093000/24229575 [12:51<02:29, 34274.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19099000/24229575 [12:51<02:11, 39026.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19104000/24229575 [12:51<02:13, 38410.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19109000/24229575 [12:52<02:31, 33809.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19114000/24229575 [12:52<02:19, 36627.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19118000/24229575 [12:52<02:33, 33379.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19122000/24229575 [12:52<03:00, 28333.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19126000/24229575 [12:52<03:04, 27687.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19129000/24229575 [12:52<03:41, 23025.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19132000/24229575 [12:53<04:52, 17426.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19135000/24229575 [12:53<04:27, 19064.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19138000/24229575 [12:53<05:20, 15871.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19140000/24229575 [12:53<05:19, 15936.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19142000/24229575 [12:53<05:21, 15828.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19145000/24229575 [12:54<04:39, 18189.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19149000/24229575 [12:54<03:44, 22630.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19152000/24229575 [12:54<03:39, 23141.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19155000/24229575 [12:54<04:16, 19819.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19159000/24229575 [12:54<03:32, 23826.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19166000/24229575 [12:54<02:32, 33142.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19170000/24229575 [12:54<03:10, 26558.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19174000/24229575 [12:55<03:14, 26016.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19179000/24229575 [12:55<02:58, 28240.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19184000/24229575 [12:55<02:49, 29827.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19188000/24229575 [12:55<02:41, 31202.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19193000/24229575 [12:55<02:30, 33409.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19197000/24229575 [12:55<02:23, 34948.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19201000/24229575 [12:55<02:29, 33740.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19205000/24229575 [12:56<02:38, 31761.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19212000/24229575 [12:56<02:07, 39487.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19217000/24229575 [12:56<02:37, 31743.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19221000/24229575 [12:56<02:40, 31269.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19225000/24229575 [12:56<03:56, 21138.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19228000/24229575 [12:56<03:55, 21236.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19231000/24229575 [12:57<05:17, 15719.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19235000/24229575 [12:57<04:20, 19143.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19238000/24229575 [12:57<04:42, 17698.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19241000/24229575 [12:57<04:25, 18805.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19244000/24229575 [12:57<04:02, 20536.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19247000/24229575 [12:58<03:55, 21158.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19250000/24229575 [12:58<03:39, 22651.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19255000/24229575 [12:58<02:57, 28014.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  79%|███████▉  | 19260000/24229575 [12:58<02:30, 32985.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19264000/24229575 [12:58<03:18, 25019.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19269000/24229575 [12:58<03:17, 25140.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19273000/24229575 [12:58<03:03, 27073.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19279000/24229575 [12:59<02:56, 28093.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19284000/24229575 [12:59<03:04, 26860.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19288000/24229575 [12:59<02:59, 27597.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19291000/24229575 [12:59<02:57, 27815.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19294000/24229575 [12:59<03:00, 27350.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19298000/24229575 [12:59<02:51, 28800.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19301000/24229575 [12:59<03:16, 25142.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19304000/24229575 [13:00<03:12, 25643.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19312000/24229575 [13:00<02:17, 35830.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19316000/24229575 [13:00<02:49, 28991.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19321000/24229575 [13:00<02:31, 32458.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19325000/24229575 [13:00<02:38, 30997.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19329000/24229575 [13:00<03:33, 22910.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19332000/24229575 [13:01<03:30, 23238.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19335000/24229575 [13:01<04:23, 18577.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19338000/24229575 [13:01<04:34, 17827.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19341000/24229575 [13:01<04:13, 19322.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19345000/24229575 [13:01<03:36, 22557.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19350000/24229575 [13:01<02:53, 28046.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19355000/24229575 [13:02<02:35, 31415.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19359000/24229575 [13:02<02:49, 28677.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19363000/24229575 [13:02<02:41, 30093.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19367000/24229575 [13:02<03:24, 23740.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19373000/24229575 [13:02<03:17, 24594.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19378000/24229575 [13:02<03:01, 26777.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|███████▉  | 19381000/24229575 [13:03<03:32, 22810.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19386000/24229575 [13:03<03:11, 25295.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19390000/24229575 [13:03<03:37, 22266.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19393000/24229575 [13:03<03:31, 22866.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19397000/24229575 [13:03<03:24, 23627.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19402000/24229575 [13:03<02:50, 28244.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19406000/24229575 [13:04<02:49, 28453.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19410000/24229575 [13:04<03:01, 26481.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19417000/24229575 [13:04<02:25, 33163.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19421000/24229575 [13:04<03:04, 26061.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19428000/24229575 [13:04<02:42, 29610.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19432000/24229575 [13:04<02:48, 28515.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19435000/24229575 [13:05<02:50, 28043.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19438000/24229575 [13:05<03:28, 22932.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19441000/24229575 [13:05<03:38, 21929.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19444000/24229575 [13:05<03:58, 20072.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19449000/24229575 [13:05<03:17, 24232.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19453000/24229575 [13:05<03:04, 25954.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19456000/24229575 [13:06<03:17, 24152.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19460000/24229575 [13:06<03:42, 21466.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19465000/24229575 [13:06<02:59, 26496.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19470000/24229575 [13:06<02:49, 28140.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19474000/24229575 [13:06<03:26, 23012.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19477000/24229575 [13:06<03:31, 22471.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19480000/24229575 [13:07<03:29, 22685.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19483000/24229575 [13:07<03:30, 22570.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19487000/24229575 [13:07<03:12, 24658.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19491000/24229575 [13:07<02:50, 27751.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19494000/24229575 [13:07<02:47, 28236.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19499000/24229575 [13:07<02:35, 30397.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  80%|████████  | 19503000/24229575 [13:07<02:37, 30082.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19507000/24229575 [13:08<02:52, 27361.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19511000/24229575 [13:08<02:36, 30157.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19516000/24229575 [13:08<02:35, 30404.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19520000/24229575 [13:08<02:42, 28939.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19523000/24229575 [13:08<02:55, 26883.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19526000/24229575 [13:08<03:35, 21795.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19529000/24229575 [13:08<03:29, 22489.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19532000/24229575 [13:08<03:15, 24057.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19536000/24229575 [13:09<03:05, 25289.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19540000/24229575 [13:09<02:45, 28397.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19543000/24229575 [13:09<03:27, 22562.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19546000/24229575 [13:09<03:46, 20640.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19549000/24229575 [13:09<03:40, 21231.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19552000/24229575 [13:10<04:35, 16968.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19560000/24229575 [13:10<03:04, 25299.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19564000/24229575 [13:10<02:55, 26608.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19567000/24229575 [13:10<02:55, 26552.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19570000/24229575 [13:10<03:25, 22723.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19574000/24229575 [13:10<02:57, 26242.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19578000/24229575 [13:10<02:38, 29273.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19582000/24229575 [13:11<02:59, 25910.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19585000/24229575 [13:11<03:06, 24925.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19591000/24229575 [13:11<02:31, 30581.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19595000/24229575 [13:11<03:13, 23936.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19600000/24229575 [13:11<02:44, 28151.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19604000/24229575 [13:11<02:39, 28939.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19608000/24229575 [13:11<02:35, 29798.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19612000/24229575 [13:12<03:10, 24297.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19615000/24229575 [13:12<03:09, 24317.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19619000/24229575 [13:12<02:50, 27095.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19622000/24229575 [13:12<03:18, 23185.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19627000/24229575 [13:12<03:15, 23519.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19633000/24229575 [13:12<02:36, 29337.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19637000/24229575 [13:13<03:15, 23501.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19642000/24229575 [13:13<02:53, 26480.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19645000/24229575 [13:13<03:46, 20196.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19648000/24229575 [13:13<03:31, 21693.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19651000/24229575 [13:13<03:26, 22160.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19654000/24229575 [13:13<03:23, 22517.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19657000/24229575 [13:14<03:38, 20912.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19664000/24229575 [13:14<02:50, 26705.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19670000/24229575 [13:14<02:23, 31859.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19674000/24229575 [13:14<02:15, 33607.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19678000/24229575 [13:14<02:23, 31814.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19682000/24229575 [13:14<03:12, 23604.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████  | 19685000/24229575 [13:15<03:14, 23366.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19688000/24229575 [13:15<03:08, 24121.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19692000/24229575 [13:15<02:46, 27253.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19695000/24229575 [13:15<03:02, 24808.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19700000/24229575 [13:15<02:28, 30506.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19704000/24229575 [13:15<02:56, 25655.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19708000/24229575 [13:15<02:37, 28715.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19712000/24229575 [13:16<03:14, 23183.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19715000/24229575 [13:16<03:20, 22521.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19722000/24229575 [13:16<02:51, 26319.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19726000/24229575 [13:16<02:56, 25505.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19730000/24229575 [13:16<02:59, 25126.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19733000/24229575 [13:17<03:22, 22242.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19736000/24229575 [13:17<03:27, 21692.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19741000/24229575 [13:17<02:46, 26894.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19744000/24229575 [13:17<02:55, 25557.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  81%|████████▏ | 19747000/24229575 [13:17<03:33, 20972.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19751000/24229575 [13:17<03:03, 24389.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19758000/24229575 [13:17<02:15, 33084.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19762000/24229575 [13:18<02:36, 28458.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19766000/24229575 [13:18<02:31, 29414.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19770000/24229575 [13:18<02:34, 28783.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19774000/24229575 [13:18<03:33, 20843.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19783000/24229575 [13:18<02:38, 28032.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19787000/24229575 [13:19<03:11, 23178.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19790000/24229575 [13:19<03:17, 22457.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19794000/24229575 [13:19<03:09, 23361.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19797000/24229575 [13:19<03:10, 23255.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19801000/24229575 [13:19<03:07, 23563.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19804000/24229575 [13:19<03:00, 24539.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19807000/24229575 [13:19<03:02, 24232.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19810000/24229575 [13:20<03:06, 23667.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19814000/24229575 [13:20<02:49, 25985.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19817000/24229575 [13:20<02:49, 25966.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19820000/24229575 [13:20<02:50, 25867.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19825000/24229575 [13:20<02:36, 28198.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19829000/24229575 [13:20<02:31, 28953.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19832000/24229575 [13:20<02:47, 26283.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19835000/24229575 [13:20<02:44, 26756.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19838000/24229575 [13:21<03:01, 24214.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19841000/24229575 [13:21<03:30, 20878.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19848000/24229575 [13:21<02:21, 30934.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19854000/24229575 [13:21<02:04, 35082.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19858000/24229575 [13:21<02:03, 35394.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19862000/24229575 [13:21<02:33, 28511.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19866000/24229575 [13:22<02:33, 28408.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19870000/24229575 [13:22<02:33, 28369.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19873000/24229575 [13:22<02:49, 25747.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19876000/24229575 [13:22<03:41, 19648.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19880000/24229575 [13:22<03:32, 20481.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19883000/24229575 [13:23<04:06, 17637.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19885000/24229575 [13:23<04:54, 14757.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19891000/24229575 [13:23<03:15, 22144.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19894000/24229575 [13:23<03:19, 21697.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19897000/24229575 [13:23<03:12, 22506.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19901000/24229575 [13:23<02:50, 25393.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19904000/24229575 [13:23<02:44, 26279.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19907000/24229575 [13:23<02:52, 25006.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19910000/24229575 [13:24<02:50, 25274.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19916000/24229575 [13:24<02:20, 30785.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19920000/24229575 [13:24<02:16, 31670.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19925000/24229575 [13:24<02:25, 29672.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19929000/24229575 [13:24<03:07, 22920.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19933000/24229575 [13:24<02:44, 26133.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19938000/24229575 [13:25<02:35, 27600.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19944000/24229575 [13:25<02:07, 33577.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19948000/24229575 [13:25<02:14, 31731.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19953000/24229575 [13:25<02:00, 35393.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19957000/24229575 [13:25<02:19, 30634.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19961000/24229575 [13:25<02:34, 27562.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19964000/24229575 [13:25<02:48, 25264.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19968000/24229575 [13:26<02:41, 26342.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19971000/24229575 [13:26<02:40, 26565.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19974000/24229575 [13:26<04:34, 15528.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19977000/24229575 [13:26<04:34, 15504.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19983000/24229575 [13:26<03:24, 20750.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19986000/24229575 [13:27<03:17, 21511.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  82%|████████▏ | 19989000/24229575 [13:27<03:50, 18386.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 19995000/24229575 [13:27<02:53, 24360.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 19998000/24229575 [13:27<02:48, 25096.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20002000/24229575 [13:27<02:33, 27496.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20007000/24229575 [13:27<02:22, 29529.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20011000/24229575 [13:28<02:48, 25045.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20014000/24229575 [13:28<02:49, 24807.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20017000/24229575 [13:28<02:51, 24533.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20021000/24229575 [13:28<02:39, 26386.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20026000/24229575 [13:28<02:18, 30412.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20031000/24229575 [13:28<02:14, 31264.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20037000/24229575 [13:28<02:06, 33030.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20044000/24229575 [13:29<02:09, 32233.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20048000/24229575 [13:29<02:33, 27210.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20056000/24229575 [13:29<01:59, 34931.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20060000/24229575 [13:29<02:21, 29450.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20064000/24229575 [13:30<03:22, 20575.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20067000/24229575 [13:30<03:30, 19772.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20071000/24229575 [13:30<03:07, 22180.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20074000/24229575 [13:30<03:15, 21220.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20077000/24229575 [13:30<03:10, 21768.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20082000/24229575 [13:30<02:46, 24981.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20085000/24229575 [13:30<02:48, 24565.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20088000/24229575 [13:31<03:12, 21460.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20091000/24229575 [13:31<03:13, 21396.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20095000/24229575 [13:31<02:44, 25199.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20098000/24229575 [13:31<02:43, 25295.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20102000/24229575 [13:31<02:24, 28639.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20106000/24229575 [13:31<02:15, 30378.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20110000/24229575 [13:31<02:54, 23591.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20113000/24229575 [13:32<02:47, 24529.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20118000/24229575 [13:32<02:15, 30270.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20125000/24229575 [13:32<01:44, 39211.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20130000/24229575 [13:32<01:42, 40086.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20135000/24229575 [13:32<02:54, 23495.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20139000/24229575 [13:32<02:53, 23511.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20143000/24229575 [13:33<02:40, 25440.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20147000/24229575 [13:33<02:40, 25444.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20153000/24229575 [13:33<02:14, 30220.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20157000/24229575 [13:33<02:17, 29541.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20161000/24229575 [13:33<03:00, 22486.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20165000/24229575 [13:33<02:48, 24057.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20168000/24229575 [13:34<02:56, 22984.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20171000/24229575 [13:34<02:46, 24324.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20174000/24229575 [13:34<02:42, 25003.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20177000/24229575 [13:34<02:35, 26048.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20180000/24229575 [13:34<03:49, 17642.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20184000/24229575 [13:34<03:21, 20077.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20187000/24229575 [13:35<03:55, 17147.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20194000/24229575 [13:35<02:31, 26617.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20199000/24229575 [13:35<02:31, 26537.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20203000/24229575 [13:35<02:24, 27897.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20207000/24229575 [13:35<02:46, 24113.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20211000/24229575 [13:35<02:29, 26915.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20216000/24229575 [13:35<02:12, 30255.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20221000/24229575 [13:36<02:14, 29902.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20225000/24229575 [13:36<02:48, 23714.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  83%|████████▎ | 20228000/24229575 [13:36<03:01, 22025.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20233000/24229575 [13:36<02:26, 27317.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20237000/24229575 [13:36<02:35, 25633.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20241000/24229575 [13:36<02:27, 26962.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20244000/24229575 [13:37<02:27, 27089.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20247000/24229575 [13:37<02:31, 26282.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20250000/24229575 [13:37<02:32, 26160.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20256000/24229575 [13:37<01:59, 33294.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20260000/24229575 [13:37<02:26, 27150.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20265000/24229575 [13:37<02:05, 31620.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20269000/24229575 [13:38<02:49, 23351.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20273000/24229575 [13:38<02:43, 24212.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20276000/24229575 [13:38<02:48, 23454.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20280000/24229575 [13:38<02:27, 26756.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20283000/24229575 [13:38<03:02, 21609.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▎ | 20289000/24229575 [13:38<02:34, 25548.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20294000/24229575 [13:38<02:21, 27722.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20297000/24229575 [13:39<03:06, 21117.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20300000/24229575 [13:39<03:02, 21474.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20303000/24229575 [13:39<02:52, 22776.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20306000/24229575 [13:39<02:42, 24130.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20309000/24229575 [13:39<02:39, 24591.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20313000/24229575 [13:39<02:21, 27682.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20316000/24229575 [13:39<02:26, 26754.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20320000/24229575 [13:40<02:24, 27027.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20323000/24229575 [13:40<02:49, 22988.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20326000/24229575 [13:40<03:32, 18384.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20331000/24229575 [13:40<02:43, 23785.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20334000/24229575 [13:40<02:40, 24263.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20338000/24229575 [13:40<02:28, 26204.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20345000/24229575 [13:40<01:52, 34499.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20352000/24229575 [13:41<01:37, 39649.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20357000/24229575 [13:41<01:43, 37261.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20361000/24229575 [13:41<02:04, 31144.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20365000/24229575 [13:41<02:34, 25003.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20368000/24229575 [13:41<02:37, 24470.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20371000/24229575 [13:41<02:37, 24557.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20374000/24229575 [13:42<03:07, 20600.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20378000/24229575 [13:42<02:42, 23655.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20381000/24229575 [13:42<02:38, 24276.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20384000/24229575 [13:42<03:25, 18693.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20389000/24229575 [13:42<02:36, 24585.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20393000/24229575 [13:42<02:24, 26483.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20397000/24229575 [13:43<02:32, 25084.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20402000/24229575 [13:43<02:51, 22310.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20406000/24229575 [13:43<02:29, 25562.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20409000/24229575 [13:43<02:34, 24673.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20412000/24229575 [13:43<02:40, 23839.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20415000/24229575 [13:43<02:43, 23389.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20420000/24229575 [13:44<02:33, 24742.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20425000/24229575 [13:44<02:41, 23519.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20428000/24229575 [13:44<02:42, 23428.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20432000/24229575 [13:44<02:26, 25894.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20436000/24229575 [13:44<02:13, 28375.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20441000/24229575 [13:44<01:59, 31676.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20447000/24229575 [13:44<01:40, 37652.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20451000/24229575 [13:45<01:48, 34774.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20455000/24229575 [13:45<01:48, 34714.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20459000/24229575 [13:45<01:54, 33012.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20463000/24229575 [13:45<02:22, 26489.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20467000/24229575 [13:45<02:31, 24868.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  84%|████████▍ | 20471000/24229575 [13:45<02:16, 27551.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20476000/24229575 [13:45<02:02, 30745.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20480000/24229575 [13:46<03:16, 19110.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20484000/24229575 [13:46<02:49, 22137.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20487000/24229575 [13:46<03:11, 19572.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20491000/24229575 [13:46<02:57, 21101.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20495000/24229575 [13:46<02:41, 23108.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20500000/24229575 [13:47<02:22, 26247.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20503000/24229575 [13:47<02:18, 26907.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20506000/24229575 [13:47<02:15, 27452.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20509000/24229575 [13:47<03:28, 17804.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20512000/24229575 [13:47<03:20, 18552.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20518000/24229575 [13:47<02:20, 26403.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20522000/24229575 [13:47<02:07, 28979.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20526000/24229575 [13:48<02:33, 24154.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20531000/24229575 [13:48<02:11, 28039.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20535000/24229575 [13:48<02:08, 28854.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20539000/24229575 [13:48<02:09, 28603.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20543000/24229575 [13:48<02:06, 29246.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20547000/24229575 [13:48<02:31, 24295.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20552000/24229575 [13:49<02:04, 29482.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20559000/24229575 [13:49<01:39, 36872.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20564000/24229575 [13:49<02:02, 29937.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20568000/24229575 [13:49<02:12, 27674.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20572000/24229575 [13:49<02:19, 26303.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20575000/24229575 [13:49<02:18, 26426.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20578000/24229575 [13:50<03:36, 16881.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20583000/24229575 [13:50<03:04, 19767.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20586000/24229575 [13:50<03:11, 19054.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20590000/24229575 [13:50<02:48, 21562.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▍ | 20594000/24229575 [13:50<02:56, 20643.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20598000/24229575 [13:51<02:39, 22821.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20604000/24229575 [13:51<02:03, 29476.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20608000/24229575 [13:51<02:12, 27373.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20614000/24229575 [13:51<02:14, 26884.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20618000/24229575 [13:51<02:11, 27428.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20624000/24229575 [13:51<01:50, 32690.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20628000/24229575 [13:52<02:16, 26463.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20631000/24229575 [13:52<02:29, 24048.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20634000/24229575 [13:52<02:28, 24250.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20640000/24229575 [13:52<01:59, 30076.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20644000/24229575 [13:52<02:38, 22664.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20647000/24229575 [13:52<02:41, 22158.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20650000/24229575 [13:53<02:48, 21274.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20653000/24229575 [13:53<02:57, 20162.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20659000/24229575 [13:53<02:11, 27214.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20663000/24229575 [13:53<02:14, 26511.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20667000/24229575 [13:53<02:06, 28216.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20672000/24229575 [13:53<01:49, 32474.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20677000/24229575 [13:53<01:37, 36312.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20681000/24229575 [13:54<02:01, 29208.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20685000/24229575 [13:54<02:14, 26307.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20688000/24229575 [13:54<03:06, 18996.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20691000/24229575 [13:54<02:49, 20844.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20696000/24229575 [13:54<02:29, 23684.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20702000/24229575 [13:54<01:56, 30399.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20706000/24229575 [13:55<02:14, 26161.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20711000/24229575 [13:55<01:58, 29588.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  85%|████████▌ | 20715000/24229575 [13:55<02:12, 26444.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20720000/24229575 [13:55<01:54, 30574.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20724000/24229575 [13:55<01:52, 31129.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20728000/24229575 [13:55<02:18, 25236.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20731000/24229575 [13:56<02:27, 23698.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20734000/24229575 [13:56<02:36, 22291.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20737000/24229575 [13:56<03:34, 16269.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20739000/24229575 [13:56<03:27, 16839.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20741000/24229575 [13:56<03:25, 16936.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20746000/24229575 [13:56<02:25, 23862.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20749000/24229575 [13:57<02:45, 20982.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20754000/24229575 [13:57<02:11, 26492.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20758000/24229575 [13:57<02:21, 24550.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20763000/24229575 [13:57<01:58, 29251.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20767000/24229575 [13:57<01:52, 30683.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20771000/24229575 [13:57<02:31, 22881.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20775000/24229575 [13:58<02:14, 25673.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20779000/24229575 [13:58<02:07, 27129.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20783000/24229575 [13:58<02:06, 27249.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20787000/24229575 [13:58<01:58, 29129.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20791000/24229575 [13:58<02:05, 27335.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20796000/24229575 [13:58<01:50, 31004.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20800000/24229575 [13:58<02:15, 25237.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20804000/24229575 [13:59<02:04, 27520.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20812000/24229575 [13:59<01:32, 37021.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20819000/24229575 [13:59<01:19, 42638.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20824000/24229575 [13:59<01:49, 31048.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20828000/24229575 [13:59<02:25, 23359.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20832000/24229575 [14:00<03:14, 17508.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20836000/24229575 [14:00<02:48, 20183.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20839000/24229575 [14:00<03:07, 18123.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20844000/24229575 [14:00<02:32, 22161.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20847000/24229575 [14:00<02:43, 20647.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20853000/24229575 [14:01<02:01, 27779.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20857000/24229575 [14:01<02:04, 27085.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20861000/24229575 [14:01<01:56, 28985.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20865000/24229575 [14:01<02:22, 23561.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20870000/24229575 [14:01<02:09, 25870.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20874000/24229575 [14:01<02:02, 27396.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20878000/24229575 [14:02<02:05, 26743.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20881000/24229575 [14:02<02:05, 26631.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20885000/24229575 [14:02<02:13, 25048.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20889000/24229575 [14:02<01:58, 28129.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▌ | 20893000/24229575 [14:02<02:14, 24858.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20899000/24229575 [14:02<01:44, 31783.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20903000/24229575 [14:02<01:49, 30337.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20909000/24229575 [14:02<01:33, 35665.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20913000/24229575 [14:03<01:48, 30454.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20918000/24229575 [14:03<01:48, 30386.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20922000/24229575 [14:03<02:25, 22740.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20925000/24229575 [14:03<02:20, 23533.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20928000/24229575 [14:03<02:32, 21656.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20934000/24229575 [14:04<02:55, 18791.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20937000/24229575 [14:04<02:41, 20383.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20943000/24229575 [14:04<02:02, 26823.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20947000/24229575 [14:04<02:08, 25520.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20950000/24229575 [14:04<02:54, 18842.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  86%|████████▋ | 20955000/24229575 [14:05<02:27, 22255.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20959000/24229575 [14:05<02:18, 23651.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20962000/24229575 [14:05<02:26, 22354.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20967000/24229575 [14:05<02:00, 26965.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20971000/24229575 [14:05<01:55, 28275.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20975000/24229575 [14:05<01:50, 29339.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20979000/24229575 [14:05<01:56, 27875.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20982000/24229575 [14:06<01:54, 28317.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20986000/24229575 [14:06<01:44, 30942.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20991000/24229575 [14:06<01:31, 35366.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 20995000/24229575 [14:06<01:32, 34826.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21000000/24229575 [14:06<01:27, 36839.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21004000/24229575 [14:06<02:12, 24406.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21008000/24229575 [14:07<02:46, 19356.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21011000/24229575 [14:07<02:51, 18748.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21015000/24229575 [14:07<02:24, 22292.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21019000/24229575 [14:07<02:39, 20189.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21023000/24229575 [14:07<02:18, 23178.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21027000/24229575 [14:07<02:01, 26275.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21031000/24229575 [14:08<01:59, 26834.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21035000/24229575 [14:08<02:01, 26223.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21039000/24229575 [14:08<02:04, 25609.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21043000/24229575 [14:08<01:58, 26961.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21046000/24229575 [14:08<02:08, 24774.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21051000/24229575 [14:08<02:07, 24927.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21055000/24229575 [14:08<01:55, 27424.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21058000/24229575 [14:09<02:10, 24357.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21062000/24229575 [14:09<01:54, 27594.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21066000/24229575 [14:09<01:57, 26930.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21069000/24229575 [14:09<02:04, 25470.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21073000/24229575 [14:09<01:56, 27169.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21076000/24229575 [14:09<02:24, 21852.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21083000/24229575 [14:09<01:41, 31026.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21090000/24229575 [14:10<01:22, 37917.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21095000/24229575 [14:10<01:55, 27144.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21099000/24229575 [14:10<02:24, 21690.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21103000/24229575 [14:10<02:15, 23036.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21106000/24229575 [14:11<02:30, 20731.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21109000/24229575 [14:11<02:20, 22238.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21114000/24229575 [14:11<01:55, 27017.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21118000/24229575 [14:11<02:14, 23166.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21122000/24229575 [14:11<02:17, 22562.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21126000/24229575 [14:11<02:05, 24716.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21131000/24229575 [14:11<01:45, 29298.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21135000/24229575 [14:12<02:32, 20326.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21138000/24229575 [14:12<02:25, 21319.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21142000/24229575 [14:12<02:05, 24644.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21146000/24229575 [14:12<01:56, 26577.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21150000/24229575 [14:12<01:54, 26838.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21153000/24229575 [14:12<02:01, 25320.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21160000/24229575 [14:13<01:31, 33436.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21164000/24229575 [14:13<01:34, 32574.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21168000/24229575 [14:13<01:38, 31141.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21172000/24229575 [14:13<02:18, 22006.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21179000/24229575 [14:13<01:56, 26120.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21184000/24229575 [14:13<01:51, 27419.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21187000/24229575 [14:14<01:51, 27352.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21193000/24229575 [14:14<01:32, 32901.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  87%|████████▋ | 21197000/24229575 [14:14<01:49, 27725.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21201000/24229575 [14:14<01:50, 27523.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21204000/24229575 [14:14<02:07, 23755.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21207000/24229575 [14:14<02:22, 21274.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21211000/24229575 [14:15<02:10, 23142.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21214000/24229575 [14:15<02:32, 19736.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21218000/24229575 [14:15<02:23, 20982.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21221000/24229575 [14:15<02:19, 21627.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21224000/24229575 [14:15<02:30, 20031.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21229000/24229575 [14:15<01:55, 25963.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21233000/24229575 [14:15<01:43, 28961.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21237000/24229575 [14:16<01:42, 29315.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21241000/24229575 [14:16<01:41, 29366.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21245000/24229575 [14:16<01:47, 27683.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21248000/24229575 [14:16<01:54, 25972.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21252000/24229575 [14:16<01:59, 24891.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21256000/24229575 [14:16<01:54, 25919.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21259000/24229575 [14:16<01:58, 25136.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21262000/24229575 [14:17<02:01, 24510.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21266000/24229575 [14:17<01:50, 26819.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21270000/24229575 [14:17<01:40, 29563.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21274000/24229575 [14:17<01:46, 27879.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21278000/24229575 [14:17<01:39, 29601.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21282000/24229575 [14:17<01:52, 26263.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21285000/24229575 [14:17<01:51, 26491.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21289000/24229575 [14:18<01:47, 27368.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21293000/24229575 [14:18<01:52, 26177.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21296000/24229575 [14:18<02:23, 20374.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21299000/24229575 [14:18<02:20, 20815.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21302000/24229575 [14:18<02:16, 21462.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21306000/24229575 [14:18<02:24, 20226.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21310000/24229575 [14:19<02:02, 23814.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21314000/24229575 [14:19<01:49, 26683.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21317000/24229575 [14:19<01:46, 27305.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21321000/24229575 [14:19<01:50, 26300.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21324000/24229575 [14:19<02:01, 23877.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21327000/24229575 [14:19<01:58, 24496.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21330000/24229575 [14:19<01:53, 25554.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21333000/24229575 [14:20<02:28, 19551.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21336000/24229575 [14:20<02:31, 19137.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21341000/24229575 [14:20<02:05, 22938.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21344000/24229575 [14:20<02:11, 21986.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21348000/24229575 [14:20<01:53, 25390.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21353000/24229575 [14:20<01:44, 27428.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21356000/24229575 [14:20<01:47, 26736.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21360000/24229575 [14:21<01:41, 28342.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21364000/24229575 [14:21<01:34, 30475.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21368000/24229575 [14:21<01:31, 31214.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21373000/24229575 [14:21<01:20, 35337.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21378000/24229575 [14:21<01:19, 35983.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21382000/24229575 [14:21<01:23, 34047.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21386000/24229575 [14:21<01:41, 28101.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21390000/24229575 [14:22<02:04, 22751.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21394000/24229575 [14:22<01:50, 25743.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21397000/24229575 [14:22<01:56, 24323.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21400000/24229575 [14:22<01:54, 24784.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21405000/24229575 [14:22<01:42, 27425.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21408000/24229575 [14:22<01:53, 24812.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21411000/24229575 [14:22<01:56, 24197.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21415000/24229575 [14:23<01:47, 26241.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21418000/24229575 [14:23<01:48, 25970.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21421000/24229575 [14:23<02:15, 20779.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21424000/24229575 [14:23<02:33, 18277.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21426000/24229575 [14:23<02:50, 16478.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21430000/24229575 [14:23<02:16, 20444.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21433000/24229575 [14:24<02:30, 18553.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21436000/24229575 [14:24<02:26, 19130.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  88%|████████▊ | 21441000/24229575 [14:24<01:50, 25242.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21444000/24229575 [14:24<01:48, 25577.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21449000/24229575 [14:24<01:28, 31288.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21454000/24229575 [14:24<01:17, 35596.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21458000/24229575 [14:24<01:23, 33207.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21462000/24229575 [14:24<01:40, 27438.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21466000/24229575 [14:25<01:35, 29058.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21470000/24229575 [14:25<01:45, 26065.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21473000/24229575 [14:25<01:58, 23165.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21476000/24229575 [14:25<01:57, 23412.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21481000/24229575 [14:25<01:35, 28875.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21485000/24229575 [14:25<02:01, 22582.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21491000/24229575 [14:26<01:44, 26229.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21494000/24229575 [14:26<02:03, 22196.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21497000/24229575 [14:26<01:57, 23354.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▊ | 21502000/24229575 [14:26<01:36, 28198.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21506000/24229575 [14:26<01:31, 29854.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21512000/24229575 [14:26<01:15, 35998.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21516000/24229575 [14:26<01:22, 32699.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21520000/24229575 [14:27<01:37, 27921.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21524000/24229575 [14:27<01:31, 29482.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21528000/24229575 [14:27<02:11, 20541.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21531000/24229575 [14:27<02:19, 19409.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21534000/24229575 [14:27<02:08, 20926.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21537000/24229575 [14:28<02:24, 18622.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21540000/24229575 [14:28<02:10, 20570.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21544000/24229575 [14:28<01:49, 24547.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21550000/24229575 [14:28<01:24, 31579.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21554000/24229575 [14:28<01:34, 28459.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21558000/24229575 [14:28<01:37, 27281.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21562000/24229575 [14:28<01:40, 26594.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21565000/24229575 [14:29<01:47, 24741.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21568000/24229575 [14:29<01:43, 25696.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21571000/24229575 [14:29<01:41, 26225.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21576000/24229575 [14:29<01:26, 30731.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21580000/24229575 [14:29<01:30, 29380.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21584000/24229575 [14:29<01:42, 25932.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21587000/24229575 [14:29<01:56, 22687.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21593000/24229575 [14:30<01:28, 29910.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21597000/24229575 [14:30<01:52, 23476.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21603000/24229575 [14:30<01:26, 30253.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21607000/24229575 [14:30<01:36, 27055.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21611000/24229575 [14:30<01:30, 29052.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21616000/24229575 [14:30<01:29, 29253.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21620000/24229575 [14:31<01:58, 22093.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21623000/24229575 [14:31<01:51, 23427.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21628000/24229575 [14:31<01:52, 23108.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21632000/24229575 [14:31<01:42, 25343.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21635000/24229575 [14:31<01:41, 25562.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21638000/24229575 [14:31<01:58, 21935.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21641000/24229575 [14:32<02:07, 20280.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21644000/24229575 [14:32<02:11, 19609.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21651000/24229575 [14:32<01:32, 27875.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21655000/24229575 [14:32<01:50, 23262.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21658000/24229575 [14:32<01:50, 23336.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21661000/24229575 [14:33<02:13, 19171.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21664000/24229575 [14:33<02:05, 20447.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21669000/24229575 [14:33<01:39, 25824.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21676000/24229575 [14:33<01:16, 33172.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21680000/24229575 [14:33<01:21, 31361.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  89%|████████▉ | 21684000/24229575 [14:33<01:29, 28539.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21691000/24229575 [14:33<01:08, 36816.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21696000/24229575 [14:33<01:12, 34768.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21700000/24229575 [14:34<01:19, 31756.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21704000/24229575 [14:34<01:35, 26551.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21707000/24229575 [14:34<01:42, 24515.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21712000/24229575 [14:34<01:27, 28732.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21716000/24229575 [14:34<01:20, 31041.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21720000/24229575 [14:34<01:37, 25610.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21723000/24229575 [14:35<01:54, 21907.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21726000/24229575 [14:35<02:00, 20707.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21731000/24229575 [14:35<01:48, 22953.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21734000/24229575 [14:35<01:43, 24006.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21737000/24229575 [14:35<01:45, 23692.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21740000/24229575 [14:35<01:54, 21695.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21743000/24229575 [14:36<02:24, 17183.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21750000/24229575 [14:36<01:33, 26601.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21754000/24229575 [14:36<01:28, 28077.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21758000/24229575 [14:36<01:33, 26409.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21762000/24229575 [14:36<01:24, 29124.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21766000/24229575 [14:36<01:31, 27015.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21769000/24229575 [14:37<01:46, 23028.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21772000/24229575 [14:37<02:11, 18634.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21777000/24229575 [14:37<01:44, 23478.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21781000/24229575 [14:37<01:32, 26495.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21786000/24229575 [14:37<01:29, 27186.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21791000/24229575 [14:37<01:32, 26277.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21796000/24229575 [14:38<01:19, 30571.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|████████▉ | 21803000/24229575 [14:38<01:11, 33839.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21807000/24229575 [14:38<01:14, 32580.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21811000/24229575 [14:38<01:20, 30016.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21815000/24229575 [14:38<01:32, 26177.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21818000/24229575 [14:38<01:38, 24605.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21821000/24229575 [14:38<01:41, 23822.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21828000/24229575 [14:39<01:21, 29428.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21831000/24229575 [14:39<01:32, 25951.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21834000/24229575 [14:39<01:58, 20195.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21838000/24229575 [14:39<01:43, 23151.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21841000/24229575 [14:39<01:59, 20015.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21847000/24229575 [14:40<01:36, 24594.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21852000/24229575 [14:40<01:36, 24590.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21856000/24229575 [14:40<01:38, 23999.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21860000/24229575 [14:40<01:27, 26989.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21863000/24229575 [14:40<01:34, 25120.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21868000/24229575 [14:40<01:24, 27812.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21871000/24229575 [14:41<01:35, 24673.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21874000/24229575 [14:41<01:54, 20614.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21877000/24229575 [14:41<01:50, 21302.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21883000/24229575 [14:41<01:20, 29114.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21887000/24229575 [14:41<01:20, 29030.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21891000/24229575 [14:41<01:24, 27647.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21895000/24229575 [14:41<01:27, 26791.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21899000/24229575 [14:42<01:22, 28361.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21904000/24229575 [14:42<01:10, 33151.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21908000/24229575 [14:42<01:18, 29681.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21912000/24229575 [14:42<01:23, 27738.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21915000/24229575 [14:42<01:23, 27661.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21918000/24229575 [14:42<01:36, 23954.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21922000/24229575 [14:42<01:30, 25409.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  90%|█████████ | 21926000/24229575 [14:43<01:25, 26801.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21930000/24229575 [14:43<01:20, 28473.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21933000/24229575 [14:43<01:22, 27827.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21936000/24229575 [14:43<01:23, 27467.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21939000/24229575 [14:43<01:38, 23148.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21942000/24229575 [14:43<01:49, 20934.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21945000/24229575 [14:43<01:56, 19544.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21948000/24229575 [14:44<01:54, 19950.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21952000/24229575 [14:44<01:36, 23680.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21955000/24229575 [14:44<01:30, 25003.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21958000/24229575 [14:44<01:49, 20675.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21962000/24229575 [14:44<01:37, 23337.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21965000/24229575 [14:44<01:42, 22144.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21968000/24229575 [14:44<01:36, 23537.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21975000/24229575 [14:45<01:12, 31285.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21979000/24229575 [14:45<01:11, 31677.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21983000/24229575 [14:45<01:19, 28229.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21988000/24229575 [14:45<01:21, 27624.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21991000/24229575 [14:45<01:28, 25236.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21996000/24229575 [14:45<01:17, 28888.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 21999000/24229575 [14:45<01:19, 28179.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22004000/24229575 [14:46<01:10, 31599.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22008000/24229575 [14:46<01:07, 32991.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22012000/24229575 [14:46<01:10, 31586.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22016000/24229575 [14:46<01:14, 29860.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22020000/24229575 [14:46<01:24, 26049.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22024000/24229575 [14:46<01:16, 28888.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22028000/24229575 [14:46<01:34, 23207.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22031000/24229575 [14:47<01:47, 20371.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22037000/24229575 [14:47<01:24, 25941.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22042000/24229575 [14:47<01:13, 29835.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22046000/24229575 [14:47<01:19, 27609.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22050000/24229575 [14:47<01:36, 22688.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22053000/24229575 [14:48<01:53, 19114.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22056000/24229575 [14:48<01:51, 19476.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22059000/24229575 [14:48<01:50, 19616.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22064000/24229575 [14:48<01:26, 25058.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22069000/24229575 [14:48<01:11, 30396.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22073000/24229575 [14:48<01:13, 29455.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22077000/24229575 [14:48<01:16, 28245.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22081000/24229575 [14:49<01:20, 26687.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22084000/24229575 [14:49<01:22, 26140.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22088000/24229575 [14:49<01:20, 26590.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22091000/24229575 [14:49<01:33, 22975.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22096000/24229575 [14:49<01:24, 25359.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22099000/24229575 [14:49<01:25, 24981.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22102000/24229575 [14:50<01:41, 20867.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22105000/24229575 [14:50<01:36, 22049.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████ | 22108000/24229575 [14:50<01:30, 23499.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22111000/24229575 [14:50<01:27, 24289.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22116000/24229575 [14:50<01:11, 29527.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22121000/24229575 [14:50<01:02, 33816.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22125000/24229575 [14:50<01:11, 29590.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22129000/24229575 [14:50<01:09, 30170.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22134000/24229575 [14:51<01:02, 33696.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22138000/24229575 [14:51<01:03, 32940.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22142000/24229575 [14:51<01:28, 23488.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22145000/24229575 [14:51<01:34, 22023.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22148000/24229575 [14:51<01:49, 18995.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22152000/24229575 [14:51<01:37, 21285.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22156000/24229575 [14:52<01:29, 23188.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22160000/24229575 [14:52<01:18, 26481.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22163000/24229575 [14:52<01:21, 25288.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22166000/24229575 [14:52<01:33, 22000.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  91%|█████████▏| 22170000/24229575 [14:52<01:20, 25598.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22174000/24229575 [14:52<01:19, 25800.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22179000/24229575 [14:52<01:05, 31094.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22184000/24229575 [14:53<01:14, 27437.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22188000/24229575 [14:53<01:21, 24926.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22193000/24229575 [14:53<01:14, 27500.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22196000/24229575 [14:53<01:16, 26645.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22200000/24229575 [14:53<01:13, 27752.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22203000/24229575 [14:54<01:45, 19199.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22206000/24229575 [14:54<01:37, 20690.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22209000/24229575 [14:54<01:29, 22560.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22212000/24229575 [14:54<01:27, 23188.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22215000/24229575 [14:54<01:26, 23230.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22218000/24229575 [14:54<01:21, 24594.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22224000/24229575 [14:54<01:03, 31701.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22228000/24229575 [14:54<00:59, 33667.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22232000/24229575 [14:54<00:57, 34857.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22236000/24229575 [14:55<00:57, 34686.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22241000/24229575 [14:55<01:04, 30973.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22245000/24229575 [14:55<01:25, 23129.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22250000/24229575 [14:55<01:15, 26323.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22253000/24229575 [14:55<01:33, 21093.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22257000/24229575 [14:56<01:22, 24019.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22260000/24229575 [14:56<01:25, 23000.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22263000/24229575 [14:56<01:33, 21106.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22266000/24229575 [14:56<01:31, 21347.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22272000/24229575 [14:56<01:15, 25908.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22276000/24229575 [14:56<01:13, 26569.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22281000/24229575 [14:56<01:03, 30593.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22285000/24229575 [14:57<01:03, 30455.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22289000/24229575 [14:57<01:04, 30026.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22293000/24229575 [14:57<01:08, 28345.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22296000/24229575 [14:57<01:18, 24632.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22299000/24229575 [14:57<01:22, 23301.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22302000/24229575 [14:57<01:28, 21714.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22306000/24229575 [14:57<01:29, 21516.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22309000/24229575 [14:58<01:24, 22710.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22313000/24229575 [14:58<01:15, 25446.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22316000/24229575 [14:58<01:32, 20600.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22321000/24229575 [14:58<01:14, 25701.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22325000/24229575 [14:58<01:06, 28739.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22330000/24229575 [14:58<00:59, 31840.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22335000/24229575 [14:58<00:53, 35356.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22339000/24229575 [14:59<01:02, 30350.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22343000/24229575 [14:59<01:10, 26888.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22348000/24229575 [14:59<01:00, 30918.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22352000/24229575 [14:59<01:05, 28775.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22356000/24229575 [14:59<01:39, 18742.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22359000/24229575 [15:00<01:31, 20445.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22363000/24229575 [15:00<01:20, 23308.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22366000/24229575 [15:00<01:31, 20358.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22370000/24229575 [15:00<01:21, 22946.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22373000/24229575 [15:00<01:19, 23446.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22379000/24229575 [15:00<00:59, 30928.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22383000/24229575 [15:00<01:12, 25611.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22386000/24229575 [15:01<01:12, 25340.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22389000/24229575 [15:01<01:11, 25792.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22392000/24229575 [15:01<01:12, 25262.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22395000/24229575 [15:01<01:26, 21189.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22400000/24229575 [15:01<01:11, 25455.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22403000/24229575 [15:01<01:09, 26172.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22407000/24229575 [15:01<01:07, 27098.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  92%|█████████▏| 22410000/24229575 [15:02<01:09, 26356.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22413000/24229575 [15:02<01:06, 27167.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22416000/24229575 [15:02<01:07, 26913.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22422000/24229575 [15:02<00:52, 34164.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22426000/24229575 [15:02<00:52, 34047.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22430000/24229575 [15:02<01:04, 28004.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22433000/24229575 [15:02<01:06, 27212.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22436000/24229575 [15:02<01:09, 25827.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22442000/24229575 [15:03<00:53, 33125.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22446000/24229575 [15:03<01:01, 29159.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22450000/24229575 [15:03<01:02, 28559.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22453000/24229575 [15:03<01:25, 20888.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22457000/24229575 [15:03<01:28, 20139.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22462000/24229575 [15:04<01:17, 22703.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22466000/24229575 [15:04<01:10, 24928.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22472000/24229575 [15:04<00:56, 31020.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22476000/24229575 [15:04<01:06, 26253.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22479000/24229575 [15:04<01:18, 22211.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22482000/24229575 [15:04<01:29, 19603.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22485000/24229575 [15:05<01:25, 20356.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22488000/24229575 [15:05<01:23, 20965.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22491000/24229575 [15:05<01:19, 21922.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22496000/24229575 [15:05<01:06, 25950.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22501000/24229575 [15:05<01:00, 28525.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22505000/24229575 [15:05<00:57, 30224.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22510000/24229575 [15:05<01:00, 28596.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22513000/24229575 [15:06<01:21, 21185.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22518000/24229575 [15:06<01:05, 26114.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22526000/24229575 [15:06<00:46, 36424.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22531000/24229575 [15:06<00:58, 28991.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22535000/24229575 [15:06<00:55, 30552.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22540000/24229575 [15:06<00:55, 30522.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22544000/24229575 [15:06<00:52, 31899.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22548000/24229575 [15:07<00:50, 33463.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22552000/24229575 [15:07<01:13, 22946.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22555000/24229575 [15:07<01:33, 17932.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22559000/24229575 [15:07<01:18, 21325.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22563000/24229575 [15:07<01:08, 24228.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22568000/24229575 [15:08<00:59, 27777.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22572000/24229575 [15:08<01:23, 19877.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22575000/24229575 [15:08<01:23, 19853.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22580000/24229575 [15:08<01:10, 23279.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22585000/24229575 [15:08<00:59, 27497.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22589000/24229575 [15:09<01:06, 24814.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22595000/24229575 [15:09<00:52, 31396.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22599000/24229575 [15:09<01:04, 25410.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22603000/24229575 [15:09<01:06, 24495.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22609000/24229575 [15:09<00:53, 30224.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22613000/24229575 [15:09<01:14, 21735.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22621000/24229575 [15:10<00:57, 28119.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22625000/24229575 [15:10<00:54, 29371.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22630000/24229575 [15:10<00:48, 32783.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22636000/24229575 [15:10<00:47, 33670.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22640000/24229575 [15:10<00:51, 30858.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22644000/24229575 [15:11<01:07, 23346.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22647000/24229575 [15:11<01:10, 22356.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22650000/24229575 [15:11<01:14, 21178.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  93%|█████████▎| 22654000/24229575 [15:11<01:06, 23715.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22657000/24229575 [15:11<01:17, 20161.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22660000/24229575 [15:11<01:13, 21368.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22663000/24229575 [15:11<01:15, 20839.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22668000/24229575 [15:12<01:01, 25446.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22672000/24229575 [15:12<00:55, 27978.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22675000/24229575 [15:12<00:59, 25931.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22679000/24229575 [15:12<00:54, 28662.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22683000/24229575 [15:12<01:01, 25349.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22686000/24229575 [15:12<00:59, 25908.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22689000/24229575 [15:12<01:10, 21700.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22694000/24229575 [15:13<00:57, 26933.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22698000/24229575 [15:13<00:58, 26049.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22701000/24229575 [15:13<01:05, 23296.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22704000/24229575 [15:13<01:10, 21787.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22707000/24229575 [15:13<01:05, 23218.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▎| 22712000/24229575 [15:13<00:53, 28380.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22718000/24229575 [15:13<00:46, 32731.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22722000/24229575 [15:14<00:45, 33057.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22726000/24229575 [15:14<00:51, 28931.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22730000/24229575 [15:14<00:50, 29602.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22734000/24229575 [15:14<00:54, 27553.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22737000/24229575 [15:14<00:53, 27925.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22740000/24229575 [15:14<01:01, 24293.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22743000/24229575 [15:14<01:05, 22804.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22746000/24229575 [15:15<01:05, 22783.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22749000/24229575 [15:15<01:00, 24351.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22752000/24229575 [15:15<01:10, 21002.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22756000/24229575 [15:15<01:07, 21949.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22761000/24229575 [15:15<00:53, 27689.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22765000/24229575 [15:15<00:59, 24701.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22769000/24229575 [15:16<01:01, 23923.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22773000/24229575 [15:16<00:55, 26042.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22776000/24229575 [15:16<01:07, 21462.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22779000/24229575 [15:16<01:12, 19886.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22783000/24229575 [15:16<01:01, 23669.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22786000/24229575 [15:16<01:00, 23819.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22789000/24229575 [15:16<01:00, 23814.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22796000/24229575 [15:17<00:43, 32776.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22800000/24229575 [15:17<00:48, 29540.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22804000/24229575 [15:17<00:55, 25485.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22812000/24229575 [15:17<00:41, 34105.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22816000/24229575 [15:17<00:44, 31591.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22820000/24229575 [15:17<00:51, 27486.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22825000/24229575 [15:18<00:51, 27445.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22828000/24229575 [15:18<00:50, 27860.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22831000/24229575 [15:18<01:02, 22546.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22834000/24229575 [15:18<00:59, 23501.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22840000/24229575 [15:18<00:44, 30932.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22844000/24229575 [15:18<00:46, 29875.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22848000/24229575 [15:18<00:51, 26741.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22851000/24229575 [15:19<00:50, 27205.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22854000/24229575 [15:19<00:51, 26612.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22857000/24229575 [15:19<01:05, 21042.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22860000/24229575 [15:19<01:12, 18873.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22865000/24229575 [15:19<00:59, 22875.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22868000/24229575 [15:19<01:05, 20770.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22871000/24229575 [15:20<01:08, 19909.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22876000/24229575 [15:20<00:56, 24121.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22880000/24229575 [15:20<00:57, 23467.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22884000/24229575 [15:20<00:50, 26748.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22887000/24229575 [15:20<00:56, 23884.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22890000/24229575 [15:20<00:54, 24615.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  94%|█████████▍| 22893000/24229575 [15:20<00:52, 25454.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22897000/24229575 [15:21<00:45, 29003.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22901000/24229575 [15:21<00:45, 29501.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22905000/24229575 [15:21<00:44, 29858.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22910000/24229575 [15:21<00:52, 25294.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22915000/24229575 [15:21<00:47, 27964.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22918000/24229575 [15:21<00:49, 26650.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22922000/24229575 [15:21<00:50, 26012.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22925000/24229575 [15:22<00:51, 25562.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22931000/24229575 [15:22<00:40, 32233.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22935000/24229575 [15:22<00:46, 27826.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22938000/24229575 [15:22<00:46, 27621.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22944000/24229575 [15:22<00:42, 30242.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22948000/24229575 [15:22<00:48, 26584.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22951000/24229575 [15:22<00:47, 27027.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22955000/24229575 [15:23<00:42, 29959.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22959000/24229575 [15:23<00:47, 26698.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22962000/24229575 [15:23<01:13, 17168.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22965000/24229575 [15:23<01:07, 18854.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22968000/24229575 [15:23<01:05, 19189.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22972000/24229575 [15:24<00:57, 21776.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22979000/24229575 [15:24<00:40, 30508.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22984000/24229575 [15:24<00:47, 26181.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22988000/24229575 [15:24<00:46, 26494.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22993000/24229575 [15:24<00:44, 27938.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 22997000/24229575 [15:24<00:45, 27372.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 23000000/24229575 [15:25<01:00, 20312.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 23005000/24229575 [15:25<00:52, 23186.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 23012000/24229575 [15:25<00:40, 30243.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▍| 23016000/24229575 [15:25<00:48, 25168.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23021000/24229575 [15:25<00:43, 27768.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23025000/24229575 [15:25<00:45, 26761.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23028000/24229575 [15:26<00:51, 23355.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23033000/24229575 [15:26<00:43, 27799.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23037000/24229575 [15:26<00:39, 29984.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23041000/24229575 [15:26<00:37, 31297.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23045000/24229575 [15:26<00:47, 25178.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23052000/24229575 [15:26<00:34, 34282.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23057000/24229575 [15:27<00:39, 29377.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23061000/24229575 [15:27<00:45, 25642.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23065000/24229575 [15:27<00:54, 21397.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23068000/24229575 [15:27<00:57, 20192.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23071000/24229575 [15:27<00:54, 21338.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23075000/24229575 [15:27<00:48, 23768.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23078000/24229575 [15:28<00:52, 21990.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23081000/24229575 [15:28<00:54, 20926.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23087000/24229575 [15:28<00:43, 26520.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23090000/24229575 [15:28<00:52, 21706.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23097000/24229575 [15:28<00:37, 30046.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23101000/24229575 [15:28<00:36, 30929.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23105000/24229575 [15:29<00:45, 24680.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23110000/24229575 [15:29<00:39, 28374.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23115000/24229575 [15:29<00:42, 26288.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23118000/24229575 [15:29<00:43, 25673.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23122000/24229575 [15:29<00:40, 27608.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23127000/24229575 [15:29<00:36, 30159.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23131000/24229575 [15:29<00:35, 30840.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  95%|█████████▌| 23135000/24229575 [15:30<00:48, 22747.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23143000/24229575 [15:30<00:38, 28165.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23147000/24229575 [15:30<00:38, 28211.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23151000/24229575 [15:30<00:37, 28508.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23155000/24229575 [15:30<00:38, 28194.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23158000/24229575 [15:31<00:41, 26109.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23163000/24229575 [15:31<00:44, 24070.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23166000/24229575 [15:31<00:52, 20426.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23170000/24229575 [15:31<00:44, 23605.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23173000/24229575 [15:31<00:50, 21042.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23176000/24229575 [15:31<00:49, 21388.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23179000/24229575 [15:32<00:50, 20769.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23184000/24229575 [15:32<00:40, 25870.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23187000/24229575 [15:32<00:47, 21907.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23192000/24229575 [15:32<00:38, 27144.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23197000/24229575 [15:32<00:37, 27810.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23200000/24229575 [15:32<00:38, 26597.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23204000/24229575 [15:32<00:38, 26515.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23209000/24229575 [15:33<00:37, 27318.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23215000/24229575 [15:33<00:30, 33097.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23219000/24229575 [15:33<00:39, 25497.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23226000/24229575 [15:33<00:37, 26803.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23231000/24229575 [15:33<00:32, 30878.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23235000/24229575 [15:34<00:33, 29415.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23242000/24229575 [15:34<00:28, 34687.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23246000/24229575 [15:34<00:42, 22931.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23250000/24229575 [15:34<00:41, 23883.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23253000/24229575 [15:34<00:43, 22228.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23256000/24229575 [15:35<00:51, 18829.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23260000/24229575 [15:35<00:45, 21214.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23263000/24229575 [15:35<00:44, 21514.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23269000/24229575 [15:35<00:33, 28672.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23273000/24229575 [15:35<00:34, 27412.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23277000/24229575 [15:35<00:36, 26357.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23280000/24229575 [15:35<00:35, 26906.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23286000/24229575 [15:36<00:31, 29805.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23290000/24229575 [15:36<00:32, 29307.89 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23295000/24229575 [15:36<00:28, 32413.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23299000/24229575 [15:36<00:34, 27242.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23303000/24229575 [15:36<00:31, 29570.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23307000/24229575 [15:36<00:34, 27021.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23312000/24229575 [15:36<00:30, 30485.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23316000/24229575 [15:37<00:31, 29403.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▌| 23320000/24229575 [15:37<00:35, 25441.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23323000/24229575 [15:37<00:40, 22424.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23325391/24229575 [15:37<00:39, 22653.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23330391/24229575 [15:37<00:33, 26937.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23335391/24229575 [15:37<00:30, 29199.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23340391/24229575 [15:38<00:29, 29849.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23344391/24229575 [15:38<00:32, 26854.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23347391/24229575 [15:38<00:33, 26643.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23350391/24229575 [15:38<00:38, 23062.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23353391/24229575 [15:38<00:43, 19963.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23356391/24229575 [15:38<00:39, 21912.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23359391/24229575 [15:38<00:37, 23221.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23362391/24229575 [15:39<00:37, 22955.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23365391/24229575 [15:39<00:50, 17031.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23372391/24229575 [15:39<00:32, 26006.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23376391/24229575 [15:39<00:35, 24339.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  96%|█████████▋| 23380391/24229575 [15:39<00:33, 25314.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23383391/24229575 [15:39<00:33, 25144.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23392391/24229575 [15:40<00:22, 37583.59 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23397391/24229575 [15:40<00:28, 28991.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23402391/24229575 [15:40<00:25, 32834.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23407391/24229575 [15:40<00:22, 36364.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23412391/24229575 [15:40<00:36, 22343.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23416391/24229575 [15:41<00:34, 23533.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23420391/24229575 [15:41<00:35, 22555.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=96):  97%|█████████▋| 23423782/24229575 [15:41<00:36, 22124.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  83%|████████▎ | 20199000/24229575 [03:38<00:45, 88407.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  83%|████████▎ | 20214000/24229575 [03:38<00:38, 103696.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  83%|████████▎ | 20225000/24229575 [03:38<00:41, 95846.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▎ | 20235000/24229575 [03:38<00:44, 90753.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▎ | 20248000/24229575 [03:39<00:40, 97888.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▎ | 20258000/24229575 [03:39<00:40, 97586.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▎ | 20269000/24229575 [03:39<00:40, 98044.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▎ | 20279000/24229575 [03:39<00:43, 90991.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▎ | 20289000/24229575 [03:39<00:44, 87655.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20301000/24229575 [03:39<00:41, 93734.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20311000/24229575 [03:39<00:43, 90199.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20321000/24229575 [03:39<00:45, 86771.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20330000/24229575 [03:39<00:44, 86720.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20349000/24229575 [03:40<00:35, 109511.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20360000/24229575 [03:40<00:37, 102154.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20371000/24229575 [03:40<00:40, 94635.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20381000/24229575 [03:40<00:42, 91148.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20393000/24229575 [03:40<00:40, 94368.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20403000/24229575 [03:40<00:43, 88380.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20413000/24229575 [03:40<00:42, 89265.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20423000/24229575 [03:40<00:41, 91219.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20435000/24229575 [03:41<00:39, 96584.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20448000/24229575 [03:41<00:38, 97459.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20458000/24229575 [03:41<00:38, 96995.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  84%|████████▍ | 20468000/24229575 [03:41<00:44, 84349.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20482000/24229575 [03:41<00:40, 93232.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20492000/24229575 [03:41<00:39, 94407.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20503000/24229575 [03:41<00:39, 95159.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20513000/24229575 [03:41<00:40, 92234.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20524000/24229575 [03:41<00:38, 95550.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20536000/24229575 [03:42<00:36, 100811.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20548000/24229575 [03:42<00:37, 97282.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20558000/24229575 [03:42<00:37, 97936.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20568000/24229575 [03:42<00:41, 87423.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20578000/24229575 [03:42<00:43, 84031.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▍ | 20590000/24229575 [03:42<00:41, 88691.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20600000/24229575 [03:42<00:39, 91472.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20612000/24229575 [03:42<00:36, 98148.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20625000/24229575 [03:43<00:35, 102790.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20636000/24229575 [03:43<00:37, 96720.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20649000/24229575 [03:43<00:34, 103928.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20660000/24229575 [03:43<00:40, 89002.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20673000/24229575 [03:43<00:40, 88494.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20684000/24229575 [03:43<00:38, 91440.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20694000/24229575 [03:43<00:40, 87984.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  85%|████████▌ | 20703000/24229575 [03:43<00:41, 85540.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20718000/24229575 [03:44<00:34, 101989.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20732000/24229575 [03:44<00:31, 111440.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20744000/24229575 [03:44<00:35, 97332.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20755000/24229575 [03:44<00:36, 94040.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20765000/24229575 [03:44<00:36, 94379.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20775000/24229575 [03:44<00:41, 83476.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20791000/24229575 [03:44<00:35, 97897.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20802000/24229575 [03:44<00:38, 89365.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20818000/24229575 [03:45<00:32, 106003.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20830000/24229575 [03:45<00:32, 104059.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20841000/24229575 [03:45<00:35, 95686.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20851000/24229575 [03:45<00:36, 93318.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20861000/24229575 [03:45<00:40, 83976.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20871000/24229575 [03:45<00:38, 87346.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▌ | 20889000/24229575 [03:45<00:32, 103713.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▋ | 20900000/24229575 [03:45<00:34, 96708.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▋ | 20912000/24229575 [03:46<00:32, 101650.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▋ | 20923000/24229575 [03:46<00:37, 88639.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▋ | 20934000/24229575 [03:46<00:35, 93421.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▋ | 20946000/24229575 [03:46<00:33, 98390.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  86%|████████▋ | 20957000/24229575 [03:46<00:36, 88478.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 20967000/24229575 [03:46<00:35, 90832.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 20980000/24229575 [03:46<00:32, 100376.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 20991000/24229575 [03:46<00:35, 90595.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21006000/24229575 [03:47<00:31, 102597.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21017000/24229575 [03:47<00:31, 101246.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21028000/24229575 [03:47<00:34, 92291.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21040000/24229575 [03:47<00:32, 98533.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21051000/24229575 [03:47<00:35, 88699.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21061000/24229575 [03:47<00:35, 89931.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21075000/24229575 [03:47<00:30, 102434.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21086000/24229575 [03:47<00:35, 89208.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21097000/24229575 [03:48<00:33, 92257.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21110000/24229575 [03:48<00:30, 101745.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21121000/24229575 [03:48<00:32, 96840.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21132000/24229575 [03:48<00:35, 88015.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21142000/24229575 [03:48<00:34, 90687.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21155000/24229575 [03:48<00:31, 98571.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21169000/24229575 [03:48<00:28, 109066.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21181000/24229575 [03:48<00:33, 91645.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  87%|████████▋ | 21191000/24229575 [03:49<00:33, 91912.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21201000/24229575 [03:49<00:32, 93963.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21211000/24229575 [03:49<00:33, 89825.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21221000/24229575 [03:49<00:35, 84142.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21234000/24229575 [03:49<00:32, 93299.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21246000/24229575 [03:49<00:30, 97777.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21258000/24229575 [03:49<00:31, 95331.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21269000/24229575 [03:49<00:30, 97989.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21279000/24229575 [03:49<00:31, 92965.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21289000/24229575 [03:50<00:31, 94175.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21300000/24229575 [03:50<00:31, 91739.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21310000/24229575 [03:50<00:32, 90913.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21320000/24229575 [03:50<00:31, 92975.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21330000/24229575 [03:50<00:33, 87038.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21343000/24229575 [03:50<00:29, 97494.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21353000/24229575 [03:50<00:29, 96125.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21363000/24229575 [03:50<00:30, 94312.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21377000/24229575 [03:51<00:27, 102015.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21388000/24229575 [03:51<00:27, 101770.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21399000/24229575 [03:51<00:29, 97387.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21409000/24229575 [03:51<00:34, 82192.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21419000/24229575 [03:51<00:32, 85716.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21432000/24229575 [03:51<00:28, 96790.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  88%|████████▊ | 21443000/24229575 [03:51<00:30, 91597.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▊ | 21457000/24229575 [03:51<00:27, 100662.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▊ | 21471000/24229575 [03:51<00:25, 108433.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▊ | 21483000/24229575 [03:52<00:30, 91274.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▊ | 21493000/24229575 [03:52<00:29, 91547.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▊ | 21503000/24229575 [03:52<00:32, 84056.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21516000/24229575 [03:52<00:29, 91070.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21528000/24229575 [03:52<00:27, 97938.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21539000/24229575 [03:52<00:29, 92102.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21551000/24229575 [03:52<00:29, 90998.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21568000/24229575 [03:53<00:24, 110528.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21580000/24229575 [03:53<00:26, 99112.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21591000/24229575 [03:53<00:28, 92790.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21602000/24229575 [03:53<00:27, 96280.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21612000/24229575 [03:53<00:29, 89028.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21622000/24229575 [03:53<00:28, 90238.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21632000/24229575 [03:53<00:31, 82108.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21643000/24229575 [03:53<00:29, 88370.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21656000/24229575 [03:54<00:26, 96703.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21672000/24229575 [03:54<00:23, 109439.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  89%|████████▉ | 21684000/24229575 [03:54<00:23, 107578.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21695000/24229575 [03:54<00:27, 92031.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21705000/24229575 [03:54<00:27, 92671.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21716000/24229575 [03:54<00:26, 95561.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21726000/24229575 [03:54<00:27, 90152.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21736000/24229575 [03:54<00:29, 83547.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21747000/24229575 [03:54<00:27, 88856.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21763000/24229575 [03:55<00:23, 103388.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21774000/24229575 [03:55<00:25, 98064.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21784000/24229575 [03:55<00:26, 92997.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|████████▉ | 21794000/24229575 [03:55<00:26, 90281.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21807000/24229575 [03:55<00:24, 99796.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21818000/24229575 [03:55<00:26, 90429.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21828000/24229575 [03:55<00:27, 87514.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21838000/24229575 [03:55<00:27, 87659.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21855000/24229575 [03:56<00:23, 102889.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21866000/24229575 [03:56<00:24, 95163.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21877000/24229575 [03:56<00:24, 96143.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21887000/24229575 [03:56<00:26, 88305.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21902000/24229575 [03:56<00:22, 102766.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21913000/24229575 [03:56<00:25, 89401.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  90%|█████████ | 21924000/24229575 [03:56<00:24, 94379.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 21937000/24229575 [03:56<00:23, 99612.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 21951000/24229575 [03:57<00:20, 110049.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 21963000/24229575 [03:57<00:26, 85410.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 21973000/24229575 [03:57<00:27, 81726.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 21984000/24229575 [03:57<00:25, 88213.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 21997000/24229575 [03:57<00:22, 97083.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 22008000/24229575 [03:57<00:23, 93954.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 22022000/24229575 [03:57<00:21, 104300.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 22033000/24229575 [03:57<00:21, 100091.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 22051000/24229575 [03:58<00:18, 120775.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 22064000/24229575 [03:58<00:25, 86578.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 22075000/24229575 [03:58<00:26, 80563.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 22089000/24229575 [03:58<00:23, 91983.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████ | 22100000/24229575 [03:58<00:23, 92147.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████▏| 22113000/24229575 [03:58<00:21, 99003.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████▏| 22124000/24229575 [03:58<00:21, 99939.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████▏| 22139000/24229575 [03:59<00:18, 112059.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████▏| 22151000/24229575 [03:59<00:19, 105621.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  91%|█████████▏| 22162000/24229575 [03:59<00:29, 70917.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22180000/24229575 [03:59<00:22, 89617.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22192000/24229575 [03:59<00:21, 96118.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22204000/24229575 [03:59<00:21, 95726.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22217000/24229575 [03:59<00:19, 103425.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22232000/24229575 [04:00<00:17, 111691.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22244000/24229575 [04:00<00:19, 103047.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22255000/24229575 [04:00<00:24, 80577.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22266000/24229575 [04:00<00:22, 86015.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22276000/24229575 [04:00<00:22, 86578.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22286000/24229575 [04:00<00:22, 87004.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22297000/24229575 [04:00<00:20, 92552.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22313000/24229575 [04:00<00:17, 110102.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22326000/24229575 [04:01<00:16, 114132.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22338000/24229575 [04:01<00:18, 104043.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22349000/24229575 [04:01<00:20, 93342.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22359000/24229575 [04:01<00:24, 77643.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22372000/24229575 [04:01<00:21, 87356.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22382000/24229575 [04:01<00:20, 89288.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22395000/24229575 [04:01<00:18, 99079.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  92%|█████████▏| 22408000/24229575 [04:01<00:17, 101378.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22423000/24229575 [04:02<00:16, 111440.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22435000/24229575 [04:02<00:17, 101343.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22446000/24229575 [04:02<00:21, 81963.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22457000/24229575 [04:02<00:21, 83627.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22471000/24229575 [04:02<00:18, 94213.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22482000/24229575 [04:02<00:18, 93424.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22493000/24229575 [04:02<00:17, 97520.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22506000/24229575 [04:02<00:16, 105828.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22518000/24229575 [04:03<00:15, 108792.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22530000/24229575 [04:03<00:17, 98964.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22541000/24229575 [04:03<00:20, 83864.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22551000/24229575 [04:03<00:20, 83770.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22565000/24229575 [04:03<00:17, 92857.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22577000/24229575 [04:03<00:16, 99297.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22588000/24229575 [04:03<00:16, 96704.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22601000/24229575 [04:03<00:15, 103149.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22612000/24229575 [04:04<00:15, 103757.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22623000/24229575 [04:04<00:16, 97994.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22633000/24229575 [04:04<00:18, 85706.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22642000/24229575 [04:04<00:19, 81232.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  93%|█████████▎| 22654000/24229575 [04:04<00:18, 87035.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▎| 22665000/24229575 [04:04<00:18, 86775.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▎| 22676000/24229575 [04:04<00:16, 91787.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▎| 22691000/24229575 [04:04<00:14, 102608.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▎| 22702000/24229575 [04:05<00:16, 94761.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22718000/24229575 [04:05<00:14, 107484.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22729000/24229575 [04:05<00:16, 89726.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22739000/24229575 [04:05<00:16, 91282.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22750000/24229575 [04:05<00:15, 95465.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22760000/24229575 [04:05<00:16, 88277.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22774000/24229575 [04:05<00:14, 100378.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22786000/24229575 [04:05<00:13, 104313.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22797000/24229575 [04:06<00:13, 104114.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22808000/24229575 [04:06<00:15, 94673.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22818000/24229575 [04:06<00:16, 83106.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22830000/24229575 [04:06<00:15, 91342.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22840000/24229575 [04:06<00:15, 92264.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22850000/24229575 [04:06<00:16, 82813.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22862000/24229575 [04:06<00:15, 90920.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22876000/24229575 [04:06<00:13, 99554.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  94%|█████████▍| 22890000/24229575 [04:07<00:12, 109476.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 22902000/24229575 [04:07<00:13, 95209.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 22913000/24229575 [04:07<00:14, 89315.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 22923000/24229575 [04:07<00:15, 85755.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 22935000/24229575 [04:07<00:14, 90076.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 22952000/24229575 [04:07<00:12, 104301.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 22963000/24229575 [04:07<00:14, 89068.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 22978000/24229575 [04:07<00:12, 98504.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 22989000/24229575 [04:08<00:13, 90008.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 23002000/24229575 [04:08<00:12, 98931.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▍| 23013000/24229575 [04:08<00:12, 96780.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23024000/24229575 [04:08<00:14, 83013.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23040000/24229575 [04:08<00:12, 97337.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23051000/24229575 [04:08<00:12, 96350.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23062000/24229575 [04:08<00:11, 99586.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23073000/24229575 [04:09<00:11, 96592.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23083000/24229575 [04:09<00:12, 94369.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23093000/24229575 [04:09<00:13, 86616.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23107000/24229575 [04:09<00:11, 97726.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23118000/24229575 [04:09<00:11, 96427.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  95%|█████████▌| 23128000/24229575 [04:09<00:11, 96593.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23141000/24229575 [04:09<00:11, 96389.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23151391/24229575 [04:09<00:11, 91204.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23166783/24229575 [04:09<00:09, 106840.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23177783/24229575 [04:10<00:12, 85913.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23187783/24229575 [04:10<00:12, 82294.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23202783/24229575 [04:10<00:10, 97337.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23213783/24229575 [04:10<00:10, 96248.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23224783/24229575 [04:10<00:10, 91702.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23239174/24229575 [04:10<00:09, 102379.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23251174/24229575 [04:10<00:09, 102510.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23262174/24229575 [04:11<00:10, 91142.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23272565/24229575 [04:11<00:11, 83272.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23281565/24229575 [04:11<00:11, 83763.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23296956/24229575 [04:11<00:09, 100827.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23307956/24229575 [04:11<00:08, 102587.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▌| 23319348/24229575 [04:11<00:09, 101124.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▋| 23330348/24229575 [04:11<00:09, 99158.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▋| 23341348/24229575 [04:11<00:08, 98763.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▋| 23351739/24229575 [04:11<00:09, 88710.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▋| 23361739/24229575 [04:12<00:10, 83132.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▋| 23370739/24229575 [04:12<00:10, 83559.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  96%|█████████▋| 23381131/24229575 [04:12<00:09, 86711.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23395131/24229575 [04:12<00:08, 100457.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23407523/24229575 [04:12<00:08, 100988.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23418523/24229575 [04:12<00:08, 98103.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23428523/24229575 [04:12<00:08, 97497.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23438523/24229575 [04:12<00:08, 97886.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23448523/24229575 [04:13<00:08, 87764.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23459523/24229575 [04:13<00:08, 89160.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23469306/24229575 [04:13<00:09, 83503.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23482697/24229575 [04:13<00:07, 95029.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23500089/24229575 [04:13<00:06, 106263.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23511481/24229575 [04:13<00:07, 96246.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23521872/24229575 [04:13<00:07, 97015.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23532264/24229575 [04:13<00:08, 87108.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23542048/24229575 [04:14<00:08, 81348.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23551048/24229575 [04:14<00:08, 78276.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23566830/24229575 [04:14<00:06, 95823.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23577830/24229575 [04:14<00:06, 94073.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23587613/24229575 [04:14<00:06, 93111.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23599613/24229575 [04:14<00:06, 93095.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23610004/24229575 [04:14<00:06, 95386.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  97%|█████████▋| 23619786/24229575 [04:14<00:07, 86694.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23628786/24229575 [04:15<00:08, 72968.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23644177/24229575 [04:15<00:06, 89960.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23654177/24229575 [04:15<00:06, 87626.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23665177/24229575 [04:15<00:06, 90909.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23676177/24229575 [04:15<00:05, 95174.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23686742/24229575 [04:15<00:05, 91294.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23696742/24229575 [04:15<00:06, 85625.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23706133/24229575 [04:15<00:06, 84330.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23715133/24229575 [04:16<00:06, 78052.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23727525/24229575 [04:16<00:05, 86086.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23738916/24229575 [04:16<00:05, 92422.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23751699/24229575 [04:16<00:04, 98152.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23761699/24229575 [04:16<00:05, 81126.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23771699/24229575 [04:16<00:05, 85285.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23781090/24229575 [04:16<00:05, 84596.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23793482/24229575 [04:16<00:04, 92340.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23803482/24229575 [04:16<00:04, 93174.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23813874/24229575 [04:17<00:05, 78828.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23825265/24229575 [04:17<00:04, 86999.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23836657/24229575 [04:17<00:04, 92124.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23849049/24229575 [04:17<00:04, 93352.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  98%|█████████▊| 23858831/24229575 [04:17<00:04, 82296.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▊| 23868223/24229575 [04:17<00:04, 81940.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▊| 23879223/24229575 [04:17<00:04, 87403.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▊| 23888223/24229575 [04:18<00:04, 84943.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▊| 23897223/24229575 [04:18<00:04, 81177.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▊| 23906396/24229575 [04:18<00:03, 82955.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▊| 23915396/24229575 [04:18<00:03, 84094.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▊| 23925396/24229575 [04:18<00:03, 86224.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 23934178/24229575 [04:18<00:03, 75099.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 23946178/24229575 [04:18<00:03, 81045.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 23957569/24229575 [04:18<00:03, 83063.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 23968569/24229575 [04:18<00:03, 85626.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 23977569/24229575 [04:19<00:03, 74154.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 23987961/24229575 [04:19<00:03, 78487.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24000352/24229575 [04:19<00:02, 88592.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24010136/24229575 [04:19<00:02, 88251.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24019310/24229575 [04:19<00:02, 77630.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24027701/24229575 [04:19<00:02, 74613.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24037701/24229575 [04:19<00:02, 79163.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24046092/24229575 [04:20<00:02, 70573.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24057484/24229575 [04:20<00:02, 78083.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24065658/24229575 [04:20<00:02, 66404.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24073440/24229575 [04:20<00:02, 67413.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24081440/24229575 [04:20<00:02, 69251.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24089223/24229575 [04:20<00:02, 61508.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24096615/24229575 [04:20<00:02, 52266.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96):  99%|█████████▉| 24105615/24229575 [04:21<00:02, 56355.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24111615/24229575 [04:21<00:02, 54745.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24117615/24229575 [04:21<00:02, 54811.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24124006/24229575 [04:21<00:02, 49668.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24133397/24229575 [04:21<00:01, 56067.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24139789/24229575 [04:21<00:01, 54222.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24145356/24229575 [04:21<00:01, 48092.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24150356/24229575 [04:21<00:01, 43315.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24155748/24229575 [04:22<00:01, 45111.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24161140/24229575 [04:22<00:01, 42861.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24166531/24229575 [04:22<00:01, 44476.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24171704/24229575 [04:22<00:01, 36042.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24178096/24229575 [04:22<00:01, 39350.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24182488/24229575 [04:22<00:01, 33389.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24186880/24229575 [04:23<00:01, 26145.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24193271/24229575 [04:23<00:01, 27040.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24196271/24229575 [04:23<00:01, 25850.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24199271/24229575 [04:23<00:01, 23572.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24203271/24229575 [04:23<00:01, 25657.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24206662/24229575 [04:24<00:01, 22246.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24209054/24229575 [04:24<00:01, 18016.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24214054/24229575 [04:24<00:00, 21766.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24216836/24229575 [04:24<00:00, 19582.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24219227/24229575 [04:24<00:00, 16624.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24221227/24229575 [04:24<00:00, 16934.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24223227/24229575 [04:25<00:00, 14614.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24225402/24229575 [04:25<00:00, 13130.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24227402/24229575 [04:25<00:00, 13028.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|█████████▉| 24229184/24229575 [04:25<00:00, 8277.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=96): 100%|██████████| 24229575/24229575 [04:26<00:00, 90836.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving data to FSx now ....\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 0/5672672 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 1000/5672672 [00:00<49:43, 1900.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 3000/5672672 [00:00<19:24, 4866.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 4000/5672672 [00:00<17:04, 5531.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 6000/5672672 [00:01<13:03, 7228.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 8000/5672672 [00:01<11:19, 8333.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 10000/5672672 [00:01<10:31, 8965.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 11000/5672672 [00:01<10:26, 9041.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 12000/5672672 [00:01<10:17, 9165.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 14000/5672672 [00:01<09:47, 9628.15 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 16000/5672672 [00:02<09:24, 10023.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/303 shards):   0%|          | 18000/5672672 [00:02<09:11, 10260.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   0%|          | 18722/5672672 [00:02<09:11, 10260.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   0%|          | 19722/5672672 [00:02<09:16, 10156.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   0%|          | 21722/5672672 [00:02<09:02, 10418.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   0%|          | 23722/5672672 [00:02<10:05, 9324.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   0%|          | 25722/5672672 [00:02<09:38, 9768.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   0%|          | 27722/5672672 [00:03<09:16, 10149.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   1%|          | 29722/5672672 [00:03<09:03, 10391.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   1%|          | 31722/5672672 [00:03<08:59, 10462.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   1%|          | 33722/5672672 [00:03<08:51, 10610.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   1%|          | 35722/5672672 [00:03<08:55, 10534.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/303 shards):   1%|          | 37444/5672672 [00:04<08:54, 10548.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 37444/5672672 [00:04<08:54, 10548.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 39444/5672672 [00:04<08:59, 10446.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 41444/5672672 [00:04<10:41, 8777.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 43444/5672672 [00:04<10:05, 9300.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 45444/5672672 [00:04<09:41, 9676.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 47444/5672672 [00:05<09:35, 9771.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 49444/5672672 [00:05<09:20, 10027.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 51444/5672672 [00:05<09:10, 10203.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 53444/5672672 [00:05<08:51, 10570.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (2/303 shards):   1%|          | 55444/5672672 [00:05<08:50, 10582.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|          | 56166/5672672 [00:05<08:50, 10582.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|          | 57166/5672672 [00:06<08:51, 10568.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|          | 59166/5672672 [00:06<09:19, 10036.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|          | 61166/5672672 [00:06<11:38, 8033.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|          | 63166/5672672 [00:06<10:36, 8810.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|          | 65166/5672672 [00:07<10:03, 9292.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|          | 67166/5672672 [00:07<09:38, 9682.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|          | 69166/5672672 [00:07<09:26, 9889.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|▏         | 71166/5672672 [00:07<09:16, 10070.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|▏         | 73166/5672672 [00:07<09:10, 10180.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (3/303 shards):   1%|▏         | 74888/5672672 [00:07<09:05, 10257.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   1%|▏         | 74888/5672672 [00:07<09:05, 10257.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   1%|▏         | 76888/5672672 [00:08<09:23, 9921.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   1%|▏         | 78888/5672672 [00:08<09:57, 9366.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   1%|▏         | 79888/5672672 [00:08<09:53, 9425.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   1%|▏         | 81888/5672672 [00:08<09:37, 9678.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   1%|▏         | 83888/5672672 [00:08<09:18, 10002.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   2%|▏         | 85888/5672672 [00:09<09:11, 10129.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   2%|▏         | 87888/5672672 [00:09<09:05, 10232.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   2%|▏         | 89888/5672672 [00:09<09:11, 10114.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   2%|▏         | 91888/5672672 [00:09<09:09, 10157.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (4/303 shards):   2%|▏         | 93610/5672672 [00:09<09:04, 10242.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 93610/5672672 [00:09<09:04, 10242.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 95610/5672672 [00:10<09:06, 10205.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 97610/5672672 [00:10<09:39, 9623.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 99610/5672672 [00:10<09:18, 9987.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 101610/5672672 [00:10<09:02, 10261.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 103610/5672672 [00:10<08:59, 10330.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 105610/5672672 [00:10<08:48, 10528.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 107610/5672672 [00:11<08:40, 10690.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 109610/5672672 [00:11<08:33, 10825.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (5/303 shards):   2%|▏         | 111610/5672672 [00:11<08:39, 10695.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 112332/5672672 [00:11<08:39, 10695.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 113332/5672672 [00:11<08:46, 10560.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 115332/5672672 [00:11<08:39, 10693.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 117332/5672672 [00:12<09:02, 10231.15 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 119332/5672672 [00:12<14:18, 6471.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 121332/5672672 [00:12<12:32, 7372.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 123332/5672672 [00:13<11:14, 8231.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 125332/5672672 [00:13<10:17, 8984.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 127332/5672672 [00:13<09:39, 9575.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 129332/5672672 [00:13<09:13, 10020.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (6/303 shards):   2%|▏         | 131054/5672672 [00:13<08:57, 10305.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   2%|▏         | 131054/5672672 [00:13<08:57, 10305.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   2%|▏         | 133054/5672672 [00:13<08:53, 10392.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   2%|▏         | 135054/5672672 [00:14<09:04, 10168.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   2%|▏         | 137054/5672672 [00:14<08:56, 10326.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   2%|▏         | 139054/5672672 [00:14<08:49, 10459.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   2%|▏         | 141054/5672672 [00:14<08:52, 10385.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   3%|▎         | 143054/5672672 [00:14<08:42, 10591.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   3%|▎         | 145054/5672672 [00:15<08:48, 10468.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   3%|▎         | 147054/5672672 [00:15<08:37, 10677.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (7/303 shards):   3%|▎         | 149054/5672672 [00:15<08:33, 10763.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 149776/5672672 [00:15<08:33, 10763.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 150776/5672672 [00:15<08:37, 10668.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 152776/5672672 [00:15<08:50, 10413.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 154776/5672672 [00:16<09:29, 9689.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 156776/5672672 [00:16<09:17, 9898.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 158776/5672672 [00:16<09:09, 10040.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 160776/5672672 [00:16<08:53, 10338.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 162776/5672672 [00:16<08:43, 10515.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 164776/5672672 [00:16<08:36, 10669.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 166776/5672672 [00:17<08:31, 10757.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (8/303 shards):   3%|▎         | 168498/5672672 [00:17<08:26, 10864.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 168498/5672672 [00:17<08:26, 10864.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 170498/5672672 [00:17<08:25, 10895.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 172498/5672672 [00:17<08:46, 10454.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 174498/5672672 [00:17<08:32, 10726.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 176498/5672672 [00:18<08:23, 10911.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 178498/5672672 [00:18<11:56, 7672.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 180498/5672672 [00:18<10:39, 8591.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 182498/5672672 [00:18<10:01, 9134.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 184498/5672672 [00:19<09:32, 9593.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (9/303 shards):   3%|▎         | 186498/5672672 [00:19<09:10, 9960.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   3%|▎         | 187220/5672672 [00:19<09:10, 9960.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   3%|▎         | 188220/5672672 [00:19<09:10, 9971.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   3%|▎         | 190220/5672672 [00:19<08:47, 10393.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   3%|▎         | 192220/5672672 [00:19<09:21, 9759.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   3%|▎         | 194220/5672672 [00:19<09:07, 10000.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   3%|▎         | 196220/5672672 [00:20<08:49, 10348.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   3%|▎         | 198220/5672672 [00:20<08:41, 10507.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   4%|▎         | 200220/5672672 [00:20<08:26, 10804.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   4%|▎         | 202220/5672672 [00:20<08:23, 10858.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   4%|▎         | 204220/5672672 [00:20<08:22, 10883.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (10/303 shards):   4%|▎         | 205942/5672672 [00:21<08:17, 10984.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▎         | 205942/5672672 [00:21<08:17, 10984.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▎         | 207942/5672672 [00:21<08:30, 10705.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▎         | 209942/5672672 [00:21<09:56, 9155.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▎         | 211942/5672672 [00:21<09:17, 9788.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▍         | 213942/5672672 [00:21<08:53, 10231.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▍         | 215942/5672672 [00:22<08:38, 10518.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▍         | 217942/5672672 [00:22<08:26, 10763.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▍         | 219942/5672672 [00:22<08:16, 10978.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▍         | 221942/5672672 [00:22<08:08, 11165.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (11/303 shards):   4%|▍         | 223942/5672672 [00:22<08:59, 10100.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 224664/5672672 [00:22<08:59, 10100.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 225664/5672672 [00:22<08:46, 10337.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 227664/5672672 [00:23<08:28, 10711.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 229664/5672672 [00:23<08:42, 10414.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 231664/5672672 [00:23<08:34, 10585.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 233664/5672672 [00:23<08:22, 10818.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 235664/5672672 [00:23<08:13, 11007.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 237664/5672672 [00:24<13:40, 6620.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 239664/5672672 [00:24<12:06, 7480.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 241664/5672672 [00:24<10:51, 8330.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (12/303 shards):   4%|▍         | 243386/5672672 [00:24<10:15, 8816.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   4%|▍         | 243386/5672672 [00:24<10:15, 8816.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   4%|▍         | 245386/5672672 [00:25<09:45, 9265.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   4%|▍         | 247386/5672672 [00:25<09:37, 9398.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   4%|▍         | 249386/5672672 [00:25<10:24, 8689.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   4%|▍         | 251386/5672672 [00:25<09:48, 9213.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   4%|▍         | 253386/5672672 [00:26<09:22, 9640.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   5%|▍         | 255386/5672672 [00:26<09:00, 10021.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   5%|▍         | 257386/5672672 [00:26<08:40, 10401.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   5%|▍         | 259386/5672672 [00:26<08:25, 10701.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (13/303 shards):   5%|▍         | 261386/5672672 [00:26<08:13, 10973.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 262108/5672672 [00:26<08:13, 10973.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 263108/5672672 [00:26<08:34, 10521.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 265108/5672672 [00:27<08:20, 10798.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 267108/5672672 [00:27<08:38, 10420.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 269108/5672672 [00:27<08:26, 10659.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 271108/5672672 [00:27<08:18, 10834.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 273108/5672672 [00:27<08:12, 10957.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 275108/5672672 [00:27<08:11, 10972.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 277108/5672672 [00:28<08:06, 11090.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 279108/5672672 [00:28<08:10, 10989.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (14/303 shards):   5%|▍         | 280830/5672672 [00:28<08:49, 10190.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▍         | 280830/5672672 [00:28<08:49, 10190.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▍         | 282830/5672672 [00:28<08:32, 10511.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▌         | 284830/5672672 [00:28<08:45, 10246.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▌         | 286830/5672672 [00:29<08:28, 10601.99 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▌         | 288830/5672672 [00:29<08:17, 10829.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▌         | 290830/5672672 [00:29<08:05, 11092.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▌         | 292830/5672672 [00:29<07:58, 11240.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▌         | 294830/5672672 [00:29<07:55, 11309.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▌         | 296830/5672672 [00:30<12:42, 7052.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (15/303 shards):   5%|▌         | 298830/5672672 [00:30<11:15, 7953.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   5%|▌         | 299552/5672672 [00:30<11:15, 7953.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   5%|▌         | 300552/5672672 [00:30<11:29, 7787.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   5%|▌         | 302552/5672672 [00:30<10:27, 8559.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   5%|▌         | 303552/5672672 [00:31<10:47, 8287.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   5%|▌         | 304552/5672672 [00:31<10:34, 8455.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   5%|▌         | 306552/5672672 [00:31<09:39, 9259.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   5%|▌         | 308552/5672672 [00:31<08:59, 9948.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   5%|▌         | 310552/5672672 [00:31<08:38, 10336.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   6%|▌         | 312552/5672672 [00:31<08:24, 10627.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   6%|▌         | 314552/5672672 [00:32<08:14, 10832.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   6%|▌         | 316552/5672672 [00:32<08:06, 11014.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (16/303 shards):   6%|▌         | 318274/5672672 [00:32<08:02, 11090.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 318274/5672672 [00:32<08:02, 11090.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 320274/5672672 [00:32<08:42, 10242.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 322274/5672672 [00:32<08:52, 10044.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 324274/5672672 [00:33<08:37, 10342.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 326274/5672672 [00:33<08:51, 10055.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 328274/5672672 [00:33<08:31, 10446.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 330274/5672672 [00:33<08:17, 10746.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 332274/5672672 [00:33<08:05, 10999.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 334274/5672672 [00:33<08:06, 10968.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (17/303 shards):   6%|▌         | 336274/5672672 [00:34<08:07, 10957.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 336996/5672672 [00:34<08:06, 10957.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 337996/5672672 [00:34<08:14, 10795.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 339996/5672672 [00:34<08:02, 11056.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 341996/5672672 [00:34<08:18, 10697.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 343996/5672672 [00:34<08:04, 10991.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 345996/5672672 [00:35<08:12, 10823.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 347996/5672672 [00:35<08:02, 11038.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 349996/5672672 [00:35<07:52, 11259.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 351996/5672672 [00:35<07:52, 11260.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▌         | 353996/5672672 [00:35<07:47, 11370.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (18/303 shards):   6%|▋         | 355718/5672672 [00:36<12:08, 7296.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   6%|▋         | 355718/5672672 [00:36<12:08, 7296.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   6%|▋         | 357718/5672672 [00:36<10:51, 8156.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   6%|▋         | 359718/5672672 [00:36<10:25, 8487.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   6%|▋         | 361718/5672672 [00:36<09:36, 9215.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   6%|▋         | 363718/5672672 [00:36<09:10, 9646.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   6%|▋         | 365718/5672672 [00:37<08:53, 9945.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   6%|▋         | 367718/5672672 [00:37<08:43, 10143.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   7%|▋         | 369718/5672672 [00:37<08:26, 10474.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   7%|▋         | 371718/5672672 [00:37<08:07, 10882.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (19/303 shards):   7%|▋         | 373718/5672672 [00:37<08:02, 10989.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 374440/5672672 [00:37<08:02, 10989.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 375440/5672672 [00:37<08:03, 10957.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 377440/5672672 [00:38<08:19, 10607.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 379440/5672672 [00:38<08:34, 10280.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 381440/5672672 [00:38<08:18, 10622.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 383440/5672672 [00:38<08:57, 9841.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 385440/5672672 [00:38<08:35, 10259.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 387440/5672672 [00:39<14:08, 6228.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 389440/5672672 [00:39<12:14, 7192.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 391440/5672672 [00:39<10:58, 8021.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (20/303 shards):   7%|▋         | 393162/5672672 [00:40<10:08, 8682.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 393162/5672672 [00:40<10:08, 8682.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 395162/5672672 [00:40<10:00, 8795.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 397162/5672672 [00:40<09:38, 9118.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 398162/5672672 [00:40<10:58, 8005.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 400162/5672672 [00:40<09:59, 8790.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 402162/5672672 [00:41<09:15, 9483.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 404162/5672672 [00:41<08:47, 9992.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 406162/5672672 [00:41<08:27, 10385.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 408162/5672672 [00:41<08:11, 10706.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 410162/5672672 [00:41<07:57, 11032.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (21/303 shards):   7%|▋         | 411884/5672672 [00:41<07:49, 11196.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   7%|▋         | 411884/5672672 [00:41<07:49, 11196.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   7%|▋         | 413884/5672672 [00:42<12:35, 6963.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   7%|▋         | 415884/5672672 [00:42<11:39, 7517.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   7%|▋         | 417884/5672672 [00:42<10:26, 8386.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   7%|▋         | 419884/5672672 [00:43<09:37, 9089.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   7%|▋         | 421884/5672672 [00:43<09:04, 9643.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   7%|▋         | 423884/5672672 [00:43<08:38, 10123.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   8%|▊         | 425884/5672672 [00:43<08:17, 10547.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   8%|▊         | 427884/5672672 [00:43<08:12, 10650.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (22/303 shards):   8%|▊         | 429884/5672672 [00:43<08:07, 10756.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 430606/5672672 [00:43<08:07, 10756.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 431606/5672672 [00:44<09:14, 9459.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 433606/5672672 [00:44<08:53, 9825.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 435606/5672672 [00:44<09:43, 8979.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 437606/5672672 [00:44<09:11, 9499.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 439606/5672672 [00:44<08:45, 9960.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 441606/5672672 [00:45<08:23, 10393.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 443606/5672672 [00:45<08:07, 10735.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 445606/5672672 [00:45<07:54, 11008.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 447606/5672672 [00:45<07:52, 11069.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (23/303 shards):   8%|▊         | 449328/5672672 [00:45<07:49, 11132.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 449328/5672672 [00:45<07:49, 11132.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 451328/5672672 [00:46<08:31, 10215.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 453328/5672672 [00:46<10:01, 8675.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 454328/5672672 [00:46<10:05, 8624.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 456328/5672672 [00:46<09:16, 9376.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 458328/5672672 [00:46<08:54, 9763.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 460328/5672672 [00:47<08:42, 9971.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 462328/5672672 [00:47<08:18, 10450.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 464328/5672672 [00:47<08:04, 10760.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 466328/5672672 [00:47<07:53, 11004.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (24/303 shards):   8%|▊         | 468050/5672672 [00:47<07:52, 11016.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   8%|▊         | 468050/5672672 [00:47<07:52, 11016.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   8%|▊         | 470050/5672672 [00:47<08:11, 10585.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   8%|▊         | 472050/5672672 [00:48<09:34, 9050.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   8%|▊         | 473050/5672672 [00:48<15:31, 5582.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   8%|▊         | 475050/5672672 [00:48<12:52, 6730.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   8%|▊         | 477050/5672672 [00:49<12:17, 7040.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   8%|▊         | 479050/5672672 [00:49<10:50, 7989.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   8%|▊         | 481050/5672672 [00:49<09:44, 8880.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   9%|▊         | 483050/5672672 [00:49<09:00, 9609.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   9%|▊         | 485050/5672672 [00:49<08:45, 9871.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (25/303 shards):   9%|▊         | 486772/5672672 [00:49<08:28, 10206.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▊         | 486772/5672672 [00:49<08:28, 10206.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▊         | 488772/5672672 [00:50<08:24, 10275.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▊         | 490772/5672672 [00:50<09:14, 9346.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▊         | 492772/5672672 [00:50<08:43, 9902.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▊         | 494772/5672672 [00:50<08:28, 10172.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▉         | 496772/5672672 [00:50<08:10, 10553.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▉         | 498772/5672672 [00:51<07:55, 10890.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▉         | 500772/5672672 [00:51<07:50, 10984.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▉         | 502772/5672672 [00:51<07:45, 11094.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (26/303 shards):   9%|▉         | 504772/5672672 [00:51<07:37, 11284.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 505494/5672672 [00:51<07:37, 11284.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 506494/5672672 [00:51<07:38, 11274.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 508494/5672672 [00:51<07:39, 11249.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 510494/5672672 [00:52<08:09, 10552.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 512494/5672672 [00:52<07:57, 10805.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 514494/5672672 [00:52<07:49, 10989.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 516494/5672672 [00:52<07:47, 11030.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 518494/5672672 [00:52<07:46, 11054.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 520494/5672672 [00:53<08:37, 9964.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 522494/5672672 [00:53<08:26, 10167.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (27/303 shards):   9%|▉         | 524216/5672672 [00:53<08:16, 10370.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):   9%|▉         | 524216/5672672 [00:53<08:16, 10370.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):   9%|▉         | 526216/5672672 [00:53<08:30, 10077.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):   9%|▉         | 528216/5672672 [00:53<08:40, 9888.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):   9%|▉         | 530216/5672672 [00:54<08:20, 10271.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):   9%|▉         | 532216/5672672 [00:54<11:08, 7686.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):   9%|▉         | 534216/5672672 [00:54<09:53, 8655.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):   9%|▉         | 536216/5672672 [00:54<09:11, 9310.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):   9%|▉         | 538216/5672672 [00:55<08:35, 9950.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):  10%|▉         | 540216/5672672 [00:55<08:17, 10314.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (28/303 shards):  10%|▉         | 542216/5672672 [00:55<07:59, 10702.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 542938/5672672 [00:55<07:59, 10702.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 543938/5672672 [00:55<07:58, 10711.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 545938/5672672 [00:55<07:52, 10850.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 547938/5672672 [00:55<08:06, 10534.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 549938/5672672 [00:56<07:53, 10829.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 551938/5672672 [00:56<07:49, 10899.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 553938/5672672 [00:56<07:52, 10830.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 555938/5672672 [00:56<07:51, 10851.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 557938/5672672 [00:56<07:50, 10877.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 559938/5672672 [00:57<08:38, 9863.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (29/303 shards):  10%|▉         | 561660/5672672 [00:57<08:30, 10017.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|▉         | 561660/5672672 [00:57<08:30, 10017.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|▉         | 563660/5672672 [00:57<08:26, 10090.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|▉         | 565660/5672672 [00:57<08:41, 9784.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|█         | 567660/5672672 [00:57<08:25, 10097.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|█         | 569660/5672672 [00:58<08:25, 10099.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|█         | 571660/5672672 [00:58<08:12, 10348.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|█         | 573660/5672672 [00:58<08:04, 10518.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|█         | 575660/5672672 [00:58<08:16, 10262.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|█         | 577660/5672672 [00:58<08:09, 10403.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (30/303 shards):  10%|█         | 579660/5672672 [00:58<07:54, 10726.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 580382/5672672 [00:59<07:54, 10726.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 581382/5672672 [00:59<07:53, 10743.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 583382/5672672 [00:59<07:43, 10984.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 585382/5672672 [00:59<08:00, 10580.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 587382/5672672 [00:59<07:45, 10915.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 589382/5672672 [00:59<07:36, 11133.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 591382/5672672 [01:00<11:49, 7160.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 593382/5672672 [01:00<10:25, 8116.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  10%|█         | 594382/5672672 [01:00<10:28, 8083.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  11%|█         | 596382/5672672 [01:00<09:31, 8887.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (31/303 shards):  11%|█         | 598382/5672672 [01:00<08:51, 9551.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 599104/5672672 [01:01<08:51, 9551.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 600104/5672672 [01:01<08:47, 9611.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 602104/5672672 [01:01<08:32, 9892.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 604104/5672672 [01:01<10:06, 8357.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 606104/5672672 [01:01<09:20, 9036.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 608104/5672672 [01:02<08:48, 9589.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 610104/5672672 [01:02<08:24, 10042.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 612104/5672672 [01:02<08:51, 9525.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 614104/5672672 [01:02<08:26, 9981.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 616104/5672672 [01:02<08:06, 10396.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (32/303 shards):  11%|█         | 617826/5672672 [01:02<07:52, 10690.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 617826/5672672 [01:02<07:52, 10690.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 619826/5672672 [01:03<08:15, 10194.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 621826/5672672 [01:03<09:07, 9217.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 623826/5672672 [01:03<08:37, 9763.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 625826/5672672 [01:03<08:18, 10119.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 627826/5672672 [01:03<07:59, 10526.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 629826/5672672 [01:04<07:46, 10817.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 631826/5672672 [01:04<07:36, 11045.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 633826/5672672 [01:04<07:28, 11231.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (33/303 shards):  11%|█         | 635826/5672672 [01:04<07:24, 11334.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█         | 636548/5672672 [01:04<07:24, 11334.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█         | 637548/5672672 [01:04<07:26, 11285.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█▏        | 639548/5672672 [01:04<07:19, 11447.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█▏        | 641548/5672672 [01:05<09:43, 8619.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█▏        | 643548/5672672 [01:05<09:05, 9214.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█▏        | 645548/5672672 [01:05<08:35, 9754.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█▏        | 647548/5672672 [01:05<08:21, 10015.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█▏        | 649548/5672672 [01:06<12:31, 6687.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  11%|█▏        | 651548/5672672 [01:06<10:56, 7651.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  12%|█▏        | 653548/5672672 [01:06<10:05, 8282.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (34/303 shards):  12%|█▏        | 655270/5672672 [01:06<09:25, 8864.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 655270/5672672 [01:06<09:25, 8864.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 657270/5672672 [01:07<09:03, 9220.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 659270/5672672 [01:07<10:05, 8285.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 661270/5672672 [01:07<09:14, 9040.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 663270/5672672 [01:07<08:38, 9661.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 665270/5672672 [01:07<08:23, 9945.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 667270/5672672 [01:08<08:06, 10286.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 669270/5672672 [01:08<07:52, 10590.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 671270/5672672 [01:08<07:41, 10843.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (35/303 shards):  12%|█▏        | 673270/5672672 [01:08<07:30, 11101.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 673992/5672672 [01:08<07:30, 11101.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 674992/5672672 [01:08<07:34, 11002.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 676992/5672672 [01:09<07:26, 11188.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 678992/5672672 [01:09<07:46, 10706.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 680992/5672672 [01:09<07:35, 10956.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 682992/5672672 [01:09<07:25, 11202.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 684992/5672672 [01:09<07:22, 11283.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 686992/5672672 [01:09<07:18, 11375.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 688992/5672672 [01:10<07:16, 11413.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 690992/5672672 [01:10<07:31, 11030.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (36/303 shards):  12%|█▏        | 692714/5672672 [01:10<07:32, 11004.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 692714/5672672 [01:10<07:32, 11004.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 694714/5672672 [01:10<07:46, 10672.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 696714/5672672 [01:10<08:26, 9817.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 698714/5672672 [01:11<08:31, 9725.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 700714/5672672 [01:11<08:11, 10107.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 702714/5672672 [01:11<07:54, 10479.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 704714/5672672 [01:11<07:39, 10809.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 706714/5672672 [01:11<09:06, 9084.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  12%|█▏        | 708714/5672672 [01:12<14:16, 5798.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (37/303 shards):  13%|█▎        | 710714/5672672 [01:12<12:09, 6800.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 711436/5672672 [01:12<12:09, 6800.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 712436/5672672 [01:12<11:07, 7433.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 714436/5672672 [01:13<09:58, 8280.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 715436/5672672 [01:13<09:56, 8314.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 716436/5672672 [01:13<10:02, 8223.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 718436/5672672 [01:13<08:56, 9237.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 720436/5672672 [01:13<08:21, 9881.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 722436/5672672 [01:13<07:58, 10343.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 724436/5672672 [01:14<07:43, 10672.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 726436/5672672 [01:14<07:30, 10987.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 728436/5672672 [01:14<07:24, 11120.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (38/303 shards):  13%|█▎        | 730158/5672672 [01:14<07:19, 11251.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 730158/5672672 [01:14<07:19, 11251.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 732158/5672672 [01:14<07:30, 10970.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 734158/5672672 [01:14<08:00, 10272.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 736158/5672672 [01:15<07:44, 10634.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 738158/5672672 [01:15<07:31, 10923.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 740158/5672672 [01:15<07:28, 10987.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 742158/5672672 [01:15<07:24, 11100.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 744158/5672672 [01:15<07:19, 11202.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 746158/5672672 [01:15<07:17, 11270.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (39/303 shards):  13%|█▎        | 748158/5672672 [01:16<09:44, 8423.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 748880/5672672 [01:16<09:44, 8423.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 749880/5672672 [01:16<09:23, 8735.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 751880/5672672 [01:16<08:41, 9431.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 753880/5672672 [01:16<09:00, 9104.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 755880/5672672 [01:17<08:21, 9805.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 757880/5672672 [01:17<08:04, 10135.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 759880/5672672 [01:17<07:46, 10541.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 761880/5672672 [01:17<07:37, 10741.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  13%|█▎        | 763880/5672672 [01:17<07:26, 11002.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  14%|█▎        | 765880/5672672 [01:17<07:17, 11225.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (40/303 shards):  14%|█▎        | 767602/5672672 [01:18<07:32, 10839.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▎        | 767602/5672672 [01:18<07:32, 10839.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▎        | 769602/5672672 [01:18<07:29, 10896.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▎        | 771602/5672672 [01:18<07:40, 10650.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▎        | 773602/5672672 [01:18<07:35, 10751.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▎        | 775602/5672672 [01:18<07:25, 10989.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▎        | 777602/5672672 [01:19<07:19, 11138.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▎        | 779602/5672672 [01:19<07:11, 11332.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▍        | 781602/5672672 [01:19<07:14, 11257.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▍        | 783602/5672672 [01:19<07:20, 11100.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (41/303 shards):  14%|█▍        | 785602/5672672 [01:19<07:20, 11102.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 786324/5672672 [01:19<07:20, 11102.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 787324/5672672 [01:19<07:37, 10685.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 789324/5672672 [01:20<07:37, 10673.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 791324/5672672 [01:20<08:01, 10134.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 793324/5672672 [01:20<07:49, 10392.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 795324/5672672 [01:20<07:34, 10732.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 797324/5672672 [01:20<07:31, 10809.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 799324/5672672 [01:21<07:33, 10740.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 801324/5672672 [01:21<07:27, 10897.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 803324/5672672 [01:21<07:19, 11080.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (42/303 shards):  14%|█▍        | 805046/5672672 [01:21<07:15, 11179.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 805046/5672672 [01:21<07:15, 11179.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 807046/5672672 [01:21<07:23, 10965.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 809046/5672672 [01:21<07:45, 10439.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 811046/5672672 [01:22<07:39, 10575.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 813046/5672672 [01:22<07:26, 10877.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 815046/5672672 [01:22<07:25, 10908.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 817046/5672672 [01:22<07:17, 11110.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 819046/5672672 [01:22<07:10, 11286.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  14%|█▍        | 821046/5672672 [01:23<07:02, 11484.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (43/303 shards):  15%|█▍        | 823046/5672672 [01:23<07:01, 11505.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 823768/5672672 [01:23<07:01, 11505.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 824768/5672672 [01:23<07:07, 11337.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 826768/5672672 [01:23<11:15, 7170.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 827768/5672672 [01:24<11:26, 7056.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 828768/5672672 [01:24<12:30, 6457.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 829768/5672672 [01:24<11:51, 6807.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 831768/5672672 [01:24<10:04, 8002.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 833768/5672672 [01:24<09:06, 8860.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 835768/5672672 [01:24<08:37, 9345.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 837768/5672672 [01:25<08:14, 9775.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 839768/5672672 [01:25<07:53, 10201.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (44/303 shards):  15%|█▍        | 841768/5672672 [01:25<07:38, 10528.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▍        | 842490/5672672 [01:25<07:38, 10528.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▍        | 843490/5672672 [01:25<09:16, 8683.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▍        | 845490/5672672 [01:25<08:36, 9353.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▍        | 846490/5672672 [01:26<08:49, 9108.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▍        | 848490/5672672 [01:26<08:18, 9678.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▍        | 850490/5672672 [01:26<07:57, 10088.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▌        | 852490/5672672 [01:26<07:44, 10375.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▌        | 854490/5672672 [01:26<07:30, 10689.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▌        | 856490/5672672 [01:26<07:26, 10786.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▌        | 858490/5672672 [01:27<07:11, 11145.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (45/303 shards):  15%|█▌        | 860490/5672672 [01:27<07:10, 11188.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 861212/5672672 [01:27<07:10, 11188.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 862212/5672672 [01:27<07:41, 10416.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 864212/5672672 [01:27<07:25, 10782.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 866212/5672672 [01:27<07:44, 10349.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 868212/5672672 [01:28<07:27, 10741.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 870212/5672672 [01:28<07:31, 10637.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 872212/5672672 [01:28<07:23, 10812.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 874212/5672672 [01:28<07:17, 10977.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 876212/5672672 [01:28<07:15, 11025.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  15%|█▌        | 878212/5672672 [01:28<07:20, 10893.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (46/303 shards):  16%|█▌        | 879934/5672672 [01:29<07:15, 11012.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 879934/5672672 [01:29<07:15, 11012.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 881934/5672672 [01:29<07:15, 10993.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 883934/5672672 [01:29<07:32, 10588.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 885934/5672672 [01:30<12:01, 6637.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 887934/5672672 [01:30<10:40, 7469.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 889934/5672672 [01:30<09:33, 8343.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 891934/5672672 [01:30<08:44, 9120.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 893934/5672672 [01:30<08:11, 9721.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 895934/5672672 [01:30<07:56, 10031.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (47/303 shards):  16%|█▌        | 897934/5672672 [01:31<07:37, 10445.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 898656/5672672 [01:31<07:37, 10445.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 899656/5672672 [01:31<07:49, 10155.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 901656/5672672 [01:31<07:31, 10572.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 903656/5672672 [01:31<07:40, 10362.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 905656/5672672 [01:31<07:25, 10691.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 907656/5672672 [01:32<07:15, 10929.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 909656/5672672 [01:32<07:09, 11077.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 911656/5672672 [01:32<07:03, 11252.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 913656/5672672 [01:32<07:00, 11305.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 915656/5672672 [01:32<06:57, 11384.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (48/303 shards):  16%|█▌        | 917378/5672672 [01:32<07:04, 11190.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▌        | 917378/5672672 [01:32<07:04, 11190.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▌        | 919378/5672672 [01:33<07:06, 11144.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▌        | 921378/5672672 [01:33<07:28, 10587.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▋        | 923378/5672672 [01:33<07:28, 10591.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▋        | 925378/5672672 [01:33<08:02, 9848.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▋        | 927378/5672672 [01:33<07:59, 9894.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▋        | 929378/5672672 [01:34<07:39, 10312.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▋        | 931378/5672672 [01:34<07:30, 10525.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▋        | 933378/5672672 [01:34<07:22, 10715.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (49/303 shards):  16%|█▋        | 935378/5672672 [01:34<07:12, 10960.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 936100/5672672 [01:34<07:12, 10960.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 937100/5672672 [01:34<07:29, 10535.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 939100/5672672 [01:34<07:20, 10742.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 941100/5672672 [01:35<08:08, 9676.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 943100/5672672 [01:35<07:55, 9937.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 945100/5672672 [01:35<11:38, 6766.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 947100/5672672 [01:36<10:08, 7760.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 949100/5672672 [01:36<09:07, 8634.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 951100/5672672 [01:36<08:26, 9330.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 953100/5672672 [01:36<07:56, 9911.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (50/303 shards):  17%|█▋        | 954822/5672672 [01:36<07:39, 10265.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 954822/5672672 [01:36<07:39, 10265.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 956822/5672672 [01:36<07:27, 10536.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 958822/5672672 [01:37<07:40, 10225.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 960822/5672672 [01:37<07:24, 10598.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 962822/5672672 [01:37<07:14, 10841.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 964822/5672672 [01:37<07:18, 10742.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 966822/5672672 [01:37<07:19, 10701.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 968822/5672672 [01:38<07:20, 10670.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 970822/5672672 [01:38<07:16, 10763.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (51/303 shards):  17%|█▋        | 972822/5672672 [01:38<07:24, 10567.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 973544/5672672 [01:38<07:24, 10567.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 974544/5672672 [01:38<07:55, 9877.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 976544/5672672 [01:38<07:45, 10098.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 978544/5672672 [01:39<09:00, 8688.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 979544/5672672 [01:39<08:51, 8832.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 981544/5672672 [01:39<08:09, 9585.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 982544/5672672 [01:39<08:06, 9639.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 984544/5672672 [01:39<07:47, 10020.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 986544/5672672 [01:39<07:31, 10383.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 988544/5672672 [01:40<07:17, 10697.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 990544/5672672 [01:40<07:09, 10905.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (52/303 shards):  17%|█▋        | 992266/5672672 [01:40<07:05, 11007.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  17%|█▋        | 992266/5672672 [01:40<07:05, 11007.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 994266/5672672 [01:40<06:59, 11150.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 996266/5672672 [01:40<07:16, 10709.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 998266/5672672 [01:40<07:04, 10999.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 1000266/5672672 [01:41<06:59, 11136.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 1002266/5672672 [01:41<06:58, 11172.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 1004266/5672672 [01:41<10:40, 7291.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 1006266/5672672 [01:41<09:31, 8160.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 1008266/5672672 [01:42<08:40, 8962.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (53/303 shards):  18%|█▊        | 1010266/5672672 [01:42<08:06, 9583.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1010988/5672672 [01:42<08:06, 9583.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1011988/5672672 [01:42<08:10, 9494.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1013988/5672672 [01:42<07:48, 9936.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1015988/5672672 [01:42<09:27, 8205.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1017988/5672672 [01:43<08:45, 8857.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1018988/5672672 [01:43<08:59, 8620.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1019988/5672672 [01:43<08:46, 8842.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1021988/5672672 [01:43<08:04, 9589.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1023988/5672672 [01:43<07:45, 9984.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1025988/5672672 [01:43<07:23, 10484.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1027988/5672672 [01:44<07:19, 10573.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (54/303 shards):  18%|█▊        | 1029710/5672672 [01:44<07:10, 10796.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1029710/5672672 [01:44<07:10, 10796.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1031710/5672672 [01:44<07:08, 10818.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1033710/5672672 [01:44<07:28, 10347.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1035710/5672672 [01:44<07:15, 10643.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1037710/5672672 [01:45<07:08, 10812.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1039710/5672672 [01:45<06:58, 11075.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1041710/5672672 [01:45<06:57, 11099.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1043710/5672672 [01:45<06:51, 11253.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1045710/5672672 [01:45<06:49, 11287.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (55/303 shards):  18%|█▊        | 1047710/5672672 [01:45<06:47, 11345.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  18%|█▊        | 1048432/5672672 [01:45<06:47, 11345.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  18%|█▊        | 1049432/5672672 [01:46<06:55, 11134.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▊        | 1051432/5672672 [01:46<06:50, 11269.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▊        | 1053432/5672672 [01:46<08:25, 9141.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▊        | 1055432/5672672 [01:46<07:54, 9723.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▊        | 1057432/5672672 [01:46<07:29, 10276.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▊        | 1059432/5672672 [01:47<07:10, 10712.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▊        | 1061432/5672672 [01:47<07:01, 10945.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▊        | 1063432/5672672 [01:47<09:59, 7682.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▉        | 1065432/5672672 [01:47<08:55, 8597.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (56/303 shards):  19%|█▉        | 1067154/5672672 [01:48<08:24, 9131.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1067154/5672672 [01:48<08:24, 9131.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1069154/5672672 [01:48<08:33, 8957.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1070154/5672672 [01:48<08:27, 9070.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1071154/5672672 [01:48<08:44, 8769.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1073154/5672672 [01:48<08:00, 9578.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1075154/5672672 [01:48<07:31, 10174.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1077154/5672672 [01:48<07:15, 10550.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1079154/5672672 [01:49<07:06, 10759.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1081154/5672672 [01:49<06:59, 10940.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1083154/5672672 [01:49<07:03, 10847.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (57/303 shards):  19%|█▉        | 1085154/5672672 [01:49<07:23, 10350.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1085876/5672672 [01:49<07:23, 10350.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1086876/5672672 [01:49<07:24, 10317.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1088876/5672672 [01:50<07:15, 10536.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1090876/5672672 [01:50<07:36, 10028.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1092876/5672672 [01:50<07:21, 10380.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1094876/5672672 [01:50<07:11, 10610.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1096876/5672672 [01:50<07:01, 10854.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1098876/5672672 [01:51<06:56, 10970.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1100876/5672672 [01:51<06:53, 11066.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1102876/5672672 [01:51<06:48, 11174.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (58/303 shards):  19%|█▉        | 1104598/5672672 [01:51<06:55, 10993.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  19%|█▉        | 1104598/5672672 [01:51<06:55, 10993.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1106598/5672672 [01:51<07:27, 10196.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1108598/5672672 [01:51<07:34, 10038.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1110598/5672672 [01:52<07:31, 10111.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1112598/5672672 [01:52<07:18, 10400.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1114598/5672672 [01:52<07:09, 10624.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1116598/5672672 [01:52<07:10, 10575.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1118598/5672672 [01:52<07:02, 10780.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1120598/5672672 [01:53<06:56, 10919.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (59/303 shards):  20%|█▉        | 1122598/5672672 [01:53<11:26, 6632.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|█▉        | 1123320/5672672 [01:53<11:25, 6632.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|█▉        | 1124320/5672672 [01:53<11:15, 6736.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|█▉        | 1126320/5672672 [01:54<10:06, 7493.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|█▉        | 1127320/5672672 [01:54<12:19, 6147.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|█▉        | 1128320/5672672 [01:54<11:53, 6371.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|█▉        | 1130320/5672672 [01:54<10:08, 7462.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|█▉        | 1132320/5672672 [01:54<09:00, 8394.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|█▉        | 1134320/5672672 [01:55<08:15, 9159.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|██        | 1136320/5672672 [01:55<07:42, 9804.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|██        | 1138320/5672672 [01:55<07:23, 10218.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|██        | 1140320/5672672 [01:55<07:11, 10511.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (60/303 shards):  20%|██        | 1142042/5672672 [01:55<07:02, 10726.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1142042/5672672 [01:55<07:02, 10726.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1144042/5672672 [01:55<07:10, 10510.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1146042/5672672 [01:56<07:29, 10068.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1148042/5672672 [01:56<07:25, 10156.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1150042/5672672 [01:56<07:17, 10328.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1152042/5672672 [01:56<07:04, 10659.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1154042/5672672 [01:56<06:59, 10760.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1156042/5672672 [01:57<07:04, 10639.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1158042/5672672 [01:57<07:13, 10413.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (61/303 shards):  20%|██        | 1160042/5672672 [01:57<06:58, 10781.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  20%|██        | 1160764/5672672 [01:57<06:58, 10781.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  20%|██        | 1161764/5672672 [01:57<06:56, 10831.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1163764/5672672 [01:57<06:48, 11033.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1165764/5672672 [01:58<08:32, 8790.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1167764/5672672 [01:58<08:02, 9338.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1169764/5672672 [01:58<07:44, 9687.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1171764/5672672 [01:58<07:24, 10134.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1173764/5672672 [01:58<07:09, 10484.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1175764/5672672 [01:59<07:01, 10666.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1177764/5672672 [01:59<06:54, 10853.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (62/303 shards):  21%|██        | 1179486/5672672 [01:59<06:47, 11017.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1179486/5672672 [01:59<06:47, 11017.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1181486/5672672 [01:59<10:09, 7365.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1183486/5672672 [02:00<09:26, 7922.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1185486/5672672 [02:00<08:33, 8730.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1187486/5672672 [02:00<08:00, 9343.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1189486/5672672 [02:00<07:35, 9848.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1191486/5672672 [02:00<07:28, 10000.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1193486/5672672 [02:00<07:26, 10034.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1195486/5672672 [02:01<07:10, 10388.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (63/303 shards):  21%|██        | 1197486/5672672 [02:01<06:59, 10678.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██        | 1198208/5672672 [02:01<06:59, 10678.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██        | 1199208/5672672 [02:01<06:56, 10730.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██        | 1201208/5672672 [02:01<06:50, 10893.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██        | 1203208/5672672 [02:02<08:45, 8499.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██        | 1205208/5672672 [02:02<08:09, 9119.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██▏       | 1207208/5672672 [02:02<07:44, 9618.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██▏       | 1209208/5672672 [02:02<07:29, 9925.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██▏       | 1211208/5672672 [02:02<08:04, 9209.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██▏       | 1213208/5672672 [02:03<07:41, 9666.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██▏       | 1215208/5672672 [02:03<07:27, 9971.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (64/303 shards):  21%|██▏       | 1216930/5672672 [02:03<07:16, 10200.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  21%|██▏       | 1216930/5672672 [02:03<07:16, 10200.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  21%|██▏       | 1218930/5672672 [02:03<07:20, 10104.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1220930/5672672 [02:03<08:30, 8723.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1221930/5672672 [02:04<09:21, 7921.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1223930/5672672 [02:04<08:30, 8722.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1225930/5672672 [02:04<07:57, 9309.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1227930/5672672 [02:04<07:37, 9723.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1229930/5672672 [02:04<07:22, 10037.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1231930/5672672 [02:04<07:09, 10328.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1233930/5672672 [02:05<07:01, 10536.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (65/303 shards):  22%|██▏       | 1235652/5672672 [02:05<06:52, 10754.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1235652/5672672 [02:05<06:52, 10754.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1237652/5672672 [02:05<07:20, 10071.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1239652/5672672 [02:05<08:38, 8548.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1240652/5672672 [02:06<11:22, 6496.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1242652/5672672 [02:06<09:50, 7498.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1244652/5672672 [02:06<08:58, 8229.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1245652/5672672 [02:06<08:40, 8500.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1247652/5672672 [02:06<07:58, 9242.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1249652/5672672 [02:06<07:32, 9780.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1251652/5672672 [02:07<07:11, 10256.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (66/303 shards):  22%|██▏       | 1253652/5672672 [02:07<07:03, 10437.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1254374/5672672 [02:07<07:03, 10437.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1255374/5672672 [02:07<07:11, 10230.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1257374/5672672 [02:07<07:03, 10434.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1259374/5672672 [02:08<10:12, 7200.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1261374/5672672 [02:08<09:09, 8023.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1263374/5672672 [02:08<08:27, 8685.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1265374/5672672 [02:08<08:00, 9173.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1267374/5672672 [02:08<07:35, 9666.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1269374/5672672 [02:09<07:22, 9953.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1271374/5672672 [02:09<07:02, 10413.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (67/303 shards):  22%|██▏       | 1273096/5672672 [02:09<06:49, 10742.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  22%|██▏       | 1273096/5672672 [02:09<06:49, 10742.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  22%|██▏       | 1275096/5672672 [02:09<06:54, 10599.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1277096/5672672 [02:09<07:40, 9537.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1278096/5672672 [02:10<07:56, 9230.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1280096/5672672 [02:10<07:36, 9616.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1282096/5672672 [02:10<07:21, 9950.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1284096/5672672 [02:10<07:04, 10331.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1286096/5672672 [02:10<06:51, 10651.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1288096/5672672 [02:10<07:03, 10341.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1290096/5672672 [02:11<06:58, 10464.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (68/303 shards):  23%|██▎       | 1291818/5672672 [02:11<07:17, 10012.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1291818/5672672 [02:11<07:17, 10012.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1293818/5672672 [02:11<07:10, 10163.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1295818/5672672 [02:11<09:18, 7841.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1296818/5672672 [02:11<09:02, 8062.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1298818/5672672 [02:12<08:18, 8772.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1299818/5672672 [02:12<12:49, 5684.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1301818/5672672 [02:12<10:39, 6832.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1302818/5672672 [02:12<10:21, 7034.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1304818/5672672 [02:13<09:00, 8075.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1306818/5672672 [02:13<08:07, 8957.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1308818/5672672 [02:13<07:36, 9561.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (69/303 shards):  23%|██▎       | 1310540/5672672 [02:13<07:15, 10023.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1310540/5672672 [02:13<07:15, 10023.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1312540/5672672 [02:13<07:27, 9736.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1314540/5672672 [02:14<08:10, 8878.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1316540/5672672 [02:14<07:45, 9352.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1318540/5672672 [02:14<07:28, 9706.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1320540/5672672 [02:14<07:14, 10017.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1322540/5672672 [02:14<07:01, 10322.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1324540/5672672 [02:15<06:53, 10525.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1326540/5672672 [02:15<06:54, 10476.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (70/303 shards):  23%|██▎       | 1328540/5672672 [02:15<07:00, 10336.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  23%|██▎       | 1329262/5672672 [02:15<07:00, 10336.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  23%|██▎       | 1330262/5672672 [02:15<06:59, 10342.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  23%|██▎       | 1332262/5672672 [02:15<06:51, 10540.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  24%|██▎       | 1334262/5672672 [02:16<07:15, 9973.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  24%|██▎       | 1336262/5672672 [02:16<07:11, 10038.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  24%|██▎       | 1338262/5672672 [02:16<07:14, 9985.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  24%|██▎       | 1340262/5672672 [02:16<07:12, 10014.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  24%|██▎       | 1342262/5672672 [02:16<07:07, 10131.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  24%|██▎       | 1344262/5672672 [02:16<06:57, 10361.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  24%|██▎       | 1346262/5672672 [02:17<06:42, 10753.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (71/303 shards):  24%|██▍       | 1347984/5672672 [02:17<06:37, 10867.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1347984/5672672 [02:17<06:37, 10867.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1349984/5672672 [02:17<06:47, 10598.99 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1351984/5672672 [02:17<07:18, 9852.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1353984/5672672 [02:17<07:10, 10028.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1355984/5672672 [02:18<07:03, 10203.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1357984/5672672 [02:18<06:56, 10363.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1359984/5672672 [02:18<10:22, 6930.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1361984/5672672 [02:18<09:06, 7887.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1363984/5672672 [02:19<08:17, 8661.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (72/303 shards):  24%|██▍       | 1365984/5672672 [02:19<07:48, 9196.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1366706/5672672 [02:19<07:48, 9196.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1367706/5672672 [02:19<08:21, 8589.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1368706/5672672 [02:19<08:08, 8805.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1369706/5672672 [02:19<07:57, 9016.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1370706/5672672 [02:19<08:14, 8698.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1371706/5672672 [02:20<08:01, 8937.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1373706/5672672 [02:20<07:38, 9371.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1375706/5672672 [02:20<07:27, 9608.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1376706/5672672 [02:20<07:45, 9238.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1378706/5672672 [02:20<08:10, 8746.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1380706/5672672 [02:20<07:39, 9336.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1382706/5672672 [02:21<07:55, 9028.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1383706/5672672 [02:21<07:51, 9087.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (73/303 shards):  24%|██▍       | 1385428/5672672 [02:21<07:24, 9649.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  24%|██▍       | 1385428/5672672 [02:21<07:24, 9649.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  24%|██▍       | 1386428/5672672 [02:21<07:28, 9547.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  24%|██▍       | 1388428/5672672 [02:21<07:02, 10142.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  25%|██▍       | 1390428/5672672 [02:22<08:21, 8537.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  25%|██▍       | 1392428/5672672 [02:22<07:49, 9108.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  25%|██▍       | 1394428/5672672 [02:22<07:32, 9464.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  25%|██▍       | 1396428/5672672 [02:22<07:14, 9833.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  25%|██▍       | 1398428/5672672 [02:22<06:59, 10198.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  25%|██▍       | 1400428/5672672 [02:22<06:44, 10572.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  25%|██▍       | 1402428/5672672 [02:23<06:33, 10859.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (74/303 shards):  25%|██▍       | 1404150/5672672 [02:23<06:27, 11016.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▍       | 1404150/5672672 [02:23<06:27, 11016.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▍       | 1406150/5672672 [02:23<07:14, 9808.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▍       | 1408150/5672672 [02:23<07:58, 8903.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▍       | 1410150/5672672 [02:24<07:35, 9362.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▍       | 1412150/5672672 [02:24<07:15, 9777.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▍       | 1414150/5672672 [02:24<07:02, 10083.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▍       | 1416150/5672672 [02:24<06:54, 10262.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▍       | 1418150/5672672 [02:25<11:18, 6273.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▌       | 1420150/5672672 [02:25<09:50, 7196.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (75/303 shards):  25%|██▌       | 1422150/5672672 [02:25<08:50, 8008.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1422872/5672672 [02:25<08:50, 8008.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1423872/5672672 [02:25<09:20, 7581.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1425872/5672672 [02:25<08:32, 8284.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1426872/5672672 [02:26<08:51, 7990.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1427872/5672672 [02:26<08:30, 8312.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1429872/5672672 [02:26<07:44, 9134.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1431872/5672672 [02:26<07:19, 9642.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1433872/5672672 [02:26<07:07, 9913.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1435872/5672672 [02:26<06:48, 10359.99 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1437872/5672672 [02:27<06:33, 10754.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1439872/5672672 [02:27<06:25, 10989.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (76/303 shards):  25%|██▌       | 1441594/5672672 [02:27<06:25, 10980.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  25%|██▌       | 1441594/5672672 [02:27<06:25, 10980.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  25%|██▌       | 1443594/5672672 [02:27<06:27, 10921.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  25%|██▌       | 1445594/5672672 [02:28<08:25, 8360.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  26%|██▌       | 1446594/5672672 [02:28<08:22, 8404.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  26%|██▌       | 1448594/5672672 [02:28<07:46, 9045.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  26%|██▌       | 1450594/5672672 [02:28<07:19, 9604.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  26%|██▌       | 1452594/5672672 [02:28<07:06, 9898.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  26%|██▌       | 1454594/5672672 [02:28<06:51, 10240.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  26%|██▌       | 1456594/5672672 [02:29<06:42, 10470.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  26%|██▌       | 1458594/5672672 [02:29<06:30, 10800.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (77/303 shards):  26%|██▌       | 1460316/5672672 [02:29<06:29, 10827.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1460316/5672672 [02:29<06:29, 10827.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1462316/5672672 [02:29<08:20, 8414.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1464316/5672672 [02:29<08:16, 8480.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1465316/5672672 [02:30<08:14, 8513.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1466316/5672672 [02:30<08:00, 8755.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1468316/5672672 [02:30<07:42, 9099.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1470316/5672672 [02:30<07:19, 9553.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1471316/5672672 [02:30<07:17, 9593.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1473316/5672672 [02:30<07:05, 9876.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1475316/5672672 [02:31<06:55, 10102.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1477316/5672672 [02:31<09:36, 7279.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (78/303 shards):  26%|██▌       | 1479038/5672672 [02:31<08:37, 8099.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▌       | 1479038/5672672 [02:31<08:37, 8099.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▌       | 1480038/5672672 [02:31<08:54, 7836.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▌       | 1482038/5672672 [02:31<08:02, 8678.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▌       | 1483038/5672672 [02:32<08:31, 8188.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▌       | 1485038/5672672 [02:32<07:44, 9007.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▌       | 1487038/5672672 [02:32<07:21, 9484.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▌       | 1489038/5672672 [02:32<07:06, 9805.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▋       | 1491038/5672672 [02:32<06:47, 10259.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▋       | 1493038/5672672 [02:33<06:34, 10606.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▋       | 1495038/5672672 [02:33<06:27, 10788.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (79/303 shards):  26%|██▋       | 1497038/5672672 [02:33<06:20, 10977.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  26%|██▋       | 1497760/5672672 [02:33<06:20, 10977.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  26%|██▋       | 1498760/5672672 [02:33<06:23, 10884.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  26%|██▋       | 1500760/5672672 [02:33<06:16, 11083.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  26%|██▋       | 1502760/5672672 [02:34<08:16, 8403.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  27%|██▋       | 1504760/5672672 [02:34<07:43, 8985.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  27%|██▋       | 1506760/5672672 [02:34<07:26, 9323.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  27%|██▋       | 1508760/5672672 [02:34<08:05, 8575.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  27%|██▋       | 1510760/5672672 [02:34<07:29, 9257.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  27%|██▋       | 1512760/5672672 [02:35<07:04, 9794.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  27%|██▋       | 1514760/5672672 [02:35<06:56, 9987.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (80/303 shards):  27%|██▋       | 1516482/5672672 [02:35<06:45, 10245.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1516482/5672672 [02:35<06:45, 10245.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1518482/5672672 [02:35<07:13, 9591.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1520482/5672672 [02:35<07:15, 9538.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1522482/5672672 [02:36<07:02, 9825.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1524482/5672672 [02:36<06:54, 10015.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1526482/5672672 [02:36<06:52, 10052.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1528482/5672672 [02:36<06:41, 10311.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1530482/5672672 [02:36<06:32, 10559.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1532482/5672672 [02:37<06:21, 10846.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (81/303 shards):  27%|██▋       | 1534482/5672672 [02:37<06:23, 10784.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1535204/5672672 [02:37<06:23, 10784.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1536204/5672672 [02:37<09:59, 6899.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1538204/5672672 [02:37<08:42, 7910.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1539204/5672672 [02:38<09:34, 7192.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1540204/5672672 [02:38<09:17, 7414.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1542204/5672672 [02:38<08:20, 8253.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1544204/5672672 [02:38<07:39, 8982.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1546204/5672672 [02:38<07:13, 9519.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1548204/5672672 [02:38<06:56, 9899.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1550204/5672672 [02:39<06:52, 10000.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1552204/5672672 [02:39<06:37, 10365.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (82/303 shards):  27%|██▋       | 1553926/5672672 [02:39<06:30, 10557.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  27%|██▋       | 1553926/5672672 [02:39<06:30, 10557.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  27%|██▋       | 1555926/5672672 [02:39<06:27, 10622.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  27%|██▋       | 1557926/5672672 [02:39<07:30, 9140.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  27%|██▋       | 1558926/5672672 [02:40<07:50, 8750.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  28%|██▊       | 1560926/5672672 [02:40<07:18, 9370.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  28%|██▊       | 1562926/5672672 [02:40<07:03, 9700.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  28%|██▊       | 1563926/5672672 [02:40<07:03, 9704.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  28%|██▊       | 1565926/5672672 [02:40<06:48, 10062.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  28%|██▊       | 1567926/5672672 [02:40<07:15, 9425.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  28%|██▊       | 1569926/5672672 [02:41<06:50, 10000.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (83/303 shards):  28%|██▊       | 1571926/5672672 [02:41<06:41, 10208.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1572648/5672672 [02:41<06:41, 10208.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1573648/5672672 [02:41<06:37, 10323.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1575648/5672672 [02:41<06:29, 10512.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1577648/5672672 [02:41<07:06, 9604.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1579648/5672672 [02:42<06:56, 9838.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1581648/5672672 [02:42<06:44, 10116.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1583648/5672672 [02:42<06:31, 10434.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1585648/5672672 [02:42<06:21, 10704.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1587648/5672672 [02:42<06:15, 10874.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1589648/5672672 [02:43<06:09, 11042.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (84/303 shards):  28%|██▊       | 1591370/5672672 [02:43<06:07, 11108.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1591370/5672672 [02:43<06:07, 11108.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1593370/5672672 [02:43<07:02, 9644.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1595370/5672672 [02:43<10:36, 6410.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1596370/5672672 [02:44<09:58, 6806.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1598370/5672672 [02:44<08:37, 7880.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1600370/5672672 [02:44<07:53, 8605.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1601370/5672672 [02:44<07:40, 8836.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1603370/5672672 [02:44<07:08, 9498.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1605370/5672672 [02:44<07:27, 9089.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1607370/5672672 [02:45<06:58, 9705.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (85/303 shards):  28%|██▊       | 1609370/5672672 [02:45<06:42, 10105.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  28%|██▊       | 1610092/5672672 [02:45<06:42, 10105.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  28%|██▊       | 1611092/5672672 [02:45<06:42, 10095.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  28%|██▊       | 1613092/5672672 [02:45<06:35, 10272.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  28%|██▊       | 1615092/5672672 [02:45<07:10, 9434.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  28%|██▊       | 1616092/5672672 [02:46<07:07, 9484.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  29%|██▊       | 1618092/5672672 [02:46<06:56, 9746.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  29%|██▊       | 1620092/5672672 [02:46<06:44, 10006.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  29%|██▊       | 1622092/5672672 [02:46<06:36, 10213.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  29%|██▊       | 1624092/5672672 [02:46<06:29, 10387.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  29%|██▊       | 1626092/5672672 [02:46<06:19, 10661.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (86/303 shards):  29%|██▊       | 1628092/5672672 [02:47<06:13, 10816.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▊       | 1628814/5672672 [02:47<06:13, 10816.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▊       | 1629814/5672672 [02:47<07:04, 9533.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1631814/5672672 [02:47<06:55, 9731.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1632814/5672672 [02:47<07:25, 9062.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1633814/5672672 [02:47<07:25, 9071.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1635814/5672672 [02:48<06:55, 9718.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1637814/5672672 [02:48<06:38, 10134.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1639814/5672672 [02:48<06:24, 10491.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1641814/5672672 [02:48<06:14, 10756.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1643814/5672672 [02:48<06:07, 10953.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1645814/5672672 [02:48<06:07, 10965.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (87/303 shards):  29%|██▉       | 1647536/5672672 [02:49<06:06, 10982.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1647536/5672672 [02:49<06:06, 10982.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1649536/5672672 [02:49<06:03, 11056.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1651536/5672672 [02:49<06:36, 10146.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1653536/5672672 [02:49<06:49, 9822.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1654536/5672672 [02:50<11:09, 5997.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1656536/5672672 [02:50<09:28, 7063.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1658536/5672672 [02:50<08:28, 7892.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1660536/5672672 [02:50<07:41, 8702.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1662536/5672672 [02:50<07:07, 9381.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1664536/5672672 [02:51<06:52, 9713.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (88/303 shards):  29%|██▉       | 1666258/5672672 [02:51<06:42, 9965.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  29%|██▉       | 1666258/5672672 [02:51<06:42, 9965.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  29%|██▉       | 1668258/5672672 [02:51<06:29, 10281.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  29%|██▉       | 1670258/5672672 [02:51<06:40, 10003.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  29%|██▉       | 1672258/5672672 [02:51<07:12, 9240.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  30%|██▉       | 1674258/5672672 [02:52<06:54, 9644.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  30%|██▉       | 1676258/5672672 [02:52<06:42, 9927.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  30%|██▉       | 1678258/5672672 [02:52<06:30, 10234.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  30%|██▉       | 1680258/5672672 [02:52<06:15, 10634.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  30%|██▉       | 1682258/5672672 [02:52<06:05, 10924.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (89/303 shards):  30%|██▉       | 1684258/5672672 [02:52<06:00, 11074.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1684980/5672672 [02:53<06:00, 11074.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1685980/5672672 [02:53<06:00, 11074.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1687980/5672672 [02:53<05:59, 11070.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1689980/5672672 [02:53<06:23, 10378.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1691980/5672672 [02:53<06:21, 10447.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1693980/5672672 [02:53<06:17, 10529.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1695980/5672672 [02:54<06:16, 10562.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1697980/5672672 [02:54<06:46, 9766.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|██▉       | 1699980/5672672 [02:54<06:32, 10110.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|███       | 1701980/5672672 [02:54<06:23, 10357.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (90/303 shards):  30%|███       | 1703702/5672672 [02:54<06:15, 10575.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1703702/5672672 [02:54<06:15, 10575.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1705702/5672672 [02:55<06:13, 10614.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1707702/5672672 [02:55<06:48, 9715.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1709702/5672672 [02:55<06:41, 9876.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1711702/5672672 [02:55<06:33, 10070.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1713702/5672672 [02:56<09:05, 7256.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1715702/5672672 [02:56<08:02, 8203.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1717702/5672672 [02:56<07:21, 8948.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1719702/5672672 [02:56<06:50, 9622.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (91/303 shards):  30%|███       | 1721702/5672672 [02:56<06:29, 10139.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  30%|███       | 1722424/5672672 [02:56<06:29, 10139.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  30%|███       | 1723424/5672672 [02:57<06:50, 9620.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  30%|███       | 1725424/5672672 [02:57<06:38, 9896.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  30%|███       | 1727424/5672672 [02:57<06:58, 9419.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  30%|███       | 1729424/5672672 [02:57<06:39, 9861.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  31%|███       | 1731424/5672672 [02:57<06:22, 10304.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  31%|███       | 1733424/5672672 [02:57<06:15, 10488.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  31%|███       | 1735424/5672672 [02:58<06:06, 10739.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  31%|███       | 1737424/5672672 [02:58<06:13, 10544.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  31%|███       | 1739424/5672672 [02:58<06:08, 10686.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (92/303 shards):  31%|███       | 1741146/5672672 [02:58<06:08, 10662.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1741146/5672672 [02:58<06:08, 10662.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1743146/5672672 [02:58<06:25, 10198.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1745146/5672672 [02:59<07:30, 8718.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1747146/5672672 [02:59<07:06, 9201.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1749146/5672672 [02:59<06:51, 9531.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1751146/5672672 [02:59<06:41, 9757.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1753146/5672672 [02:59<06:29, 10052.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1755146/5672672 [03:00<06:17, 10388.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1757146/5672672 [03:00<06:15, 10415.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (93/303 shards):  31%|███       | 1759146/5672672 [03:00<06:09, 10579.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███       | 1759868/5672672 [03:00<06:09, 10579.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███       | 1760868/5672672 [03:00<06:22, 10224.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███       | 1762868/5672672 [03:00<06:22, 10231.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███       | 1764868/5672672 [03:01<06:48, 9557.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███       | 1766868/5672672 [03:01<06:32, 9959.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███       | 1768868/5672672 [03:01<06:18, 10319.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███       | 1770868/5672672 [03:01<06:08, 10602.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███▏      | 1772868/5672672 [03:02<09:05, 7144.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███▏      | 1774868/5672672 [03:02<08:03, 8063.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███▏      | 1776868/5672672 [03:02<07:27, 8703.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (94/303 shards):  31%|███▏      | 1778590/5672672 [03:02<06:59, 9272.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  31%|███▏      | 1778590/5672672 [03:02<06:59, 9272.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  31%|███▏      | 1780590/5672672 [03:02<06:41, 9693.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  31%|███▏      | 1782590/5672672 [03:03<06:58, 9291.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  31%|███▏      | 1783590/5672672 [03:03<07:32, 8594.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  31%|███▏      | 1785590/5672672 [03:03<07:02, 9199.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  32%|███▏      | 1787590/5672672 [03:03<06:42, 9661.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  32%|███▏      | 1789590/5672672 [03:03<06:33, 9872.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  32%|███▏      | 1791590/5672672 [03:04<06:19, 10237.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  32%|███▏      | 1793590/5672672 [03:04<06:03, 10671.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  32%|███▏      | 1795590/5672672 [03:04<06:02, 10707.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (95/303 shards):  32%|███▏      | 1797312/5672672 [03:04<05:57, 10840.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (96/303 shards):  32%|███▏      | 1797312/5672672 [03:04<05:57, 10840.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (207/303 shards):  68%|██████▊   | 3885454/5672672 [06:34<02:48, 10616.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (207/303 shards):  69%|██████▊   | 3887454/5672672 [06:34<02:44, 10863.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (207/303 shards):  69%|██████▊   | 3889454/5672672 [06:35<02:43, 10903.99 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (207/303 shards):  69%|██████▊   | 3891454/5672672 [06:35<02:40, 11104.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (207/303 shards):  69%|██████▊   | 3893454/5672672 [06:35<02:44, 10846.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▊   | 3894176/5672672 [06:35<02:43, 10846.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▊   | 3895176/5672672 [06:35<02:44, 10820.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▊   | 3897176/5672672 [06:35<02:41, 10996.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▊   | 3899176/5672672 [06:36<02:51, 10351.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▉   | 3901176/5672672 [06:36<03:52, 7627.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▉   | 3903176/5672672 [06:36<03:29, 8458.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▉   | 3905176/5672672 [06:36<03:13, 9113.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▉   | 3907176/5672672 [06:37<03:04, 9548.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▉   | 3909176/5672672 [06:37<02:55, 10036.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▉   | 3911176/5672672 [06:37<02:48, 10428.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (208/303 shards):  69%|██████▉   | 3912898/5672672 [06:37<02:45, 10620.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3912898/5672672 [06:37<02:45, 10620.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3914898/5672672 [06:37<02:45, 10605.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3916898/5672672 [06:37<02:52, 10191.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3918898/5672672 [06:38<02:50, 10278.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3920898/5672672 [06:38<02:49, 10321.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3922898/5672672 [06:38<02:45, 10581.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3924898/5672672 [06:38<02:42, 10774.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3926898/5672672 [06:38<02:39, 10955.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3928898/5672672 [06:39<02:39, 10933.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (209/303 shards):  69%|██████▉   | 3930898/5672672 [06:39<02:43, 10683.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  69%|██████▉   | 3931619/5672672 [06:39<02:42, 10683.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  69%|██████▉   | 3932619/5672672 [06:39<02:50, 10229.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  69%|██████▉   | 3934619/5672672 [06:39<02:57, 9810.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  69%|██████▉   | 3935619/5672672 [06:39<03:25, 8436.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  69%|██████▉   | 3936619/5672672 [06:39<03:20, 8653.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  69%|██████▉   | 3938619/5672672 [06:40<03:09, 9129.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  69%|██████▉   | 3939619/5672672 [06:40<03:06, 9282.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  69%|██████▉   | 3941619/5672672 [06:40<02:57, 9728.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  70%|██████▉   | 3942619/5672672 [06:40<02:57, 9772.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  70%|██████▉   | 3944619/5672672 [06:40<02:48, 10246.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  70%|██████▉   | 3946619/5672672 [06:40<02:45, 10419.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  70%|██████▉   | 3948619/5672672 [06:41<02:43, 10541.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (210/303 shards):  70%|██████▉   | 3950340/5672672 [06:41<02:41, 10680.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3950340/5672672 [06:41<02:41, 10680.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3952340/5672672 [06:41<02:40, 10730.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3954340/5672672 [06:41<02:46, 10316.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3956340/5672672 [06:41<02:45, 10372.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3958340/5672672 [06:42<02:47, 10224.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3960340/5672672 [06:42<02:40, 10666.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3962340/5672672 [06:42<02:37, 10857.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3964340/5672672 [06:42<02:35, 10983.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3966340/5672672 [06:42<02:33, 11117.15 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (211/303 shards):  70%|██████▉   | 3968340/5672672 [06:42<02:32, 11162.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|██████▉   | 3969061/5672672 [06:42<02:32, 11162.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|██████▉   | 3970061/5672672 [06:43<02:37, 10829.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3972061/5672672 [06:43<02:34, 11026.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3974061/5672672 [06:43<02:40, 10553.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3976061/5672672 [06:43<02:36, 10820.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3978061/5672672 [06:43<02:34, 11001.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3980061/5672672 [06:43<02:32, 11066.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3982061/5672672 [06:44<02:30, 11249.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3984061/5672672 [06:44<02:29, 11258.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3986061/5672672 [06:44<02:28, 11352.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (212/303 shards):  70%|███████   | 3987782/5672672 [06:44<02:29, 11295.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  70%|███████   | 3987782/5672672 [06:44<02:29, 11295.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  70%|███████   | 3989782/5672672 [06:44<02:29, 11228.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  70%|███████   | 3991782/5672672 [06:45<02:37, 10669.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  70%|███████   | 3993782/5672672 [06:45<02:35, 10828.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  70%|███████   | 3995782/5672672 [06:45<02:33, 10894.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  70%|███████   | 3997782/5672672 [06:45<02:31, 11034.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  71%|███████   | 3999782/5672672 [06:45<02:29, 11215.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  71%|███████   | 4001782/5672672 [06:45<02:27, 11290.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  71%|███████   | 4003782/5672672 [06:46<02:30, 11125.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (213/303 shards):  71%|███████   | 4005782/5672672 [06:46<02:28, 11230.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4006503/5672672 [06:46<02:28, 11230.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4007503/5672672 [06:46<02:31, 11014.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4009503/5672672 [06:46<02:27, 11243.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4011503/5672672 [06:46<02:37, 10547.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4013503/5672672 [06:47<02:36, 10577.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4015503/5672672 [06:47<02:35, 10664.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4017503/5672672 [06:47<03:18, 8328.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4019503/5672672 [06:47<02:59, 9193.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4021503/5672672 [06:47<02:49, 9753.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4023503/5672672 [06:48<02:41, 10223.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (214/303 shards):  71%|███████   | 4025224/5672672 [06:48<02:35, 10581.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4025224/5672672 [06:48<02:35, 10581.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4027224/5672672 [06:48<02:46, 9865.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4029224/5672672 [06:48<02:48, 9754.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4031224/5672672 [06:48<02:42, 10073.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4033224/5672672 [06:49<02:40, 10210.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4035224/5672672 [06:49<02:35, 10530.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4037224/5672672 [06:49<02:31, 10783.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4039224/5672672 [06:49<02:28, 10971.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████   | 4041224/5672672 [06:49<02:26, 11104.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (215/303 shards):  71%|███████▏  | 4043224/5672672 [06:49<02:24, 11267.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  71%|███████▏  | 4043945/5672672 [06:50<02:24, 11267.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  71%|███████▏  | 4044945/5672672 [06:50<02:28, 10928.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  71%|███████▏  | 4046945/5672672 [06:50<02:25, 11144.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  71%|███████▏  | 4048945/5672672 [06:50<02:34, 10482.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  71%|███████▏  | 4050945/5672672 [06:50<02:33, 10549.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  71%|███████▏  | 4052945/5672672 [06:50<02:33, 10582.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  71%|███████▏  | 4054945/5672672 [06:51<02:29, 10804.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  72%|███████▏  | 4056945/5672672 [06:51<02:27, 10954.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  72%|███████▏  | 4058945/5672672 [06:51<02:25, 11076.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  72%|███████▏  | 4060945/5672672 [06:51<02:24, 11131.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (216/303 shards):  72%|███████▏  | 4062666/5672672 [06:51<02:22, 11274.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4062666/5672672 [06:51<02:22, 11274.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4064666/5672672 [06:51<02:23, 11206.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4066666/5672672 [06:52<02:32, 10550.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4068666/5672672 [06:52<02:30, 10634.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4070666/5672672 [06:52<02:29, 10739.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4072666/5672672 [06:52<02:26, 10914.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4074666/5672672 [06:52<02:24, 11078.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4076666/5672672 [06:53<03:16, 8130.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4078666/5672672 [06:53<02:59, 8871.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (217/303 shards):  72%|███████▏  | 4080666/5672672 [06:53<02:47, 9519.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4081387/5672672 [06:53<02:47, 9519.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4082387/5672672 [06:53<02:41, 9854.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4084387/5672672 [06:53<02:34, 10263.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4086387/5672672 [06:54<02:41, 9830.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4088387/5672672 [06:54<02:38, 10007.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4090387/5672672 [06:54<02:33, 10307.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4092387/5672672 [06:54<02:29, 10565.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4094387/5672672 [06:54<02:26, 10805.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4096387/5672672 [06:55<02:23, 10958.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4098387/5672672 [06:55<02:23, 11003.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (218/303 shards):  72%|███████▏  | 4100108/5672672 [06:55<02:21, 11083.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  72%|███████▏  | 4100108/5672672 [06:55<02:21, 11083.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  72%|███████▏  | 4102108/5672672 [06:55<02:22, 11008.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  72%|███████▏  | 4104108/5672672 [06:55<02:30, 10438.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  72%|███████▏  | 4106108/5672672 [06:55<02:36, 10016.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  72%|███████▏  | 4108108/5672672 [06:56<02:31, 10323.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  72%|███████▏  | 4110108/5672672 [06:56<02:28, 10553.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  72%|███████▏  | 4112108/5672672 [06:56<02:24, 10815.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  73%|███████▎  | 4114108/5672672 [06:56<02:23, 10861.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  73%|███████▎  | 4116108/5672672 [06:56<02:20, 11096.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (219/303 shards):  73%|███████▎  | 4118108/5672672 [06:57<02:18, 11234.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4118829/5672672 [06:57<02:18, 11234.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4119829/5672672 [06:57<02:18, 11195.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4121829/5672672 [06:57<02:16, 11340.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4123829/5672672 [06:57<02:26, 10587.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4125829/5672672 [06:57<02:26, 10592.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4127829/5672672 [06:57<02:25, 10648.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4129829/5672672 [06:58<02:23, 10788.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4131829/5672672 [06:58<02:20, 10985.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4133829/5672672 [06:58<02:19, 11049.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4135829/5672672 [06:58<03:14, 7888.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (220/303 shards):  73%|███████▎  | 4137550/5672672 [06:59<02:57, 8644.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4137550/5672672 [06:59<02:57, 8644.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4139550/5672672 [06:59<02:44, 9316.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4141550/5672672 [06:59<02:44, 9295.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4143550/5672672 [06:59<02:36, 9769.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4145550/5672672 [06:59<02:32, 9983.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4147550/5672672 [07:00<02:28, 10246.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4149550/5672672 [07:00<02:24, 10531.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4151550/5672672 [07:00<02:21, 10762.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4153550/5672672 [07:00<02:18, 10966.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (221/303 shards):  73%|███████▎  | 4155550/5672672 [07:00<02:17, 11001.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  73%|███████▎  | 4156271/5672672 [07:00<02:17, 11001.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  73%|███████▎  | 4157271/5672672 [07:00<02:17, 11006.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  73%|███████▎  | 4159271/5672672 [07:01<02:15, 11168.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  73%|███████▎  | 4161271/5672672 [07:01<02:24, 10477.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  73%|███████▎  | 4163271/5672672 [07:01<02:23, 10523.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  73%|███████▎  | 4165271/5672672 [07:01<02:21, 10632.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  73%|███████▎  | 4167271/5672672 [07:01<02:19, 10779.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  73%|███████▎  | 4169271/5672672 [07:02<02:17, 10961.01 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  74%|███████▎  | 4171271/5672672 [07:02<02:14, 11133.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  74%|███████▎  | 4173271/5672672 [07:02<02:13, 11236.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (222/303 shards):  74%|███████▎  | 4174992/5672672 [07:02<02:13, 11188.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▎  | 4174992/5672672 [07:02<02:13, 11188.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▎  | 4176992/5672672 [07:02<02:14, 11115.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▎  | 4178992/5672672 [07:02<02:21, 10551.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▎  | 4180992/5672672 [07:03<02:20, 10614.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▎  | 4182992/5672672 [07:03<02:20, 10631.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▍  | 4184992/5672672 [07:03<02:20, 10617.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▍  | 4186992/5672672 [07:03<02:17, 10828.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▍  | 4188992/5672672 [07:03<02:14, 11030.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▍  | 4190992/5672672 [07:03<02:12, 11186.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (223/303 shards):  74%|███████▍  | 4192992/5672672 [07:04<02:12, 11138.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4193713/5672672 [07:04<02:12, 11138.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4194713/5672672 [07:04<03:05, 7960.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4196713/5672672 [07:04<02:47, 8830.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4197713/5672672 [07:04<02:50, 8675.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4199713/5672672 [07:05<02:39, 9256.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4201713/5672672 [07:05<02:32, 9619.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4203713/5672672 [07:05<02:27, 9935.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4205713/5672672 [07:05<02:23, 10254.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4207713/5672672 [07:05<02:18, 10590.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4209713/5672672 [07:05<02:15, 10835.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (224/303 shards):  74%|███████▍  | 4211713/5672672 [07:06<02:14, 10875.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  74%|███████▍  | 4212434/5672672 [07:06<02:14, 10875.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  74%|███████▍  | 4213434/5672672 [07:06<02:14, 10810.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  74%|███████▍  | 4215434/5672672 [07:06<02:11, 11082.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  74%|███████▍  | 4217434/5672672 [07:06<02:20, 10324.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  74%|███████▍  | 4219434/5672672 [07:06<02:19, 10433.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  74%|███████▍  | 4221434/5672672 [07:07<02:21, 10286.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  74%|███████▍  | 4223434/5672672 [07:07<02:22, 10180.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  74%|███████▍  | 4225434/5672672 [07:07<02:25, 9946.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  75%|███████▍  | 4227434/5672672 [07:07<02:26, 9862.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  75%|███████▍  | 4228434/5672672 [07:07<02:27, 9805.99 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  75%|███████▍  | 4229434/5672672 [07:07<02:27, 9789.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (225/303 shards):  75%|███████▍  | 4230434/5672672 [07:08<02:27, 9777.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4231155/5672672 [07:08<02:27, 9777.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4232155/5672672 [07:08<02:31, 9521.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4233155/5672672 [07:08<02:31, 9530.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4234155/5672672 [07:08<02:32, 9433.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4235155/5672672 [07:08<03:01, 7903.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4236155/5672672 [07:08<02:56, 8119.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4238155/5672672 [07:08<02:39, 8975.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4240155/5672672 [07:09<02:30, 9499.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4241155/5672672 [07:09<02:30, 9532.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4243155/5672672 [07:09<02:25, 9845.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4245155/5672672 [07:09<02:21, 10092.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4247155/5672672 [07:09<02:19, 10255.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (226/303 shards):  75%|███████▍  | 4249155/5672672 [07:09<02:18, 10314.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▍  | 4249876/5672672 [07:10<02:17, 10314.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▍  | 4250876/5672672 [07:10<02:18, 10246.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▍  | 4252876/5672672 [07:10<02:15, 10514.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▌  | 4254876/5672672 [07:10<03:14, 7292.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▌  | 4256876/5672672 [07:10<02:52, 8219.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▌  | 4258876/5672672 [07:11<02:39, 8886.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▌  | 4260876/5672672 [07:11<02:28, 9490.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▌  | 4262876/5672672 [07:11<02:21, 9957.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▌  | 4264876/5672672 [07:11<02:17, 10274.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▌  | 4266876/5672672 [07:11<02:13, 10503.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (227/303 shards):  75%|███████▌  | 4268597/5672672 [07:11<02:11, 10709.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  75%|███████▌  | 4268597/5672672 [07:11<02:11, 10709.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  75%|███████▌  | 4270597/5672672 [07:12<02:10, 10709.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  75%|███████▌  | 4272597/5672672 [07:12<02:16, 10267.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  75%|███████▌  | 4274597/5672672 [07:12<02:12, 10532.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  75%|███████▌  | 4276597/5672672 [07:12<02:10, 10705.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  75%|███████▌  | 4278597/5672672 [07:12<02:09, 10796.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  75%|███████▌  | 4280597/5672672 [07:13<02:08, 10828.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  75%|███████▌  | 4282597/5672672 [07:13<02:07, 10928.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  76%|███████▌  | 4284597/5672672 [07:13<02:07, 10899.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (228/303 shards):  76%|███████▌  | 4286597/5672672 [07:13<02:09, 10717.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4287318/5672672 [07:13<02:09, 10717.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4288318/5672672 [07:13<02:13, 10371.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4290318/5672672 [07:14<02:10, 10583.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4292318/5672672 [07:14<02:15, 10160.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4294318/5672672 [07:14<02:16, 10075.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4296318/5672672 [07:14<02:12, 10426.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4298318/5672672 [07:14<02:08, 10684.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4300318/5672672 [07:14<02:06, 10882.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4302318/5672672 [07:15<02:05, 10889.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4304318/5672672 [07:15<02:04, 10964.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (229/303 shards):  76%|███████▌  | 4306039/5672672 [07:15<02:04, 11008.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4306039/5672672 [07:15<02:04, 11008.87 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4308039/5672672 [07:15<02:04, 10957.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4310039/5672672 [07:15<02:08, 10576.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4312039/5672672 [07:16<02:06, 10783.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4314039/5672672 [07:16<02:52, 7884.15 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4316039/5672672 [07:16<02:35, 8696.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4318039/5672672 [07:16<02:24, 9352.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4320039/5672672 [07:16<02:16, 9884.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4322039/5672672 [07:17<02:11, 10257.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (230/303 shards):  76%|███████▌  | 4324039/5672672 [07:17<02:08, 10528.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  76%|███████▌  | 4324760/5672672 [07:17<02:08, 10528.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  76%|███████▋  | 4325760/5672672 [07:17<02:07, 10573.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  76%|███████▋  | 4327760/5672672 [07:17<02:04, 10805.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  76%|███████▋  | 4329760/5672672 [07:17<02:08, 10411.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  76%|███████▋  | 4331760/5672672 [07:18<02:05, 10688.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  76%|███████▋  | 4333760/5672672 [07:18<02:03, 10800.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  76%|███████▋  | 4335760/5672672 [07:18<02:02, 10912.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  76%|███████▋  | 4337760/5672672 [07:18<02:01, 11000.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  77%|███████▋  | 4339760/5672672 [07:18<02:02, 10878.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  77%|███████▋  | 4341760/5672672 [07:18<02:00, 11039.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (231/303 shards):  77%|███████▋  | 4343481/5672672 [07:19<02:01, 10919.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4343481/5672672 [07:19<02:01, 10919.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4345481/5672672 [07:19<02:04, 10619.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4347481/5672672 [07:19<02:22, 9299.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4348481/5672672 [07:19<02:20, 9400.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4350481/5672672 [07:19<02:13, 9878.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4352481/5672672 [07:20<02:19, 9456.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4354481/5672672 [07:20<02:11, 10020.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4356481/5672672 [07:20<02:06, 10417.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4358481/5672672 [07:20<02:03, 10647.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4360481/5672672 [07:20<02:00, 10913.99 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (232/303 shards):  77%|███████▋  | 4362202/5672672 [07:20<01:57, 11136.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4362202/5672672 [07:20<01:57, 11136.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4364202/5672672 [07:21<01:58, 11064.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4366202/5672672 [07:21<02:03, 10561.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4368202/5672672 [07:21<02:02, 10690.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4370202/5672672 [07:21<02:02, 10594.15 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4372202/5672672 [07:22<02:48, 7722.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4374202/5672672 [07:22<02:30, 8615.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4376202/5672672 [07:22<02:20, 9254.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4378202/5672672 [07:22<02:11, 9877.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (233/303 shards):  77%|███████▋  | 4380202/5672672 [07:22<02:05, 10320.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4380923/5672672 [07:22<02:05, 10320.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4381923/5672672 [07:23<02:03, 10421.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4383923/5672672 [07:23<02:00, 10691.44 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4385923/5672672 [07:23<02:05, 10219.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4387923/5672672 [07:23<02:04, 10299.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4389923/5672672 [07:23<02:03, 10394.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4391923/5672672 [07:23<02:00, 10647.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4393923/5672672 [07:24<01:57, 10869.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  77%|███████▋  | 4395923/5672672 [07:24<01:58, 10786.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  78%|███████▊  | 4397923/5672672 [07:24<01:56, 10954.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (234/303 shards):  78%|███████▊  | 4399644/5672672 [07:24<01:55, 10987.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4399644/5672672 [07:24<01:55, 10987.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4401644/5672672 [07:24<02:05, 10163.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4403644/5672672 [07:25<02:09, 9774.58 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4405644/5672672 [07:25<02:06, 10003.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4407644/5672672 [07:25<02:04, 10139.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4409644/5672672 [07:25<02:03, 10209.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4411644/5672672 [07:25<02:04, 10158.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4413644/5672672 [07:26<02:06, 9951.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4414644/5672672 [07:26<02:07, 9830.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4415644/5672672 [07:26<02:09, 9720.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4416644/5672672 [07:26<02:10, 9612.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (235/303 shards):  78%|███████▊  | 4417644/5672672 [07:26<02:09, 9677.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4418365/5672672 [07:26<02:09, 9677.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4419365/5672672 [07:26<02:07, 9855.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4421365/5672672 [07:26<02:03, 10135.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4423365/5672672 [07:27<02:09, 9679.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4424365/5672672 [07:27<02:08, 9722.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4426365/5672672 [07:27<02:03, 10110.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4428365/5672672 [07:27<02:02, 10149.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4430365/5672672 [07:27<01:59, 10362.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4432365/5672672 [07:28<02:48, 7363.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4434365/5672672 [07:28<02:30, 8200.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (236/303 shards):  78%|███████▊  | 4436365/5672672 [07:28<02:17, 8959.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4437086/5672672 [07:28<02:17, 8959.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4438086/5672672 [07:28<02:12, 9329.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4440086/5672672 [07:28<02:05, 9832.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4442086/5672672 [07:29<02:08, 9567.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4444086/5672672 [07:29<02:02, 10010.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4446086/5672672 [07:29<01:59, 10279.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4448086/5672672 [07:29<01:55, 10594.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4450086/5672672 [07:29<01:53, 10745.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  78%|███████▊  | 4452086/5672672 [07:30<01:53, 10746.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  79%|███████▊  | 4454086/5672672 [07:30<01:53, 10769.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (237/303 shards):  79%|███████▊  | 4455807/5672672 [07:30<01:52, 10788.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▊  | 4455807/5672672 [07:30<01:52, 10788.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▊  | 4457807/5672672 [07:30<01:53, 10746.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▊  | 4459807/5672672 [07:30<01:57, 10346.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▊  | 4461807/5672672 [07:30<01:54, 10593.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▊  | 4463807/5672672 [07:31<01:55, 10505.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▊  | 4465807/5672672 [07:31<01:52, 10701.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▉  | 4467807/5672672 [07:31<01:52, 10730.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▉  | 4469807/5672672 [07:31<01:51, 10784.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▉  | 4471807/5672672 [07:31<01:49, 10924.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (238/303 shards):  79%|███████▉  | 4473807/5672672 [07:32<01:49, 10978.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4474528/5672672 [07:32<01:49, 10978.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4475528/5672672 [07:32<01:50, 10851.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4477528/5672672 [07:32<01:51, 10739.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4479528/5672672 [07:32<02:01, 9814.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4481528/5672672 [07:32<01:56, 10227.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4483528/5672672 [07:32<01:52, 10571.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4485528/5672672 [07:33<01:49, 10810.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4487528/5672672 [07:33<01:48, 10932.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4489528/5672672 [07:33<01:48, 10918.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4491528/5672672 [07:33<02:28, 7974.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (239/303 shards):  79%|███████▉  | 4493249/5672672 [07:34<02:16, 8635.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4493249/5672672 [07:34<02:16, 8635.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4495249/5672672 [07:34<02:07, 9266.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4497249/5672672 [07:34<02:06, 9299.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4499249/5672672 [07:34<02:00, 9726.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4501249/5672672 [07:34<01:57, 9958.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4503249/5672672 [07:35<01:54, 10191.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4505249/5672672 [07:35<01:50, 10538.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4507249/5672672 [07:35<01:48, 10756.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  79%|███████▉  | 4509249/5672672 [07:35<01:46, 10958.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (240/303 shards):  80%|███████▉  | 4511249/5672672 [07:35<01:44, 11122.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4511970/5672672 [07:35<01:44, 11122.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4512970/5672672 [07:35<01:45, 11034.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4514970/5672672 [07:36<01:43, 11213.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4516970/5672672 [07:36<01:50, 10445.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4518970/5672672 [07:36<01:50, 10431.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4520970/5672672 [07:36<01:50, 10445.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4522970/5672672 [07:36<01:49, 10476.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4524970/5672672 [07:37<01:48, 10549.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4526970/5672672 [07:37<01:46, 10772.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4528970/5672672 [07:37<01:44, 10935.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (241/303 shards):  80%|███████▉  | 4530691/5672672 [07:37<01:43, 11006.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|███████▉  | 4530691/5672672 [07:37<01:43, 11006.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|███████▉  | 4532691/5672672 [07:37<01:42, 11118.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|███████▉  | 4534691/5672672 [07:37<01:48, 10479.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|███████▉  | 4536691/5672672 [07:38<01:47, 10562.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|████████  | 4538691/5672672 [07:38<01:48, 10441.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|████████  | 4540691/5672672 [07:38<01:48, 10427.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|████████  | 4542691/5672672 [07:38<01:46, 10639.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|████████  | 4544691/5672672 [07:38<01:43, 10866.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|████████  | 4546691/5672672 [07:39<01:41, 11068.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (242/303 shards):  80%|████████  | 4548691/5672672 [07:39<01:42, 11004.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4549412/5672672 [07:39<01:42, 11004.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4550412/5672672 [07:39<02:25, 7727.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4552412/5672672 [07:39<02:09, 8669.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4553412/5672672 [07:39<02:11, 8503.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4555412/5672672 [07:40<02:03, 9077.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4557412/5672672 [07:40<01:56, 9547.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4559412/5672672 [07:40<01:53, 9840.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4561412/5672672 [07:40<01:49, 10162.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4563412/5672672 [07:40<01:45, 10549.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  80%|████████  | 4565412/5672672 [07:41<01:43, 10743.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (243/303 shards):  81%|████████  | 4567412/5672672 [07:41<01:41, 10851.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4568133/5672672 [07:41<01:41, 10851.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4569133/5672672 [07:41<01:41, 10832.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4571133/5672672 [07:41<01:39, 11108.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4573133/5672672 [07:41<01:45, 10406.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4575133/5672672 [07:41<01:46, 10305.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4577133/5672672 [07:42<01:45, 10397.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4579133/5672672 [07:42<01:44, 10450.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4581133/5672672 [07:42<01:42, 10654.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4583133/5672672 [07:42<01:42, 10670.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4585133/5672672 [07:42<01:39, 10879.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (244/303 shards):  81%|████████  | 4586854/5672672 [07:43<01:39, 10965.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4586854/5672672 [07:43<01:39, 10965.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4588854/5672672 [07:43<01:39, 10848.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4590854/5672672 [07:43<01:44, 10389.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4592854/5672672 [07:43<01:43, 10456.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4594854/5672672 [07:43<01:42, 10481.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4596854/5672672 [07:44<01:42, 10533.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4598854/5672672 [07:44<01:41, 10615.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4600854/5672672 [07:44<01:39, 10801.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4602854/5672672 [07:44<01:36, 11079.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (245/303 shards):  81%|████████  | 4604854/5672672 [07:44<01:35, 11191.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████  | 4605575/5672672 [07:44<01:35, 11191.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████  | 4606575/5672672 [07:44<01:35, 11127.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████  | 4608575/5672672 [07:45<01:36, 10987.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4610575/5672672 [07:45<03:12, 5525.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4611575/5672672 [07:45<02:58, 5959.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4612575/5672672 [07:46<02:45, 6416.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4614575/5672672 [07:46<02:21, 7454.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4616575/5672672 [07:46<02:07, 8265.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4618575/5672672 [07:46<01:59, 8786.51 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4620575/5672672 [07:46<01:54, 9221.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4621575/5672672 [07:46<01:52, 9345.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  81%|████████▏ | 4622575/5672672 [07:47<01:52, 9334.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (246/303 shards):  82%|████████▏ | 4624296/5672672 [07:47<01:47, 9713.15 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4624296/5672672 [07:47<01:47, 9713.15 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4625296/5672672 [07:47<01:52, 9300.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4627296/5672672 [07:47<01:48, 9621.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4628296/5672672 [07:47<01:57, 8884.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4630296/5672672 [07:47<01:50, 9453.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4632296/5672672 [07:48<01:46, 9803.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4633296/5672672 [07:48<01:46, 9803.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4635296/5672672 [07:48<01:43, 10026.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4637296/5672672 [07:48<01:41, 10192.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4639296/5672672 [07:48<01:39, 10338.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4641296/5672672 [07:48<01:38, 10426.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (247/303 shards):  82%|████████▏ | 4643017/5672672 [07:49<01:38, 10485.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4643017/5672672 [07:49<01:38, 10485.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4645017/5672672 [07:49<01:38, 10402.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4647017/5672672 [07:49<01:41, 10115.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4649017/5672672 [07:49<01:38, 10401.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4651017/5672672 [07:49<01:36, 10598.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4653017/5672672 [07:50<01:34, 10757.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4655017/5672672 [07:50<01:33, 10901.24 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4657017/5672672 [07:50<01:32, 10977.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4659017/5672672 [07:50<01:31, 11094.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (248/303 shards):  82%|████████▏ | 4661017/5672672 [07:50<01:30, 11133.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4661738/5672672 [07:50<01:30, 11133.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4662738/5672672 [07:50<01:33, 10805.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4664738/5672672 [07:51<01:32, 10913.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4666738/5672672 [07:51<01:38, 10209.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4668738/5672672 [07:51<02:32, 6586.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4670738/5672672 [07:52<02:12, 7544.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4672738/5672672 [07:52<02:00, 8311.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4674738/5672672 [07:52<01:51, 8930.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4676738/5672672 [07:52<01:45, 9400.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  82%|████████▏ | 4678738/5672672 [07:52<01:40, 9914.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (249/303 shards):  83%|████████▎ | 4680459/5672672 [07:52<01:36, 10277.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4680459/5672672 [07:52<01:36, 10277.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4682459/5672672 [07:53<01:34, 10466.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4684459/5672672 [07:53<01:37, 10115.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4686459/5672672 [07:53<01:35, 10357.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4688459/5672672 [07:53<01:32, 10670.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4690459/5672672 [07:53<01:29, 10914.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4692459/5672672 [07:54<01:29, 11012.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4694459/5672672 [07:54<01:27, 11130.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4696459/5672672 [07:54<01:26, 11226.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (250/303 shards):  83%|████████▎ | 4698459/5672672 [07:54<01:26, 11293.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4699180/5672672 [07:54<01:26, 11293.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4700180/5672672 [07:54<01:27, 11165.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4702180/5672672 [07:54<01:26, 11226.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4704180/5672672 [07:55<01:31, 10629.64 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4706180/5672672 [07:55<01:29, 10776.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4708180/5672672 [07:55<01:28, 10920.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4710180/5672672 [07:55<01:26, 11068.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4712180/5672672 [07:55<01:26, 11162.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4714180/5672672 [07:55<01:25, 11232.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4716180/5672672 [07:56<01:25, 11182.07 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (251/303 shards):  83%|████████▎ | 4717901/5672672 [07:56<01:27, 10946.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4717901/5672672 [07:56<01:27, 10946.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4719901/5672672 [07:56<01:39, 9535.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4721901/5672672 [07:56<01:40, 9494.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4722901/5672672 [07:56<01:39, 9530.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4724901/5672672 [07:57<01:36, 9866.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4726901/5672672 [07:57<01:33, 10149.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4728901/5672672 [07:57<02:04, 7555.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4730901/5672672 [07:57<01:52, 8344.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4732901/5672672 [07:58<01:43, 9071.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4734901/5672672 [07:58<01:36, 9694.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (252/303 shards):  83%|████████▎ | 4736622/5672672 [07:58<01:31, 10183.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  83%|████████▎ | 4736622/5672672 [07:58<01:31, 10183.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▎ | 4738622/5672672 [07:58<01:30, 10366.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▎ | 4740622/5672672 [07:58<01:32, 10048.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▎ | 4742622/5672672 [07:58<01:30, 10250.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▎ | 4744622/5672672 [07:59<01:29, 10348.28 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▎ | 4746622/5672672 [07:59<01:27, 10530.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▎ | 4748622/5672672 [07:59<01:26, 10726.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▎ | 4750622/5672672 [07:59<01:24, 10943.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▍ | 4752622/5672672 [07:59<01:22, 11143.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (253/303 shards):  84%|████████▍ | 4754622/5672672 [08:00<01:21, 11223.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4755343/5672672 [08:00<01:21, 11223.33 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4756343/5672672 [08:00<01:21, 11235.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4758343/5672672 [08:00<01:20, 11372.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4760343/5672672 [08:00<01:26, 10577.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4762343/5672672 [08:00<01:26, 10580.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4764343/5672672 [08:00<01:25, 10572.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4766343/5672672 [08:01<01:25, 10607.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4768343/5672672 [08:01<01:24, 10746.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4770343/5672672 [08:01<01:22, 10920.30 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4772343/5672672 [08:01<01:22, 10916.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (254/303 shards):  84%|████████▍ | 4774064/5672672 [08:01<01:21, 11034.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4774064/5672672 [08:01<01:21, 11034.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4776064/5672672 [08:02<01:36, 9332.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4778064/5672672 [08:02<01:49, 8162.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4779064/5672672 [08:02<01:53, 7889.74 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4781064/5672672 [08:02<01:42, 8707.18 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4783064/5672672 [08:02<01:35, 9299.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4785064/5672672 [08:03<01:31, 9740.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4787064/5672672 [08:03<01:59, 7382.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4789064/5672672 [08:03<01:48, 8133.17 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4791064/5672672 [08:03<01:38, 8950.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (255/303 shards):  84%|████████▍ | 4792785/5672672 [08:04<01:32, 9482.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  84%|████████▍ | 4792785/5672672 [08:04<01:32, 9482.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4794785/5672672 [08:04<01:51, 7848.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4796785/5672672 [08:04<02:35, 5649.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4797785/5672672 [08:05<02:53, 5044.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4799785/5672672 [08:05<02:22, 6114.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4801785/5672672 [08:05<02:02, 7127.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4803785/5672672 [08:05<01:48, 7986.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4805785/5672672 [08:06<01:40, 8617.26 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4807785/5672672 [08:06<01:34, 9121.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4808785/5672672 [08:06<01:33, 9241.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (256/303 shards):  85%|████████▍ | 4810785/5672672 [08:06<01:27, 9803.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▍ | 4811506/5672672 [08:06<01:27, 9803.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▍ | 4812506/5672672 [08:06<01:53, 7578.92 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▍ | 4814506/5672672 [08:07<01:44, 8219.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▍ | 4815506/5672672 [08:07<03:40, 3882.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▍ | 4816506/5672672 [08:08<05:12, 2736.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▍ | 4817506/5672672 [08:08<04:20, 3281.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▍ | 4819506/5672672 [08:08<03:08, 4516.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▍ | 4821506/5672672 [08:09<02:28, 5723.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▌ | 4823506/5672672 [08:09<02:04, 6811.48 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▌ | 4825506/5672672 [08:09<01:48, 7791.50 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▌ | 4827506/5672672 [08:09<01:38, 8593.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (257/303 shards):  85%|████████▌ | 4829506/5672672 [08:09<01:31, 9198.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4830227/5672672 [08:09<01:31, 9198.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4831227/5672672 [08:10<01:59, 7038.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4833227/5672672 [08:10<01:46, 7895.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4834227/5672672 [08:10<02:46, 5040.99 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4835227/5672672 [08:11<04:15, 3283.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4836227/5672672 [08:11<03:35, 3878.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4837227/5672672 [08:11<03:03, 4553.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4838227/5672672 [08:11<02:38, 5265.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4839227/5672672 [08:12<02:21, 5903.98 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4840227/5672672 [08:12<02:05, 6656.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4841227/5672672 [08:12<01:53, 7299.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4842227/5672672 [08:12<01:45, 7887.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4843227/5672672 [08:12<01:38, 8394.11 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4844227/5672672 [08:12<01:35, 8695.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4846227/5672672 [08:13<02:12, 6219.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4847227/5672672 [08:13<02:03, 6710.47 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (258/303 shards):  85%|████████▌ | 4848948/5672672 [08:13<01:46, 7717.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  85%|████████▌ | 4848948/5672672 [08:13<01:46, 7717.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  85%|████████▌ | 4849948/5672672 [08:13<01:46, 7714.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4851948/5672672 [08:13<01:34, 8663.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4852948/5672672 [08:14<02:32, 5380.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4853948/5672672 [08:14<03:42, 3685.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4855948/5672672 [08:14<02:42, 5019.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4857948/5672672 [08:14<02:10, 6244.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4859948/5672672 [08:15<01:51, 7284.43 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4861948/5672672 [08:15<01:39, 8159.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4863948/5672672 [08:15<01:31, 8845.75 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4865948/5672672 [08:15<01:26, 9328.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (259/303 shards):  86%|████████▌ | 4867669/5672672 [08:15<01:22, 9719.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4867669/5672672 [08:15<01:22, 9719.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4869669/5672672 [08:16<01:36, 8281.12 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4871669/5672672 [08:16<02:29, 5358.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4872669/5672672 [08:17<03:07, 4263.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4874669/5672672 [08:17<02:28, 5359.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4876669/5672672 [08:17<02:04, 6419.23 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4878669/5672672 [08:17<01:47, 7414.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4880669/5672672 [08:18<01:35, 8259.93 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4882669/5672672 [08:18<01:28, 8930.21 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4884669/5672672 [08:18<01:23, 9477.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (260/303 shards):  86%|████████▌ | 4886390/5672672 [08:18<01:20, 9725.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▌ | 4886390/5672672 [08:18<01:20, 9725.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▌ | 4888390/5672672 [08:18<01:25, 9192.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▌ | 4890390/5672672 [08:19<01:25, 9169.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▌ | 4892390/5672672 [08:19<01:21, 9515.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▋ | 4894390/5672672 [08:19<01:18, 9889.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▋ | 4896390/5672672 [08:19<01:16, 10109.42 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▋ | 4898390/5672672 [08:19<01:15, 10264.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▋ | 4900390/5672672 [08:19<01:14, 10386.40 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▋ | 4902390/5672672 [08:20<01:13, 10461.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (261/303 shards):  86%|████████▋ | 4904390/5672672 [08:20<01:12, 10608.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  86%|████████▋ | 4905111/5672672 [08:20<01:12, 10608.66 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  86%|████████▋ | 4906111/5672672 [08:20<01:42, 7495.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4908111/5672672 [08:20<01:33, 8210.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4909111/5672672 [08:21<01:34, 8066.96 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4911111/5672672 [08:21<01:26, 8769.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4912111/5672672 [08:21<01:24, 8959.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4914111/5672672 [08:21<01:21, 9359.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4915111/5672672 [08:21<01:20, 9415.94 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4916111/5672672 [08:21<01:19, 9492.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4918111/5672672 [08:21<01:16, 9836.90 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4920111/5672672 [08:22<01:14, 10101.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4922111/5672672 [08:22<01:13, 10265.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (262/303 shards):  87%|████████▋ | 4923832/5672672 [08:22<01:12, 10322.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4923832/5672672 [08:22<01:12, 10322.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4925832/5672672 [08:22<01:12, 10296.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4927832/5672672 [08:22<01:15, 9864.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4929832/5672672 [08:23<01:12, 10192.78 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4931832/5672672 [08:23<01:12, 10255.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4933832/5672672 [08:23<01:12, 10238.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4935832/5672672 [08:23<01:13, 10093.52 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4937832/5672672 [08:23<01:11, 10256.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4939832/5672672 [08:24<01:11, 10305.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (263/303 shards):  87%|████████▋ | 4941832/5672672 [08:24<01:11, 10262.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4942553/5672672 [08:24<01:11, 10262.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4943553/5672672 [08:24<01:11, 10253.36 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4945553/5672672 [08:24<01:09, 10412.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4947553/5672672 [08:24<01:12, 9950.82 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4949553/5672672 [08:25<01:11, 10179.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4951553/5672672 [08:25<01:09, 10302.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4953553/5672672 [08:25<01:08, 10531.35 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4955553/5672672 [08:25<01:06, 10799.76 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4957553/5672672 [08:25<01:05, 10932.63 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4959553/5672672 [08:25<01:04, 11027.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (264/303 shards):  87%|████████▋ | 4961274/5672672 [08:26<01:05, 10847.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  87%|████████▋ | 4961274/5672672 [08:26<01:05, 10847.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  87%|████████▋ | 4963274/5672672 [08:26<01:05, 10885.56 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  88%|████████▊ | 4965274/5672672 [08:26<01:32, 7624.14 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  88%|████████▊ | 4967274/5672672 [08:26<01:23, 8435.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  88%|████████▊ | 4969274/5672672 [08:27<01:17, 9125.83 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  88%|████████▊ | 4971274/5672672 [08:27<01:12, 9683.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  88%|████████▊ | 4973274/5672672 [08:27<01:09, 10025.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  88%|████████▊ | 4975274/5672672 [08:27<01:07, 10383.77 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  88%|████████▊ | 4977274/5672672 [08:27<01:05, 10649.61 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (265/303 shards):  88%|████████▊ | 4979274/5672672 [08:27<01:03, 10859.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4979995/5672672 [08:28<01:03, 10859.68 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4980995/5672672 [08:28<01:04, 10802.97 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4982995/5672672 [08:28<01:02, 10999.10 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4984995/5672672 [08:28<01:07, 10128.46 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4986995/5672672 [08:28<01:05, 10423.22 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4988995/5672672 [08:28<01:04, 10623.37 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4990995/5672672 [08:29<01:03, 10814.57 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4992995/5672672 [08:29<01:02, 10840.27 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4994995/5672672 [08:29<01:01, 11008.84 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4996995/5672672 [08:29<01:00, 11155.41 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (266/303 shards):  88%|████████▊ | 4998716/5672672 [08:29<01:00, 11185.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 4998716/5672672 [08:29<01:00, 11185.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5000716/5672672 [08:29<01:00, 11088.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5002716/5672672 [08:30<01:03, 10590.32 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5004716/5672672 [08:30<01:01, 10813.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5006716/5672672 [08:30<01:00, 11017.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5008716/5672672 [08:30<00:59, 11146.08 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5010716/5672672 [08:30<00:59, 11175.62 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5012716/5672672 [08:31<00:58, 11215.91 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5014716/5672672 [08:31<00:58, 11248.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (267/303 shards):  88%|████████▊ | 5016716/5672672 [08:31<00:58, 11305.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  88%|████████▊ | 5017437/5672672 [08:31<00:57, 11305.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  88%|████████▊ | 5018437/5672672 [08:31<00:58, 11140.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▊ | 5020437/5672672 [08:31<00:58, 11224.69 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▊ | 5022437/5672672 [08:31<01:00, 10744.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▊ | 5024437/5672672 [08:32<01:21, 8001.89 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▊ | 5026437/5672672 [08:32<01:13, 8836.85 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▊ | 5028437/5672672 [08:32<01:08, 9472.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▊ | 5030437/5672672 [08:32<01:04, 10028.95 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▊ | 5032437/5672672 [08:33<01:02, 10190.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▊ | 5034437/5672672 [08:33<01:03, 10127.25 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (268/303 shards):  89%|████████▉ | 5036158/5672672 [08:33<01:05, 9745.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5036158/5672672 [08:33<01:05, 9745.79 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5037158/5672672 [08:33<01:07, 9483.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5038158/5672672 [08:33<01:07, 9453.05 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5039158/5672672 [08:33<01:06, 9556.60 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5040158/5672672 [08:33<01:09, 9070.29 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5041158/5672672 [08:34<01:10, 9015.16 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5042158/5672672 [08:34<01:08, 9166.31 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5044158/5672672 [08:34<01:05, 9629.04 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5045158/5672672 [08:34<01:16, 8242.67 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5047158/5672672 [08:34<01:09, 8945.39 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5049158/5672672 [08:34<01:05, 9467.80 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5051158/5672672 [08:35<01:03, 9816.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5053158/5672672 [08:35<01:01, 10042.00 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (269/303 shards):  89%|████████▉ | 5054879/5672672 [08:35<01:00, 10139.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5054879/5672672 [08:35<01:00, 10139.45 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5056879/5672672 [08:35<00:59, 10298.20 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5058879/5672672 [08:35<01:01, 9952.13 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5060879/5672672 [08:36<01:00, 10054.34 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5062879/5672672 [08:36<00:59, 10230.71 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5064879/5672672 [08:36<00:58, 10353.38 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5066879/5672672 [08:36<00:57, 10457.06 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5068879/5672672 [08:36<00:58, 10405.73 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5070879/5672672 [08:36<00:57, 10462.02 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (270/303 shards):  89%|████████▉ | 5072879/5672672 [08:37<00:57, 10495.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  89%|████████▉ | 5073600/5672672 [08:37<00:57, 10495.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  89%|████████▉ | 5074600/5672672 [08:37<00:57, 10441.88 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  89%|████████▉ | 5076600/5672672 [08:37<00:56, 10547.03 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  90%|████████▉ | 5078600/5672672 [08:37<00:58, 10143.65 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  90%|████████▉ | 5080600/5672672 [08:37<00:57, 10365.19 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  90%|████████▉ | 5082600/5672672 [08:38<01:17, 7620.81 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  90%|████████▉ | 5084600/5672672 [08:38<01:09, 8416.72 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  90%|████████▉ | 5086600/5672672 [08:38<01:05, 8980.55 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  90%|████████▉ | 5088600/5672672 [08:38<01:01, 9469.54 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  90%|████████▉ | 5090600/5672672 [08:39<00:58, 9925.49 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (271/303 shards):  90%|████████▉ | 5092321/5672672 [08:39<00:56, 10226.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (272/303 shards):  90%|████████▉ | 5092321/5672672 [08:39<00:56, 10226.86 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (272/303 shards):  90%|████████▉ | 5094321/5672672 [08:39<01:07, 8624.70 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (272/303 shards):  90%|████████▉ | 5096321/5672672 [08:39<01:05, 8844.53 examples/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_output_path = \"s3://sagemaker-demo-c4/\"\n",
    "\n",
    "base_job_name = f'huggingface-dataset-workertest'\n",
    "\n",
    "job_names=[]\n",
    "\n",
    "#len(worker_batches)\n",
    "\n",
    "for worker_index in range(1):\n",
    "    current_time = datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "\n",
    "    dataset_job_name = f'{base_job_name}-{worker_index}-{current_time}'\n",
    "\n",
    "    print(dataset_job_name)\n",
    "\n",
    "    job_names.append(dataset_job_name)\n",
    "    \n",
    "    # hyperparameters, which are passed into the training job\n",
    "    hyperparameters = {\n",
    "        \"num_proc\": 96,\n",
    "        \"split_range\": worker_split_range[worker_index],\n",
    "        \"job_name\": dataset_job_name,\n",
    "        \"model_id\": model_id\n",
    "    }\n",
    "    \n",
    "    # estimator\n",
    "    estimator = PyTorch(\n",
    "        entry_point=\"data.py\",\n",
    "        base_job_name=dataset_job_name,\n",
    "        max_run=4000,\n",
    "        role=role,\n",
    "        framework_version=\"2.0.1\",\n",
    "        py_version=\"py310\",\n",
    "        source_dir=\"./scripts\",\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.24xlarge\",\n",
    "        volume_size=100,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        disable_output_compression=True,\n",
    "        keep_alive_period_in_seconds=600,\n",
    "        hyperparameters=hyperparameters,\n",
    "        output_path=data_output_path,\n",
    "        subnets=['subnet-0067baa7d7be55e38'],\n",
    "        security_group_ids=['sg-05ffe325d7d90c501']\n",
    "    )\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    estimator.fit(inputs=data_channels, wait=True, job_name=dataset_job_name)\n",
    "\n",
    "    time.sleep(60)\n",
    "    print(\"Program resumed after 60 seconds for the next run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can started multiple Sagemaker job, with the `.fit()` method passing our FSx file system path to the data script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-0-14-50-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface-dataset-workertest-0-14-50-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-1-14-50-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-1-14-50-02\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-2-14-50-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-2-14-50-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-3-14-50-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-3-14-50-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-4-14-50-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-4-14-50-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-5-14-50-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-5-14-50-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-6-14-50-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-6-14-50-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-7-14-50-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-7-14-50-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-8-14-50-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-8-14-50-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-9-14-50-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-9-14-50-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-10-14-50-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-10-14-50-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-11-14-50-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-11-14-50-15\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-12-14-50-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-12-14-50-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-13-14-50-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-13-14-50-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-dataset-workertest-14-14-50-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program resumed after 60 seconds for the next run\n",
      "huggingface-dataset-workertest-14-14-50-18\n",
      "Program resumed after 60 seconds for the next run\n"
     ]
    }
   ],
   "source": [
    "data_output_path = \"s3://sagemaker-demo-c4/\"\n",
    "\n",
    "base_job_name = f'huggingface-dataset-workertest'\n",
    "\n",
    "job_names=[]\n",
    "\n",
    "#len(worker_split_range)\n",
    "\n",
    "for worker_index in range(len(worker_split_range)):\n",
    "    current_time = datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "\n",
    "    dataset_job_name = f'{base_job_name}-{worker_index}-{current_time}'\n",
    "\n",
    "    print(dataset_job_name)\n",
    "\n",
    "    job_names.append(dataset_job_name)\n",
    "    \n",
    "    # hyperparameters, which are passed into the training job\n",
    "    hyperparameters = {\n",
    "        \"num_proc\": 96,\n",
    "        \"split_range\": worker_split_range[worker_index],\n",
    "        \"job_name\": dataset_job_name,\n",
    "        \"model_id\": model_id\n",
    "    }\n",
    "    \n",
    "    # estimator\n",
    "    estimator = PyTorch(\n",
    "        entry_point=\"data.py\",\n",
    "        base_job_name=dataset_job_name,\n",
    "        max_run=4000,\n",
    "        role=role,\n",
    "        framework_version=\"2.0.1\",\n",
    "        py_version=\"py310\",\n",
    "        source_dir=\"./scripts\",\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.24xlarge\",\n",
    "        volume_size=100,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        disable_output_compression=True,\n",
    "        keep_alive_period_in_seconds=600,\n",
    "        hyperparameters=hyperparameters,\n",
    "        output_path=data_output_path,\n",
    "        subnets=['subnet-0067baa7d7be55e38'],\n",
    "        security_group_ids=['sg-05ffe325d7d90c501']\n",
    "    )\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    estimator.fit(inputs=data_channels, wait=False, job_name=dataset_job_name)\n",
    "\n",
    "    time.sleep(60)\n",
    "    print(\"Program resumed after 60 seconds for the next run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the data in FSx using Amazon EC2 "
   ]
  },
  {
   "attachments": {
    "1fcf4419-1ba5-4c9c-8336-1c3ff84866ec.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:1fcf4419-1ba5-4c9c-8336-1c3ff84866ec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SageMaker training and use FSx as the data channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "training_job_name = f\"huggingface-training-worker-{current_time}\"\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,  # model id from huggingface.co/models\n",
    "    \"dataset_path\": \"/opt/ml/input/data/train\",  # path where sagemaker will save training dataset\n",
    "    \"valid_path\": \"/opt/ml/input/data/valid\",\n",
    "    \"gradient_checkpointing\": True,  # enable gradient checkpointing\n",
    "    \"bf16\": True,  # enable mixed precision training\n",
    "    \"optimizer\": \"adamw_torch\",  # optimizer\n",
    "    \"per_device_train_batch_size\": 1,  # batch size per device during training\n",
    "    \"epochs\": 1,  # number of epochs to train\n",
    "    \"fsdp\": '\"full_shard auto_wrap\"',  # fully sharded data parallelism\n",
    "    \"cache_dir\": \"/opt/ml/sagemaker/warmpoolcache\",  # change this to /tmp if not using warmpools\n",
    "    \"max_steps\": 10,\n",
    "}\n",
    "\n",
    "# estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    max_run=1800,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    framework_version=\"2.0.1\",\n",
    "    py_version=\"py310\",\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    disable_output_compression=True,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    keep_alive_period_in_seconds=600,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=training_input_path,\n",
    "    subnets=[\"subnet-02fa8e3bbd6d5cc5e\"],\n",
    "    security_group_ids=[\"sg-05ffe325d7d90c501\"],\n",
    ")\n"
   ]
  },
  {
   "attachments": {
    "f2c9523f-6ef0-4a5d-9b32-2e0fbe34758d.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:f2c9523f-6ef0-4a5d-9b32-2e0fbe34758d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-04-23-15-12-27-839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-23 15:12:28 Starting - Starting the training job\n",
      "2024-04-23 15:12:28 Pending - Training job waiting for capacity......\n",
      "2024-04-23 15:13:01 Pending - Preparing the instances for training...........................\n",
      "2024-04-23 15:17:39 Downloading - Downloading input data...\n",
      "2024-04-23 15:18:14 Downloading - Downloading the training image...............\n",
      "2024-04-23 15:20:40 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:33,614 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:33,711 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:33,720 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:33,721 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:33,721 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:33,814 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:33,907 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:33,916 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:33,918 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:33,918 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:35,053 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:35,242 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.33.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.9/119.9 kB 10.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.21 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (0.19.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (1.26.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (2023.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.33.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.9/119.9 kB 4.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting datasets (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate>=0.21 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.22.0)\u001b[0m\n",
      "\u001b[35mCollecting bitsandbytes (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (0.19.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (1.26.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (2023.10.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (14.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (3.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 75.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 53.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (0.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (14.0.1)\u001b[0m\n",
      "\u001b[35mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (3.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (3.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 117.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 60.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 22.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 47.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 126.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 17.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 43.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 110.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 29.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, pyarrow-hotfix, huggingface-hub, transformers, bitsandbytes, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.15.0\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.19.4\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.19.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.19.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.38.2\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.38.2:\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, xxhash, pyarrow-hotfix, huggingface-hub, transformers, bitsandbytes, datasets\u001b[0m\n",
      "\u001b[35mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[35mFound existing installation: tokenizers 0.15.0\u001b[0m\n",
      "\u001b[35mUninstalling tokenizers-0.15.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled tokenizers-0.15.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[35mFound existing installation: huggingface-hub 0.19.4\u001b[0m\n",
      "\u001b[35mUninstalling huggingface-hub-0.19.4:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled huggingface-hub-0.19.4\u001b[0m\n",
      "\u001b[35mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[35mFound existing installation: transformers 4.38.2\u001b[0m\n",
      "\u001b[35mUninstalling transformers-4.38.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.38.2\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled transformers-4.38.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed bitsandbytes-0.43.1 datasets-2.19.0 huggingface-hub-0.22.2 pyarrow-hotfix-0.6 tokenizers-0.13.3 transformers-4.33.0 xxhash-3.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35mSuccessfully installed bitsandbytes-0.43.1 datasets-2.19.0 huggingface-hub-0.22.2 pyarrow-hotfix-0.6 tokenizers-0.13.3 transformers-4.33.0 xxhash-3.4.1\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.3.1 -> 24.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:47,112 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:47,112 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:47,231 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:47,338 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:47,347 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:47,441 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-23 15:21:47,450 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"cache_dir\": \"/opt/ml/sagemaker/warmpoolcache\",\n",
      "        \"dataset_path\": \"/opt/ml/input/data/train\",\n",
      "        \"epochs\": 1,\n",
      "        \"fsdp\": \"\\\"full_shard auto_wrap\\\"\",\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"max_steps\": 30,\n",
      "        \"model_id\": \"tiiuae/falcon-7b\",\n",
      "        \"optimizer\": \"adamw_torch\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"valid_path\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-04-23-15-12-27-839\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-365792799466/pytorch-training-2024-04-23-15-12-27-839/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"cache_dir\":\"/opt/ml/sagemaker/warmpoolcache\",\"dataset_path\":\"/opt/ml/input/data/train\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"gradient_checkpointing\":true,\"max_steps\":30,\"model_id\":\"tiiuae/falcon-7b\",\"optimizer\":\"adamw_torch\",\"per_device_train_batch_size\":1,\"valid_path\":\"/opt/ml/input/data/valid\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-365792799466/pytorch-training-2024-04-23-15-12-27-839/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"cache_dir\":\"/opt/ml/sagemaker/warmpoolcache\",\"dataset_path\":\"/opt/ml/input/data/train\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"gradient_checkpointing\":true,\"max_steps\":30,\"model_id\":\"tiiuae/falcon-7b\",\"optimizer\":\"adamw_torch\",\"per_device_train_batch_size\":1,\"valid_path\":\"/opt/ml/input/data/valid\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"pytorch-training-2024-04-23-15-12-27-839\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-365792799466/pytorch-training-2024-04-23-15-12-27-839/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--cache_dir\",\"/opt/ml/sagemaker/warmpoolcache\",\"--dataset_path\",\"/opt/ml/input/data/train\",\"--epochs\",\"1\",\"--fsdp\",\"\\\"full_shard auto_wrap\\\"\",\"--gradient_checkpointing\",\"True\",\"--max_steps\",\"30\",\"--model_id\",\"tiiuae/falcon-7b\",\"--optimizer\",\"adamw_torch\",\"--per_device_train_batch_size\",\"1\",\"--valid_path\",\"/opt/ml/input/data/valid\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_CACHE_DIR=/opt/ml/sagemaker/warmpoolcache\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_FSDP=\"full_shard auto_wrap\"\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=30\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=tiiuae/falcon-7b\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adamw_torch\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_VALID_PATH=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 0 train.py --bf16 True --cache_dir /opt/ml/sagemaker/warmpoolcache --dataset_path /opt/ml/input/data/train --epochs 1 --fsdp \"full_shard auto_wrap\" --gradient_checkpointing True --max_steps 30 --model_id tiiuae/falcon-7b --optimizer adamw_torch --per_device_train_batch_size 1 --valid_path /opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:47,440 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:47,440 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:47,585 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:47,690 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:47,699 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:47,793 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-04-23 15:21:47,802 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"cache_dir\": \"/opt/ml/sagemaker/warmpoolcache\",\n",
      "        \"dataset_path\": \"/opt/ml/input/data/train\",\n",
      "        \"epochs\": 1,\n",
      "        \"fsdp\": \"\\\"full_shard auto_wrap\\\"\",\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"max_steps\": 30,\n",
      "        \"model_id\": \"tiiuae/falcon-7b\",\n",
      "        \"optimizer\": \"adamw_torch\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"valid_path\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-04-23-15-12-27-839\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-365792799466/pytorch-training-2024-04-23-15-12-27-839/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"bf16\":true,\"cache_dir\":\"/opt/ml/sagemaker/warmpoolcache\",\"dataset_path\":\"/opt/ml/input/data/train\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"gradient_checkpointing\":true,\"max_steps\":30,\"model_id\":\"tiiuae/falcon-7b\",\"optimizer\":\"adamw_torch\",\"per_device_train_batch_size\":1,\"valid_path\":\"/opt/ml/input/data/valid\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-365792799466/pytorch-training-2024-04-23-15-12-27-839/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"cache_dir\":\"/opt/ml/sagemaker/warmpoolcache\",\"dataset_path\":\"/opt/ml/input/data/train\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"gradient_checkpointing\":true,\"max_steps\":30,\"model_id\":\"tiiuae/falcon-7b\",\"optimizer\":\"adamw_torch\",\"per_device_train_batch_size\":1,\"valid_path\":\"/opt/ml/input/data/valid\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"pytorch-training-2024-04-23-15-12-27-839\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-365792799466/pytorch-training-2024-04-23-15-12-27-839/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--bf16\",\"True\",\"--cache_dir\",\"/opt/ml/sagemaker/warmpoolcache\",\"--dataset_path\",\"/opt/ml/input/data/train\",\"--epochs\",\"1\",\"--fsdp\",\"\\\"full_shard auto_wrap\\\"\",\"--gradient_checkpointing\",\"True\",\"--max_steps\",\"30\",\"--model_id\",\"tiiuae/falcon-7b\",\"--optimizer\",\"adamw_torch\",\"--per_device_train_batch_size\",\"1\",\"--valid_path\",\"/opt/ml/input/data/valid\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[35mSM_HP_CACHE_DIR=/opt/ml/sagemaker/warmpoolcache\u001b[0m\n",
      "\u001b[35mSM_HP_DATASET_PATH=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[35mSM_HP_FSDP=\"full_shard auto_wrap\"\u001b[0m\n",
      "\u001b[35mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_STEPS=30\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_ID=tiiuae/falcon-7b\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIMIZER=adamw_torch\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[35mSM_HP_VALID_PATH=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mtorchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 train.py --bf16 True --cache_dir /opt/ml/sagemaker/warmpoolcache --dataset_path /opt/ml/input/data/train --epochs 1 --fsdp \"full_shard auto_wrap\" --gradient_checkpointing True --max_steps 30 --model_id tiiuae/falcon-7b --optimizer adamw_torch --per_device_train_batch_size 1 --valid_path /opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[34mSMDDP Version 2.0.1\u001b[0m\n",
      "\u001b[34mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[34mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[34mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[34mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[34mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[34mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[34mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[34mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[35mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[35mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[35mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[35mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[35mSMDDP process group backend is being usedSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[35mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[35mSMDDP process group backend is being used\u001b[0m\n",
      "\u001b[34mNumber of update steps per epoch 25\u001b[0m\n",
      "\u001b[34m0%|          | 0/25 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.17.1+cuda11.8\u001b[0m\n",
      "\u001b[34malgo-1:66:257 [0] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:67:259 [1] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:68:258 [2] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:67:251 [2] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:69:261 [3] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:73:263 [7] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:71:254 [6] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:72:255 [7] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:69:257 [4] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:68:260 [3] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:66:261 [1] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:70:263 [5] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:65:265 [0] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:71:265 [5] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:70:268 [4] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:72:269 [6] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35mSMDDP Optimized Collective is being usedSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[35mSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[35mSMDDP Optimized Collective is being usedSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[35mSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[35mSMDDP Optimized Collective is being usedSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[34mSMDDP Optimized Collective is being usedSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[34mSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[34mSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[34mSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[34mSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[34mSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[34mSMDDP Optimized Collective is being used\u001b[0m\n",
      "\u001b[34mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[34mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[34mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[34mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[34mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[34mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[34mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[34mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[35mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[35mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[35mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[35mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[35mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[35mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[35mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[35mThe current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\u001b[0m\n",
      "\u001b[34m4%|▍         | 1/25 [00:10<04:11, 10.47s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 2/25 [00:11<01:56,  5.08s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 3/25 [00:13<01:13,  3.36s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 4/25 [00:14<00:53,  2.55s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 5/25 [00:15<00:42,  2.10s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 6/25 [00:17<00:34,  1.84s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 7/25 [00:18<00:29,  1.67s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 8/25 [00:19<00:26,  1.55s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 9/25 [00:20<00:23,  1.48s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 10/25 [00:22<00:21,  1.43s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 11/25 [00:23<00:19,  1.39s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 12/25 [00:24<00:17,  1.37s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 13/25 [00:26<00:16,  1.35s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 14/25 [00:27<00:14,  1.33s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 15/25 [00:28<00:13,  1.33s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 16/25 [00:30<00:11,  1.33s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 17/25 [00:31<00:10,  1.32s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 18/25 [00:32<00:09,  1.32s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 19/25 [00:34<00:07,  1.32s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 20/25 [00:35<00:06,  1.32s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 21/25 [00:36<00:05,  1.32s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 22/25 [00:38<00:03,  1.32s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 23/25 [00:39<00:02,  1.32s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 24/25 [00:40<00:01,  1.31s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:41<00:00,  1.31s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:41<00:00,  1.68s/it]\u001b[0m\n",
      "\u001b[34m******epoch=0: train_ppl=tensor(43260.7148, device='cuda:0') train_loss=tensor(10.6750, device='cuda:0')******\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m*******epoch=0: eval_ppl=tensor(nan, device='cuda:0') eval_loss=tensor(nan, device='cuda:0')*******\u001b[0m\n",
      "\u001b[34mTraining done!\u001b[0m\n",
      "\u001b[34m2024-04-23 15:25:39,881 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-23 15:25:39,881 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-23 15:25:39,882 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2024-04-23 15:25:39,877 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-04-23 15:25:39,878 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-04-23 15:25:39,878 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-04-23 15:26:41 Uploading - Uploading generated training model\n",
      "2024-04-23 15:27:27 Completed - Training job completed\n",
      "Training seconds: 1178\n",
      "Billable seconds: 1178\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=data_channels, wait=True, job_name=training_job_name)"
   ]
  },
  {
   "attachments": {
    "2fec8305-73df-43c0-b8e8-1ed0de1374da.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:2fec8305-73df-43c0-b8e8-1ed0de1374da.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
