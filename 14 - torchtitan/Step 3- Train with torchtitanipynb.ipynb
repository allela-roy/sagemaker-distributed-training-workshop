{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5176b1cd-324b-4b0e-a645-87cfbd795d34",
   "metadata": {},
   "source": [
    "# Pre-train Llama-3 8B model using FSDP2 with torchtitan on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddf2b4-9a21-4f73-a262-e83ec85d14f2",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to accelerate distributed training of the Llama-3 models using the torchtitan library on SageMaker training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997becc-4ea9-4020-90d6-e164b0dd4a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2506727-243f-455f-908d-795a8af4a764",
   "metadata": {
    "tags": []
   },
   "source": [
    "You need to run the Notebook from **Step 1-Build your Custom Container Jupyter Notebook** to build the torchtitan custom container for training your model and if you want to use your custom dataset, you can follow the instructions in the **Step 2: Prepare your Dataset Jupyter Notebook** to download your dataset(s) to s3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b556b-3182-411d-bb6d-b80eafc0d9bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Amazon SageMaker Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb9a71-d313-4e36-bc0d-be2039e6d2f8",
   "metadata": {},
   "source": [
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment, such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker execution role. Upgrade SageMaker SDK to the latest version.\n",
    "\n",
    "NOTE: This step might require a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead3d1b-f462-4d5f-8e6b-5ee1ce22b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade \"sagemaker>=2.224\"\n",
    "%pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4096f5-00a4-4f89-8164-757a2974c915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "#set default path for data channels\n",
    "data_channels=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f91b8-ccb4-46b0-a9bf-8616266c9394",
   "metadata": {},
   "source": [
    "### Clone the torchtitan repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86d23b-c5c8-48b2-81b5-30bf7b0a94f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/torchtitan.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f09c9-268c-4e3b-bf2f-cf55400bcbf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we create a source directory that will contain the the training source code dependencies and files required to execute the training. We also move the required dependencies from the torchtitan directory to our source direcroty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99de369-aa5d-453c-8cc1-2788176b7a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir torchtitan/src\n",
    "!cp -r torchtitan/torchtitan/ torchtitan/train_configs/ torchtitan/train.py  torchtitan/src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a83278-cdd8-4606-b2fe-cab0c5c90aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba4df7-49bd-42f6-9d0f-32ae36609c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!yes | rm -r torchtitan/torchtitan/ torchtitan/train_configs/ torchtitan/train.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d152b3e-ed27-475a-adc0-dc0000230d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd torchtitan/src/train_configs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17a6bc-d76c-4d74-a87e-6b3db8645314",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Downloading a tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac21edd-1454-4462-8621-aba80abb9302",
   "metadata": {},
   "source": [
    "We will need the Llama-3 tokenizer that will be used to pre-process the dataset to generate tokens. Update the command below with your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8ee33-8d6c-452c-8c01-ffe1048d4f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir torchtitan/src/llama-3-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bcd5b6-b896-4b53-8be3-d7daa15d2b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python torchtitan/src/torchtitan/datasets/download_tokenizer.py --repo_id meta-llama/Meta-Llama-3-8B --local_dir torchtitan/src/llama-3-tokenizer  --tokenizer_path \"original\" --hf_token=\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50913c56-a14d-4e57-8a8b-2ff6bb997561",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Update the LLama-3 8B toml configuration file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfd87f-37d9-418f-9a3d-49ebc3867014",
   "metadata": {
    "tags": []
   },
   "source": [
    "The options for training models with torchtitan are easily configured via the toml files. In this tutorial we will be working with the Llama-3.toml file located in torchtitan/src/train_configs/ to configure the training options. We will need to modify the sections below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43df3c1-4739-413d-9431-8cc149a566d5",
   "metadata": {},
   "source": [
    "1. Enable Tensorboard profiling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ef257-0a01-4715-9327-d04d4f0b2e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[metrics]\n",
    "log_freq = 10\n",
    "enable_tensorboard = true\n",
    "save_tb_folder = \"/opt/ml/output/tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965418f-3caa-4398-85e6-81bbc35b5adf",
   "metadata": {},
   "source": [
    "2.Enable torch.compile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040243f2-cfca-4549-814f-314326a2c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d4b606-b73d-4dad-b88a-426494c39794",
   "metadata": {},
   "source": [
    "3. Enable fp8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da731c95-644e-45a0-93e1-96f259bdd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_float8_linear = true\n",
    "enable_fsdp_float8_all_gather = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304e3f2-7fd0-486d-a81d-d3cd6cb8e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Enable fp8 all-gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce812ec3-e7a4-4a02-9765-2e00c1e58a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_fsdp_float8_all_gather= true\n",
    "precompute_float8_dynamic_scale_for_fsdp = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179af7c7-7c84-4711-8123-cde493da5372",
   "metadata": {},
   "source": [
    "Below is the full updated configuration with the above optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab822a3-d6fc-49ae-bcc4-ee6cd4a666a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile torchtitan/src/train_configs/llama3_8b_optimisations.toml\n",
    "# torchtitan Config.toml\n",
    "\n",
    "[job]\n",
    "dump_folder = \"./outputs\"\n",
    "description = \"Llama 3 8B training\"\n",
    "\n",
    "[profiling]\n",
    "enable_profiling = false\n",
    "save_traces_folder = \"profile_trace\"\n",
    "profile_freq = 100\n",
    "\n",
    "[metrics]\n",
    "log_freq = 10\n",
    "enable_tensorboard = true\n",
    "save_tb_folder = \"/opt/ml/output/tensorboard\"\n",
    "\n",
    "[model]\n",
    "name = \"llama3\"\n",
    "flavor = \"8B\"\n",
    "norm_type = \"rmsnorm\"  # layernorm / np_layernorm / rmsnorm / fused_rmsnorm\n",
    "tokenizer_path = \"./llama-3-tokenizer/original/tokenizer.model\"\n",
    "\n",
    "[optimizer]\n",
    "name = \"AdamW\"\n",
    "lr = 3e-4\n",
    "\n",
    "[training]\n",
    "batch_size = 1\n",
    "seq_len = 8192\n",
    "warmup_steps = 200  # lr scheduler warm up\n",
    "max_norm = 1.0  # grad norm clipping\n",
    "steps = 1000\n",
    "data_parallel_degree = -1\n",
    "tensor_parallel_degree = 1\n",
    "compile = true\n",
    "dataset = \"c4\"\n",
    "\n",
    "[experimental]\n",
    "pipeline_parallel_degree = 1\n",
    "\n",
    "[checkpoint]\n",
    "enable_checkpoint = false\n",
    "folder = \"checkpoint\"\n",
    "interval_type = \"steps\"\n",
    "interval = 500\n",
    "model_weights_only = false\n",
    "export_dtype = \"float32\"\n",
    "async_mode = \"disabled\" # [\"disabled\", \"async\", \"async_with_pinned_mem\"]\n",
    "\n",
    "[activation_checkpoint]\n",
    "mode = 'selective'  # ['none', 'selective', 'full']\n",
    "selective_ac_option = 'op'  # 'int' = ac every positive int layer or 'op', ac based on ops policy\n",
    "\n",
    "[float8]\n",
    "enable_float8_linear = true\n",
    "enable_fsdp_float8_all_gather= true\n",
    "precompute_float8_dynamic_scale_for_fsdp = true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ffa48-5182-4eb5-a42a-d58f2ef5d5c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure Tensorboard for estimator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17b010-58a2-4315-9c1f-c4b60bf6361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "LOG_DIR=\"/opt/ml/output/tensorboard\"\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f\"s3://sagemaker-{region}-{account}/tensorboard/\",\n",
    "    container_local_output_path=LOG_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8522bd-2b38-4ce8-9218-6fb981504d86",
   "metadata": {},
   "source": [
    "### (Optional) Configure path to the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857831e7-e7f6-4705-8440-9377ded30de2",
   "metadata": {},
   "source": [
    "We are going to use the default dataset c4 that is pre-configured for the torchtitan dataset. However, if you have your own dataset residing in s3 you need to configure the input data channels below to point to your dataset. We have provided a sample Jupyter Notebook in Step 2 to enable you to download c4 dataset to s3 to guide you how to use your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b473f-1f34-4231-9174-dfe6781b566e",
   "metadata": {},
   "source": [
    "Next, we set up the data channels for SageMaker training by creating TrainingInput objects from the provided S3 bucket paths for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80040516-7f06-4183-9754-666040398bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_location = \"path to s3 dataset from the second Notebook\"\n",
    "\n",
    "s3_train_bucket = training_dataset_location\n",
    "\n",
    "if s3_train_bucket != None:\n",
    "    train = sagemaker.inputs.TrainingInput(s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\")\n",
    "    data_channels = {\"train\": train}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f1909-9d23-401b-99c9-8468d196b657",
   "metadata": {},
   "source": [
    "You will also need to add the utility function below to the torchtitan/src/torchtitan/datasets/hf_datasets.py to load your dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55a1b226-f454-4c62-9460-d3996142f8cf",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "if dataset_name == \"c4_custom\":\n",
    "            logger.info(f\"Loading dataset from {dataset_path}\")\n",
    "\n",
    "            try:\n",
    "                ds = load_arrow_dataset(dataset_path)\n",
    "                logger.info(f\"Loaded dataset with {len(ds)} examples\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading dataset: {str(e)}\")\n",
    "                raise\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06940e2c-441d-4415-823e-f88ef8a47199",
   "metadata": {},
   "source": [
    "Lastly, in your configuration, you will need to update the dataset entry in the torchtitan/src/torchtitan/datasets/hf_datasets.py file to include your custom dataset e.g in this case c4_custom"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce8c1743-4d14-4dfe-91b1-fff68629e9a9",
   "metadata": {},
   "source": [
    "_supported_datasets = {\n",
    "    \"c4_mini\": \"torchtitan/datasets/c4_mini\",\n",
    "    \"c4\": \"allenai/c4\",\n",
    "    \"c4_custom\": \"/opt/ml/input/data/train/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20000f2b-f131-4976-aa9d-da7d5243651e",
   "metadata": {},
   "source": [
    "### Create the SageMaker estimator function for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4e622-afe3-4c10-8110-40bc83185ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1122252-d0e8-43cf-93df-fbf8b07a0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "hyperparameters = {\n",
    "    \"config_file\": \"train_configs/llama3_8b_optimisations.toml\"\n",
    "}\n",
    "env = {}\n",
    "env['HF_HUB_ETAG_TIMEOUT'] = '500'\n",
    "\n",
    "timestamp = strftime(\"%Y-%m-%d-%H-%M\", gmtime())\n",
    "\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    base_job_name=f'llama3-8b-compile-fp8-fp8-comms-{timestamp}',\n",
    "    entry_point=\"train.py\",\n",
    "    image_uri=\"<path to image uri>\",\n",
    "    source_dir=os.path.join(os.getcwd(), \"torchtitan/src\"),\n",
    "    role=role,\n",
    "    instance_type=\"ml.p5.48xlarge\",\n",
    "    volume_size=800,\n",
    "    instance_count=4,\n",
    "    environment=env,\n",
    "    hyperparameters=hyperparameters,\n",
    "    use_spot_instances = False,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    distribution={\n",
    "    'torch_distributed': {'enabled': True},\n",
    "    },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb509ddd-04a4-4f86-a4b8-67d705746502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0044e4ef-39f8-4427-84d0-fcacb039a698",
   "metadata": {},
   "source": [
    "Then we finally, launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e717ff0-3683-44d0-bf6e-d4738b5116f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_estimator.fit(inputs=data_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fefc80-606a-46bd-bf7d-073ad18b9abf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perfomance Comparison with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4cdce-d036-4604-9e3c-5c3ee184e4b0",
   "metadata": {},
   "source": [
    "To compare the various optimisations, you can start with a baseline training job and apply the optimizations incrementally in subsequent runs. You can visualise the performance speedup and loss curves through [Tensorboard](https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efdc2b-f4f1-4231-93e4-9c08a49e1e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
